<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Glove on Learn and record</title>
    <link>http://cheriszhang.github.io/tags/glove/</link>
    <description>Recent content in Glove on Learn and record</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 Mar 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://cheriszhang.github.io/tags/glove/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>[ml] GloVe模型理解</title>
      <link>http://cheriszhang.github.io/post/ml/glove/</link>
      <pubDate>Fri, 03 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://cheriszhang.github.io/post/ml/glove/</guid>
      <description>

&lt;p&gt;一篇初学者的论文阅读笔记。尽量通俗易懂，尽量不加公式。&lt;/p&gt;

&lt;h2 id=&#34;写在前面的例子:d4a4a0664e02b718076bfd7252ab4c65&#34;&gt;写在前面的例子&lt;/h2&gt;

&lt;p&gt;首先，是一个分析词之间共现概率的例子：&lt;/p&gt;

&lt;p&gt;考虑词i和词j是某个特定领域（如：热力学）的词，比如i=&lt;r&gt;ice&lt;/r&gt;, j=&lt;r&gt;steam&lt;/r&gt;。这里还会用到一个词k，分析\(ratio = \frac{P_{ik}}{P_{jk}} \)的情况：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;词k和ice有关，和steam无关，如k=&lt;r&gt;solid&lt;/r&gt;。那么\( ratio \)的值会很大。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;词k和steam有关，和ice无关，如k=&lt;r&gt;gas&lt;/r&gt;。那么\( ratio \)的值会很小。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;词k和ice, steam均相关/均不相关，如k=&lt;r&gt;water/fashion&lt;/r&gt;。那么\( ratio \)的值会趋于1。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这个例子想说明什么呢？ 下面是论文中的原话:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Compared to the raw probabilities, the ratio is better able to distinguish relevant words (solid and gas) from irrelevant words (water and fashion) and it is also better able to discriminate between the two relevant words.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;翻译一下，大概是，和原始概率值相比，&lt;strong&gt;ratio可以更好的区分相关词(solid, gas)和不相关词(water和fashion)&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;比起单纯分析两个词共现概率的值大小，共现概率的比更能说明词与词之间的相关程度。打个不恰当的比方，没有对比就没有伤害。&lt;/p&gt;

&lt;h2 id=&#34;关于模型:d4a4a0664e02b718076bfd7252ab4c65&#34;&gt;关于模型&lt;/h2&gt;

&lt;p&gt;这也就因此引出了本文的正题。&lt;/p&gt;

&lt;p&gt;作为一个标杆性质的模型，word2vec可谓是集万千宠爱于一身。embedding的思想也被用于机器学习能涉及的各个领域。&lt;/p&gt;

&lt;p&gt;Word2Vec呢，通过滑动窗口，利用词之间的共现，训练词的向量表示。而glove捕捉的是全局信息，所谓全局，就是例子中ratio的引入。&lt;/p&gt;

&lt;p&gt;模型的核心， 即&lt;r&gt;损失函数&lt;/r&gt; J：&lt;/p&gt;

&lt;p&gt;$$ J = \sum_{i,j=1}^V f(X_{ij}) (w_i^T \tilde{w_j} + b_i + \tilde{b_j} - logX_{ij})^2 $$&lt;/p&gt;

&lt;p&gt;看上去，与矩阵分解的损失函数有些类似。关于推导过程，这里就不做过多赘述（可阅读论文相关段落）。以笔者的理解，简单的概况如下：&lt;/p&gt;

&lt;p&gt;从一个未知的F函数（即ratio）入手，&lt;/p&gt;

&lt;p&gt;$$ F(w_i,w_j, \tilde{w_k}) = \frac{P_{ik}}{P_{jk}} $$&lt;/p&gt;

&lt;p&gt;发现函数F在一些限制条件（向量线性关系的保持，对称性）下，可以被定义成一种唯一的形式。&lt;/p&gt;

&lt;p&gt;$$ F(w_i^T\tilde{w_k}) = \frac{X_{ik}}{X_{i}} $$&lt;/p&gt;

&lt;p&gt;其中F = exp， 可以推导出：\( w_i^T\tilde{w_k} + b_i + \tilde{b_k} = log(X_{ik}) \)&lt;/p&gt;

&lt;p&gt;加入权重因子的考虑，这也就如愿得到了上面的损失函数。&lt;/p&gt;

&lt;p&gt;&lt;r&gt;权重因子&lt;/r&gt;：使低频的词共现，高频的词共现都能不被overweighted。文中给出了如下的定义：&lt;/p&gt;

&lt;p&gt;$$
f(x)=\begin {cases}
(x/x_{max})^\alpha, &amp;amp; x&amp;lt;x_{max} \\
1, &amp;amp; x\geq{x_{max}}
\end {cases}
$$&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>