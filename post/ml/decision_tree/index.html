
<!DOCTYPE html>
<html lang="en-us">
<head>

  
  <meta charset="UTF-8">
  <title>
    [ml] Decision Tree模型 | Learn and record
  </title>


  
  <meta name="viewport" content="width=device-width,user-scalable=no,maximum-scale=1,initial-scale=1">

  
  <link rel="canonical" href="http://cherishzhang.github.io/post/ml/decision_tree/"/>

  
  <link rel="stylesheet" href="/css/sanitize.css">
  <link rel="stylesheet" href="/css/responsive.css">
  <link rel="stylesheet" href="/css/highlight_monokai.css">
  <link rel="stylesheet" href="/css/theme.css">
  <link rel="stylesheet" href="/css/custom.css">

  
  <link href="http://cherishzhang.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Learn and record" />
  <link href="http://cherishzhang.github.io/index.xml" rel="feed" type="application/rss+xml" title="Learn and record" />

  
  
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-84092408-1', 'auto');
  ga('send', 'pageview');

</script>
</head>



<body>
<div class="container">

  
  <header role="banner">
    <div class="row gutters">
      <div id="site-title" class="col span_6">
        <h1><a href="http://cherishzhang.github.io/">Learn and record</a></h1>
        
      </div>
      
      <img src="/images/avatar.jpg" width="64" height="64" style="margin-left:30px;">
      
      <div id="social" class="col span_4">
        <ul>
          <li><a href="http://cherishzhang.github.io/about" target="_blank">About me</a></li>
          <li><a href="https://github.com/Cherishzhang" target="_blank">GitHub</a></li>
          <li><a href="mailto:cherishzhang920508@gmail.com" target="_blank">Email</a></li>
        </ul>
      </div>
  </header>


  
  <main id="single" role="main">
    <div class="article-header">
      <h1>[ml] Decision Tree模型</h1>
      <div class="meta" >
        Aug 1, 2016 &nbsp;
        
          #<a href="/tags/machine-learning">machine learning</a>&nbsp;
        
          #<a href="/tags/tree">tree</a>&nbsp;
        
          #<a href="/tags/cart">cart</a>&nbsp;
        
      </div>
    </div>
    <article style="padding:30px; border: 2px solid;border-radius: 4px; border-color: #ddd;">
      

<h2 id="1-intro:998e3d52f8819c2e5f687d19cc26909b">1.Intro</h2>

<p>决策树常用的算法有<r>ID3</r>,<r>C4.5</r>和<r>CART</r>，学习过程分为三部分：<r>特征选择</r>，<r>决策树生成</r>和<r>决策树剪枝</r>。</p>

<p>决策树的<r>生成</r>只考虑局部最优（<r>贪心算法</r>），而决策树的<r>剪枝</r>则考虑<r>全局最优</r>。简而言之，这是<strong>一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对每个子数据集有一个最好的分类过程</strong>。</p>

<p>They are easy to <r>interpret</r>, <r>handle categorical features</r>, extend to the <r>multiclass classification setting</r>, do not require <r>feature scaling</r>, and are able to <r>capture non-linearities and feature interactions</r>.</p>

<h2 id="2-准则函数:998e3d52f8819c2e5f687d19cc26909b">2.准则函数</h2>

<p>主要用来衡量节点数据集合的有序性，有<r>熵(Entropy)</r>，<r>基尼指数(Gini)</r>，<r>方差(Variance)</r>，其中前两种针对分类问题，而方差针对回归问题。寻找最好的分割点是通过量化<strong>分割后</strong>类的纯度来确定的，计算方式总结如下表：</p>

<table>
<thead>
<tr>
<th>Impurity</th>
<th>Task</th>
<th>Description</th>
<th>formula</th>
</tr>
</thead>

<tbody>
<tr>
<td>熵</td>
<td>分类</td>
<td>假设有K类，样本点属于第i类的概率为\(p_i\)，度量数据的不确定性</td>
<td>\(E(D) =\sum_{i=1}^{K} -p_ilogp_i\)</td>
</tr>

<tr>
<td>基尼</td>
<td>分类</td>
<td>同上</td>
<td>\(Gini(D) = \sum_{i=1}^{K} p_i*(1-p_i)\)</td>
</tr>

<tr>
<td>方差</td>
<td>回归</td>
<td>度量数据的离散程度</td>
<td>\(Variance(D) = \frac{1}{N} \sum_{i=1}^N (y_i-\mu)^2\),\(\mu = \frac{1}{N} \sum_{i=1}^N y_i\)</td>
</tr>
</tbody>
</table>

<p><strong>信息增益</strong>: 得知某特征X的信息而使目标Y的信息<r>不确定减少</r>的程度。信息增益大的特征有更强的分类能力。</p>

<p><strong>具体方法</strong>：对训练集D，计算每个特征的信息增益，比较大小，选择信息增益大的特征列来分割集合。假设分割点将数据集分成左右两部分，则信息增益为 \( IG(D,s) = Impurity(D) - \frac{N_{left}}{N} Impurity(D_{left}) - \frac{N_{right}}{N} Impurity(D_{right})\)</p>

<p>ID3s算法选择信息增益作分类，为了校正信息增益存在“偏向选择取值较多的特征”的缺点，C4.5采用<r>信息增益比</r>\(IG_R(D,s) = \frac{IG(D,s)}{E_s(D)}\), \(E_s(D)= \sum_{i=1}^K -\frac{|D_i|}{|D|}*log_2(\frac{|D_i|}{|D|})\), 其中K是特征s的取值个数。</p>

<h2 id="3-生成算法:998e3d52f8819c2e5f687d19cc26909b">3 生成算法</h2>

<p><strong>具体方法</strong>：从根节点开始，计算所有可能的特征的准则值(应用准则函数)，选择最优的特征作为节点的特征，由该特征的不同取值建立子节点；再对子节点递归调用以上方法，构建决策树。</p>

<h4 id="cart:998e3d52f8819c2e5f687d19cc26909b">CART</h4>

<p>CART(classification and regression tree)，又称分类与回归树。</p>

<p><strong>CART的生成是一个递归地构建<r>二叉树</r>的过程</strong>，对回归树应用平方误差最小化准则，对分类树用Gini指数最小化准则，进行特征选择，生成二叉树。预测时，先根据样本特征判断该样本点位于哪块区域，然后用该区域内的训练样本点集的<r>众数</r> (分类) 或<r>均值</r> (回归) 作为该样本点的预测值。</p>

<p><strong>算法描述</strong>：CART树生成算法<br/>
<strong>输入</strong>: 训练数据集D，停止条件<br/>
<strong>输出</strong>: 决策树f(x)<br/>
<strong>(1)</strong>在平方误差最小或Gini最小的准则下选择<r>最优切分变量j与切分点s</r>，切分变量从可用特征集中选择。
回归树中，单元\(R_m\)下的最优值是\(R_m\)上所有输入实例对应的输出\(y_i\)的均值，即切分点。分类中，切分点是该变量所有可能的取值。<br/>
<strong>(2)</strong>用选定的对(j,s)划分区域并决定相应的输出值<br/>
<strong>(3)</strong>继续对子区域调用(1),(2),直至满足停止条件<br/>
<strong>(4)</strong>将输入空间划分为M个区域\(R_1\),\(R_2\),&hellip;,\(R_M\).生成如下决策树：<br/></p>

<p>$$f(x) = \sum_{m=1}^{M} c_m*I(x \in R_m)$$</p>

<p>算法的<r>停止条件</r>是节点中的样本个数小于预定阈值，或样本集的基尼指数小于预订阈值（样本基本属于同一类），或没有更多特征。</p>

<h2 id="4-剪枝过程-pruning:998e3d52f8819c2e5f687d19cc26909b">4 剪枝过程(pruning)</h2>

<p>将已生成的树进行简化的过程，通过极小化决策树整体的损失函数（loss function）来实现。 分为前向剪枝和后向剪枝。前向剪枝是在构造决策树的同时对树进行剪枝；后向剪枝是在决策树构建完成后，对树从根节点向上递归剪枝。</p>

<h2 id="5-mllib实现:998e3d52f8819c2e5f687d19cc26909b">5.MLlib实现</h2>

<h4 id="连续值特征:998e3d52f8819c2e5f687d19cc26909b">连续值特征</h4>

<p>对于连续值类型的特征，单台机器上的做法是选取所有出现过的值作为切分点候选集。加速计算的方法是对该集合按值大小排序，用排序好的数组做候选集。</p>

<p>在分布式中，对于规模大的数据集，排序是比较耗时的。于是先<r>对整体数据按比例采样</r>，然后<r>采用分位数(等频分箱)作为分割点的近似候选集</r>(<gr><strong>优化点</strong></gr>)。</p>

<p><r>采样策略</r>为选取样本集中的<strong><r>max</r>(maxBins*maxBins, 10000)</strong>条记录。<r>等频分箱策略</r>有三种(<strong>Sort</strong>,MinMax,ApproxHist)，目前只支持第一种。规定分箱总数不能超过训练样本的实例数(maxBins的默认值为32)。</p>

<h4 id="类别值特征:998e3d52f8819c2e5f687d19cc26909b">类别值特征</h4>

<p>对于离散值类型的特征，有m个值，那么最多产生\(2^{m-1}-1\)种划分可能，\(2*(2^{m-1}-1)\)个分箱。对于二分类或回归问题，可以将划分候选缩减到m-1种可能。</p>

<p>举个例子，在二分类问题中，一个特征有A,B,C三个值。特征值为A的样本集中label是1的比例为0.2，B和C分别为0.6，0.4。则产生了一个类别值的有序序列ACB，共有两种划分可能：A|C,B，A,C|B。</p>

<p>多分类问题中，离散特征会产生\(2^{m-1}-1\)种划分可能，当划分数大于maxBins，将m个类别值根据impurity排序，产生共m-1种划分可能。</p>

<h4 id="bin和split:998e3d52f8819c2e5f687d19cc26909b">bin和split</h4>

<p>split是划分点，bin是划分区间。每个特征都分别有一组bin和split。</p>

<p>其中bin中<r>预先计算样本的统计信息</r>，节省计算开销(<gr><strong>优化点</strong></gr>)。<strong>分类问题</strong>，bin中保存训练样本个数 (count) 以及各 label 数目 (count for label)；<strong>回归问题</strong>，bin中保存训练样本个数 (count)、 label 之和 (sum) 以及 label 的平方和 (squared sum)。</p>

<p>特征选择时，对于某个特征，计算不同切分变量下信息增益的大小，确定出该特征下的最佳切分点。选择全部候选特征集中最优的切分点作为当前决策。特征选择是树的level级别的并行，对于同一层次的节点，查找可以并行，查找操作的时间复杂度为O(L),L为树的层数(<gr><strong>优化点</strong></gr>)。</p>

<h4 id="终止条件:998e3d52f8819c2e5f687d19cc26909b">终止条件</h4>

<p>为了防止过拟合，采用前向剪枝。当任一以下情况发生，节点就终止划分，形成叶子节点：
(1)树高度达到maxDepth <br/>
(2)minInfoGain，当前节点的所有属性分割带来的信息增益都比这个值要小 <br/>
(3)minInstancesPerNode，需要保证节点分割出的左右子节点的最少的样本数量达到这个值 <br/></p>

<h4 id="源码分析:998e3d52f8819c2e5f687d19cc26909b">源码分析</h4>

<p>下面就一起揭开decision tree的真正面纱吧！</p>

<p><img src="http://ocwpxekrq.bkt.clouddn.com/DecisionTree.png" alt="" /></p>

<p><gr>输入 - RDD[LabelPoint]</gr>：
LabelPoint类有两个属性label(标签值), features(特征向量)。将训练数据集转换为LabelPoint形式，作为决策树训练的输入之一。</p>

<p><gr>输入 - Strategy</gr>:
这是关于模型训练过程中需要指定的参数集。</p>

<p><gr>buildMetadata()</gr>:
DecisionTreeMetadata是一个决策树<r>元数据信息</r>类，其中大部分属性和Strategy类重合。该函数的<r>功能</r>: <strong>对于类别值特征，区分有序/无序；统计类别值特征的划分点数目和分箱数目。</strong>是训练决策树模型的一个准备工作。</p>

<pre><code>def buildMetadata(input:RDD[LabeledPoint], strategy:Strategy,
numTrees: Int, featureSubsetStrategy: String):DecisionTreeMetadata =
{...}
</code></pre>

<p>对于每个类别值特征fi，<r>多分类情况下</r>：首先计算最大可能分箱maxPossibleBins确定情况下，能够允许的最多类别值m，如果fi的类别值超过m，则当作有序处理；否则认为该特征是无序的，并计算分箱数。</p>

<p><r>二分类/回归情况下</r>：当作有序处理，即numBins(fi) = numCategories(fi)。</p>

<p><gr>findSplitsBins</gr>:
这是DecisionTree.scala中一个方法，基于buildMetadata统计的划分点数目和分箱数，加上数据采样，确定每个特征的划分点值。</p>

<pre><code>def findSplitsBins(input:RDD[LabeledPoint],
metadata:DecisionTreeMetadata): (Array[Array[Split]], Array[Array[Bin]]) =
{ ... }
</code></pre>

<p>如果存在连续值特征，则对数据集进行无放回采样，采样个数为max(maxBins*maxBins, 10000)。接着进行分箱操作（见findSplitsBinsBySorting函数，findSplitsForContinuousFeature函数）。</p>

<p><strong>treeRDD,BaggedRDD</strong>:
treeRDD的类型是：RDD[TreePoint],treePoint类有两个属性：label和binnedFeatures。BaggedPoint类有两个属性datum和subsampleWeights(在各个采样后数据集中的权重，即出现次数)。</p>

<p><gr>findBestSplits</gr>:
对于当前节点，计算准则函数下的信息增益，选择最优切分特征和最优切分点。</p>

<p><gr>其他</gr>:</p>

<p><strong>configuration</strong>:</p>

<table>
<thead>
<tr>
<th>类名</th>
<th>枚举值</th>
</tr>
</thead>

<tbody>
<tr>
<td>Algo</td>
<td>Classification, Regression</td>
</tr>

<tr>
<td>FeatureType</td>
<td>Continuous, Categorical</td>
</tr>

<tr>
<td>QuantileStrategy</td>
<td>Sort, MinMax, ApproxHist</td>
</tr>

<tr>
<td>EnsembleCombiningStrategy</td>
<td>Average, Sum, Vote</td>
</tr>
</tbody>
</table>

<p><strong>impurtiy,loss</strong>:</p>

<p>Impurity是准则函数的抽象类，其三个子类分别为Gini,Variance,Entropy。Impurities是Impurity的工厂类，负责解析用户参数，选择对应的准则函数。</p>

<pre><code>private[mllib] object Impurities {
  def fromString(name: String): Impurity = name match {
    case &quot;gini&quot; =&gt; Gini
    case &quot;entropy&quot; =&gt; Entropy
    case &quot;variance&quot; =&gt; Variance
    case _ =&gt; throw new IllegalArgumentException(s&quot;Did not recognize
    Impurity name: $name&quot;)
  }
}
</code></pre>

<p>Loss与Impurity情况类似，它是损失函数的抽象类，其三个子类分别是AbsoluteError,SquaredError,LogLoss。Losses是Loss的工厂类，负责判断损失的所属类别。</p>

<h2 id="参考链接:998e3d52f8819c2e5f687d19cc26909b">参考链接</h2>

<p><a href="http://spark.apache.org/docs/latest/mllib-decision-tree.html">spark mllib - decision tree</a></p>

<p>统计学习方法，李航著</p>


      
    </article>
  </main>

  <nav class="pagination-single">
    
      <span class="previous">&larr; <a href="http://cherishzhang.github.io/post/ml/boosting/" rel="prev">[ml] Boosting</a></span>
    
    
      <span class="next"><a href="http://cherishzhang.github.io/post/ml/word2vec/" rel="next">[ml] word2vec模型和源码解析</a> &rarr;</span>
    
  </nav>
  
 <aside><div id="disqus_thread"></div></aside>

<script type="text/javascript">
     
    var disqus_shortname = 'http-cherishzhang-github-io';

     
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>




  
  <footer role="contentinfo">
    <div style="text-align:center;">
      
    </div>
  </footer>


</div>

<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



</body>
</html>

