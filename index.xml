<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Learn and record</title>
    <link>http://cherishzhang.github.io/</link>
    <description>Recent content on Learn and record</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 08 Aug 2016 19:13:33 +0800</lastBuildDate>
    <atom:link href="http://cherishzhang.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Decision tree</title>
      <link>http://cherishzhang.github.io/post/decision_tree/</link>
      <pubDate>Mon, 08 Aug 2016 19:13:33 +0800</pubDate>
      
      <guid>http://cherishzhang.github.io/post/decision_tree/</guid>
      <description>

&lt;h2 id=&#34;1-intro:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;1.Intro&lt;/h2&gt;

&lt;p&gt;决策树常用的算法有ID3,C4.5和CART，学习过程包括：特征选择，决策树生成和决策树剪枝。决策树的生成只考虑局部最优（贪心算法），而决策树的剪枝则考虑全局最优。决策树学习的算法是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使对每个子数据集有一个最好的分类过程。&lt;/p&gt;

&lt;h2 id=&#34;2-特征选择:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;2.特征选择&lt;/h2&gt;

&lt;p&gt;衡量节点数据集合的有序性，有熵，基尼指数，方差，其中熵和Gini是针对分类的，方差是针对回归的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;熵&lt;/strong&gt;(entropy)：表示随机变量不确定性的度量。X的概率分布为&lt;code&gt;$P(X=x_i） = p_i$&lt;/code&gt;,则X的熵定义为：&lt;code&gt;$H(X) =-\sum_{i=1}^{n} p_ilogp_i$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;条件熵&lt;/strong&gt;(conditional entropy)：随机变量X给定条件下Y的条件概率分布的熵对x的数学期望。定义为：&lt;code&gt;$H(Y|X)=\sum_{i=1}^n p_i H(Y|x=x_i)$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;互信息&lt;/strong&gt;(mutual information)：熵与条件熵之差&lt;/p&gt;

&lt;p&gt;分类问题中，假设有K个类，样本点属于第k类的概率为&lt;code&gt;$p_k$&lt;/code&gt;,则概率分布的&lt;strong&gt;Gini指数&lt;/strong&gt;定义为：
&lt;code&gt;$Gini(p) = \sum_{k=1}^K p_k*(1-p_k) = 1-\sum_{k=1}^K p_k^2$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;如果样本集合D根据特征A是否取某一可能值a被分割成&lt;code&gt;$D_1$&lt;/code&gt;和&lt;code&gt;$D_2$&lt;/code&gt;两部分，则在特征A的条件下，集合D的Gini指数定义为：
&lt;code&gt;$Gini(D,A)=\frac{|D1|}{|D|} Gini(D1) + \frac{|D2|}{|D|} Gini(D2)$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Gini(D)表示集合D的不确定性，Gini(D,A)表示经A分割后集合D的不确定性，基尼指数越大，样本集合的不确定性就越大（与熵类似）。&lt;/p&gt;

&lt;p&gt;寻找最好的分割点是通过量化分割后类的纯度来确定的，目前的三种纯度计算方式总结如下：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Impurity&lt;/th&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;formula&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Entropy&lt;/td&gt;
&lt;td&gt;classification&lt;/td&gt;
&lt;td&gt;$-\sum_{k=1}^{K} p_klogp_k$&lt;/td&gt;
&lt;td&gt;K is the number of unique labels&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Gini&lt;/td&gt;
&lt;td&gt;classification&lt;/td&gt;
&lt;td&gt;$1-\sum_{k=1}^K p_k^2$&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Variance&lt;/td&gt;
&lt;td&gt;regression&lt;/td&gt;
&lt;td&gt;$\frac{1}{N} \sum_{i=1}^N (y_i-\mu)^2$&lt;/td&gt;
&lt;td&gt;\(\mu\)is the mean value, N is the number of instances&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;2-1-信息增益:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;2.1 信息增益&lt;/h3&gt;

&lt;p&gt;定义： 得知特征X的信息而使类Y的信息不确定减少的程度。信息增益大的特征有更强的分类能力。&lt;br/&gt;
计算：&lt;code&gt;$g(D,A) = H(D)-H(D|A)$&lt;/code&gt;；方法： 对训练数据集（或子集）D，计算其每个特征的信息增益，比较大小，选择信息增益大的特征。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;输入：训练特征集D和特征A
输出：特征A对训练数据集D的信息增益g(D,A)
1.计算数据集D的经验熵H(D）
2.计算特征A对数据集D的经验条件熵H(D|A)
3.计算信息增益
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-2-信息增益比:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;2.2 信息增益比&lt;/h3&gt;

&lt;p&gt;以信息增益作为划分训练集的特征，&lt;strong&gt;存在偏向选择取值较多的特征&lt;/strong&gt;，信息增益比可以校正此问题。&lt;/p&gt;

&lt;p&gt;定义：其信息增益g(D,A)与训练数据集D关于特征A的值的熵H(D)之比。&lt;br/&gt;
计算：&lt;code&gt;$g_R(D,A) = \frac{g(D,A)}{H_A(D)}$&lt;/code&gt;. 其中&lt;code&gt;$H_A(D)=-\sum_{i=1}^{n} \frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$&lt;/code&gt;,n是特征A的取值个数。&lt;/p&gt;

&lt;h2 id=&#34;3-生成算法:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;3 生成算法&lt;/h2&gt;

&lt;h3 id=&#34;3-1-id3:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;3.1 ID3&lt;/h3&gt;

&lt;p&gt;核心是在应用&lt;strong&gt;信息增益准则&lt;/strong&gt;选择特征，递归地构建决策树。&lt;strong&gt;具体方法&lt;/strong&gt;：从根节点开始，计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，由该特征的不同取值建立子节点；再对子节点递归调用以上方法，构建决策树。&lt;strong&gt;结束条件&lt;/strong&gt;：直到所有特征的信息增益均很小或没有特征可选择为止。&lt;/p&gt;

&lt;p&gt;ID3算法只有树的生成，容易产生过拟合。&lt;/p&gt;

&lt;h3 id=&#34;3-2-c4-5:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;3.2 C4.5&lt;/h3&gt;

&lt;p&gt;C4.5对ID3进行了改进，应用&lt;strong&gt;信息增益比&lt;/strong&gt;选择特征。&lt;/p&gt;

&lt;h3 id=&#34;3-3-cart:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;3.3 CART&lt;/h3&gt;

&lt;p&gt;分类与回归树(classification and regression tree)由特征选择，树的生成及剪枝组成。决策树的生成是递归地构建二叉树的过程，对回归树用平方误差最小化准则，对分类树用Gini指数最小化准则，进行特征选择，生成二叉树。预测时，先根据样本特征判断该样本点位于哪块区域，然后用该区域内的训练样本点集的众数 (分类) 或均值 (回归) 作为该样本点的预测值。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;生成算法&lt;/strong&gt;:&lt;br/&gt;
&lt;font color=&#34;F08080&#34;&gt;输入&lt;/font&gt;: 训练数据集D &lt;br/&gt;
&lt;font color=&#34;F08080&#34;&gt;输出&lt;/font&gt;: 决策树f(x) &lt;br/&gt;
(1)在平方误差最小或Gini最小的准则下选择最优切分变量j与切分点s，切分变量从可用特征集中选择，回归树中，单元&lt;code&gt;$R_m$&lt;/code&gt;下的最优值是&lt;code&gt;$R_m$&lt;/code&gt;上所有输入实例对应的输出&lt;code&gt;$y_i$&lt;/code&gt;的均值，即切分点。分类中，切分点是该变量所有可能的取值。&lt;br/&gt;
(2)用选定的对(j,s)划分区域并决定相应的输出值&lt;br/&gt;
(3)继续对子区域调用(1),(2),直至满足停止条件&lt;br/&gt;
(4)将输入空间划分为M个区域&lt;code&gt;$R_1,R_2,...,R_M$&lt;/code&gt;，生成决策树：&lt;code&gt;$f(x)=\sum_{m=1}^{M}c_mI(x \in R_m)$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;算法的&lt;strong&gt;停止条件&lt;/strong&gt;是节点中的样本个数小于预定阈值，或样本集的基尼指数小于预订阈值（样本基本属于同一类），或没有更多特征。&lt;/p&gt;

&lt;h2 id=&#34;4-剪枝过程-pruning:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;4 剪枝过程(pruning)&lt;/h2&gt;

&lt;p&gt;将已生成的树进行简化的过程，通过极小化决策树整体的损失函数（loss function）来实现。 分为前向剪枝和后向剪枝。&lt;/p&gt;

&lt;h2 id=&#34;5-mllib实现:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;5.MLlib实现&lt;/h2&gt;

&lt;h3 id=&#34;5-1-bin和split:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;5.1 bin和split&lt;/h3&gt;

&lt;p&gt;对于离散的feature,如果有m个值，最多有\(2^{m-1}-1\)个划分，如果是有序的，则有m-1个划分；对于连续的feature，进行范围划分。其中划分的点是split，而划分的区间是bin。训练时，两个重要的参数maxDepth（树的最大高度），maxBins（每个特征分裂时，最大划分桶数量）。&lt;/p&gt;

&lt;p&gt;bin中预先计算样本的统计信息，节省计算开销。&lt;strong&gt;分类问题&lt;/strong&gt;，bin中保存训练样本个数 (count) 以及各 label 数目 (count for label)；&lt;strong&gt;回归问题&lt;/strong&gt;，bin中保存训练样本个数 (count)、 label 之和 (sum) 以及 label 的平方和 (squared sum)。&lt;/p&gt;

&lt;p&gt;特征选择时，统计同一分区下，选择不同切分变量时，对应bin中的信息（ImpurityAggregator），合并所有分区，确定最佳切分变量和切分点，从而实现并行化。&lt;/p&gt;

&lt;p&gt;为了防止过拟合，采用前向剪枝。当任一以下情况发生，节点就终止划分，形成叶子节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(1)树高度达到maxDepth
(2)minInfoGain，当前节点的所有属性分割带来的信息增益都比这个值要小
(3)minInstancesPerNode，需要保证节点分割出的左右子节点的最少的样本数量达到这个值
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-2-关键优化点:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;5.2 关键优化点：&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;查找最优split，是树的level级别的并行，处于同一层次的节点，查找是并行的。查找次数的时间复杂度为O(L）,其中L是树的层数。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;连续特征，是用排序后的唯一特征值集合作为计算最佳split的候选，对于大规模数据，为了减小开销，采用分位数（等频分箱）作为split的候选。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;统计信息在bin中计算，节省开销。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;5-3-源码分析:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;5.3 源码分析&lt;/h3&gt;

&lt;p&gt;类：CARTPoint(样本点),TreeNode(树结点)&lt;/p&gt;

&lt;p&gt;从根节点开始，对所有特征，从0开始编号，n_fs是全部特征数。统计所有特征所有分箱结果，对应每个样本集，计算该样本，每个特征对应的特征值会被分配到的箱子id。&lt;/p&gt;

&lt;p&gt;统计属于该节点的训练样本集（总样本数，各类别下的样本数，各类别对应的索引，类别总数）；根据统计信息计算当前节点的信息不确定性(Gini或Varience),并给出该节点的预测值（众数或平均值），根据以上信息更新当前树节点的信息，包括节点ID，预测类别ID，所在深度，样本数，特征总数，列采样率，该节点的Gini或Variance,里面包括一系列函数，判断是否是叶节点，分割方式，左右孩子，采样后的特征等。&lt;/p&gt;

&lt;p&gt;决策树生成的过程，用队列Queue实现，将根节点插入队列，每次取队列中的全部节点（层级并行化），并行处理，作为当前节点，计算当前节点的最好分割，将其左右孩子插入队列中。&lt;/p&gt;

&lt;p&gt;并行是通过mapPartitions(foreach, update), reduceByKey(),然后对aggs后的信息进行findBestSplits&lt;/p&gt;

&lt;p&gt;每个待分裂节点，都根据其父节点而初始化，全部特征，每个特征对应的分箱下的n个状态，初始化为0.0&lt;/p&gt;

&lt;h3 id=&#34;6-scikit-learn:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;6.Scikit-learn&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Random Forest</title>
      <link>http://cherishzhang.github.io/post/ml/random_forest/</link>
      <pubDate>Wed, 03 Aug 2016 16:15:24 +0800</pubDate>
      
      <guid>http://cherishzhang.github.io/post/ml/random_forest/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://www.cnblogs.com/hrlnw/p/3850459.html&#34;&gt;http://www.cnblogs.com/hrlnw/p/3850459.html&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-intro:00635f635026fbac82e8a20eddd81309&#34;&gt;1. Intro&lt;/h2&gt;

&lt;p&gt;基本原理：由多个决策树构成的森林，算法分类结果由这些决策树投票得到，决策树在生成的过程当中分别在行方向和列方向上添加随机过程，行方向上构建决策树时采用放回抽样（bootstraping）得到训练数据，列方向上采用无放回随机抽样得到特征子集，并据此得到其最优切分点。随机森林通过多个决策树投票结果进行分类，算法不容易出现过度拟合问题。&lt;/p&gt;

&lt;h2 id=&#34;2-construct:00635f635026fbac82e8a20eddd81309&#34;&gt;2. Construct&lt;/h2&gt;

&lt;p&gt;分为两方面:&lt;strong&gt;&lt;font color=#F08080&gt;数据&lt;/font&gt;&lt;/strong&gt;的随机性选取，以及&lt;strong&gt;&lt;font color=#F08080&gt;待选特征&lt;/font&gt;&lt;/strong&gt;的随机选取。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;数据&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;从原始数据集中*有放回*的抽样，构造子数据集，子数据集的数据量和原始数据集相同。&lt;/li&gt;
&lt;li&gt;利用子数据集构建子决策树&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;待选特征&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在子树的每一个分裂过程中并未用到所有的待选特征。&lt;/li&gt;
&lt;li&gt;从原始特征无放回随机抽取一定比例的特征，用所抽取的特征集作特征选择。&lt;/li&gt;
&lt;li&gt;假设总的特征数量是M，则比例可为sqrt(M), 1/2sqrt(M), 2sqrt(M)。&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;训练过程：&lt;br/&gt;
input: 给定训练集S，测试集T，特征维数F。确定参数集(CART的数量t，每棵树的深度d，每个节点用到的特征数量f，终止条件：节点上最少的样本数，节点上最少的信息增益m)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于i = 1,2,&amp;hellip;,t;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;(1) 从S中有放回的抽样，生成一个大小和S相同的训练集S(i)，作为根节点的样本集，从根节点开始训练。&lt;br/&gt;
(2) a. 如果当前节点满足终止条件，则设置当前节点为叶子节点。&lt;/p&gt;

&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;分类问题，该叶子节点的预测输出为当前样本集和中数量最多的那一类C(j),概率p为c(j)占当前样本集的比例；&lt;/li&gt;
&lt;li&gt;回归问题，预测输出为当前样本集所有样本的平均值。&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;(2) b. 如果不满足终止条件，则从F维特征中无放回的随机选取f维特征。利用这f维特征，寻找分类效果最好的那一维特征k及阈值th。当前节点上样本第k维特征小于th的样本被划分到左节点，其余的被划分到右节点。&lt;/p&gt;

&lt;p&gt;(3) 重复(1)(2)直到所有节点都被训练过或标记为叶子节点。 &lt;br/&gt;
(4) 重复(1)(2)(3)直到所有的决策树被训练过。 &lt;br/&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;预测过程：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于i = 1,2,&amp;hellip;,t;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;(1) 从当前树的根节点开始，根据节点对应阈值th，判断是进入左节点还是右节点，直到到达某个叶子节点，并输出预测值。&lt;br/&gt;
(2) 重复执行(1)，直到得到所有树的预测值。&lt;strong&gt;分类问题&lt;/strong&gt;，则输出所有树中预测概率总和最大的那一类，即对每个c(j)的p进行累计；&lt;strong&gt;回归问题&lt;/strong&gt;，则输出为所有树的平均预测值。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;3-evaluation:00635f635026fbac82e8a20eddd81309&#34;&gt;3. Evaluation&lt;/h2&gt;

&lt;p&gt;rf无需交叉验证来评价分类的准确性，本身自带OOB（out-of-bag）错误估计。&lt;/p&gt;

&lt;h2 id=&#34;4-spark-mllib:00635f635026fbac82e8a20eddd81309&#34;&gt;4. Spark MLlib&lt;/h2&gt;

&lt;p&gt;(1)切分点抽样统计&lt;/p&gt;

&lt;p&gt;在单机环境下的决策树对连续变量进行切分点选择时，一般是通过对特征点进行排序，然后取相邻两个数之间的点作为切分点，这在单机环境下是可行的，但如果在分布式环境下如此操作的话，会带来大量的网络传输操作，特别是当数据量达到 PB 级时，算法效率将极为低下。为避免该问题，Spark 中的随机森林在构建决策树时，会对各分区采用一定的子特征策略进行抽样，然后生成各个分区的统计数据，并最终得到切分点。&lt;/p&gt;

&lt;p&gt;(2)特征装箱（Binning）&lt;/p&gt;

&lt;p&gt;决策树的构建过程就是对特征的取值不断进行划分的过程，对于离散的特征，如果有 M 个值，最多\(2^M-1\)个划分，如果值是有序的，那么就最多 M-1 个划分。比如年龄特征，有老，中，少 3 个值，如果无序有\(2^M-1\)个，即 3 种划分：老|中，少；老，中|少；老，少|中；如果是有序的，即按老，中，少的序，那么只有 m-1 个，即 2 种划分，老|中，少；老，中|少。对于连续的特征，其实就是进行范围划分，而划分的点就是 split（切分点），划分出的区间就是 bin。对于连续特征，理论上 split 是无数的，在分布环境下不可能取出所有的值，因此它采用的是（1）中的切点抽样统计方法。&lt;/p&gt;

&lt;p&gt;(3)逐层训练（level-wise training）&lt;/p&gt;

&lt;p&gt;单机版本的决策数生成过程是通过递归调用（本质上是深度优先）的方式构造树，在构造树的同时，需要移动数据，将同一个子节点的数据移动到一起。此方法在分布式数据结构上无法有效的执行，而且也无法执行，因为数据太大，无法放在一起，所以在分布式环境下采用的策略是逐层构建树节点（本质上是广度优先），这样遍历所有数据的次数等于所有树中的最大层数。每次遍历时，只需要计算每个节点所有切分点统计参数，遍历完后，根据节点的特征划分，决定是否切分，以及如何切分。&lt;/p&gt;

&lt;h2 id=&#34;5-codes:00635f635026fbac82e8a20eddd81309&#34;&gt;5. Codes&lt;/h2&gt;

&lt;p&gt;参考sklearn.ensemble.RandomForestRegressor，sklearn.ensemble.RandomForestClassifier&lt;/p&gt;

&lt;p&gt;输入参数&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;名称&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;n_estimators&lt;/td&gt;
&lt;td&gt;一个森林中决策树的个数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;criterion&lt;/td&gt;
&lt;td&gt;分类准则，如&amp;rsquo;rmse&amp;rsquo;,&amp;lsquo;gini&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;max_depth&lt;/td&gt;
&lt;td&gt;树的最大深度，tree-specific&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;min_samples_split&lt;/td&gt;
&lt;td&gt;中间节点分裂前的最少样本数，tree-specific&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;min_samples_leaf&lt;/td&gt;
&lt;td&gt;新生成叶节点上的最少样本数，tree-specific&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;min_weight_fraction_leaf&lt;/td&gt;
&lt;td&gt;叶节点上样本数占全部样本的最小比例值，tree-specific&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;max_features&lt;/td&gt;
&lt;td&gt;每个节点用到的特征数量，tree-specific&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;max_leaf_nodes&lt;/td&gt;
&lt;td&gt;每棵树中的叶节点能达到的最大值，tree-specific&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;bootstrap&lt;/td&gt;
&lt;td&gt;是否构建决策树过程中，对样本进行bootstrap&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;oob_score&lt;/td&gt;
&lt;td&gt;是否使用out-of-bag样本估计泛化误差&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;n_jobs&lt;/td&gt;
&lt;td&gt;并行运行的进程数，默认等于CPU的核数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;random_state&lt;/td&gt;
&lt;td&gt;随机生成器的种子&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;verbose&lt;/td&gt;
&lt;td&gt;是否输出构建过程中的信息&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;warm_start&lt;/td&gt;
&lt;td&gt;设置为是，则使用上次拟合后的solution，再附加决策树，否则，拟合一个新的森林&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;class_weight&lt;/td&gt;
&lt;td&gt;类别权重列表&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;模型属性&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;名称&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;estimators_&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;classes_&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;n&lt;em&gt;classes&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;n&lt;em&gt;features&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;n_outputs&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;feature&lt;em&gt;importances&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;oob_score&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Boosting</title>
      <link>http://cherishzhang.github.io/post/ml/boosting/</link>
      <pubDate>Fri, 20 May 2016 19:20:56 +0800</pubDate>
      
      <guid>http://cherishzhang.github.io/post/ml/boosting/</guid>
      <description>

&lt;h2 id=&#34;intro:d21755d9d058d9a9b44db3100ac0dea8&#34;&gt;Intro&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_boosting&#34;&gt;Boosting&lt;/a&gt; can be interpreted as an optimization algorithm on a suitable cost function. The algorithms that optimize a cost function over function space by iteratively choosing a function(week hypothesis) that points in the negative gradient direction.&lt;/p&gt;

&lt;p&gt;在分类问题中，提升(Boosting)思想，通过改变训练数据的概率分布（权重分布），针对不同的训练数据分布，学习多个弱分类器，将这些分类器线性组合，提高分类的性能。&lt;/p&gt;

&lt;p&gt;围绕提升方法，有&lt;strong&gt;&lt;font color=#F08080&gt;两个问题&lt;/font&gt;&lt;/strong&gt;需要考虑：1.每一轮如何改变训练数据的概率或权值分布；2.如何将弱分类器组合成一个强分类器。&lt;/p&gt;

&lt;h2 id=&#34;adaboost-adaptive-boosting:d21755d9d058d9a9b44db3100ac0dea8&#34;&gt;AdaBoost(Adaptive Boosting)&lt;/h2&gt;

&lt;p&gt;AdaBoost提高那些被前几轮弱分类器线性组成的分类器&lt;strong&gt;错误分类&lt;/strong&gt;的样本权值，在下一轮的弱分类器中更加关注没有得到正确分类的数据；采用&lt;strong&gt;加权多数表决&lt;/strong&gt;的方法，误差率小的弱分类器权值大，误差率大的弱
分类器权值小。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;算法流程&lt;/strong&gt;：（二分类问题）&lt;br/&gt;
&lt;font color=#F08080&gt;输入&lt;/font&gt;：训练数据集$T={(x_1,y_1),(x_2,y_2),&amp;hellip;,(x_N,y_N)}$和弱分类器算法 &lt;br/&gt;
&lt;font color=#F08080&gt;输出&lt;/font&gt;：最终分类器&lt;code&gt;$G(x)$&lt;/code&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;初始化训练样本的权值分布(均匀分布)&lt;br/&gt;
&lt;code&gt;$D_1=(w_{11},...,w_{1i},...,w_{1N}), w_{1i}=\frac{1}{N}, i=1,2,...,N$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;对于$m=1,2,&amp;hellip;,M$(迭代)

&lt;ul&gt;
&lt;li&gt;使用具有权值分布$D_m$的训练数据集学习，得到基本分类器$G_m(x)$&lt;/li&gt;
&lt;li&gt;计算$G_m(x)$在权值分布$D_m$的训练数据集上的&lt;strong&gt;分类误差率&lt;/strong&gt;
&lt;code&gt;$e_m = P(G_m(x_i) \neq y_i) = \sum_{i=1}^N w_{mi}I(G_m(x_i) \neq y_i)$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;计算$G_m(x)$的系数，即&lt;font color=#F08080&gt;表决权&lt;/font&gt;
&lt;code&gt;$\alpha_{m} = \frac{1}{2} \log \frac{1-e_m}{e_m}$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;更新训练数据集的权值分布
&lt;code&gt;$D_{m+1} = (w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N})$&lt;/code&gt;
&lt;code&gt;$w_{m+1,i} = \frac{w_{mi}}{Z_m} \exp (-\alpha_m y_i G_m(x_i))$&lt;/code&gt;&lt;br/&gt;
其中$Z_m$是规范化因子,&lt;code&gt;$Z_m = \sum_{i=1}^N w_{mi} exp(-\alpha_m y_i G_m(x_i))$&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;构建基本分类器的线性组合
$$f(x) = \sum_{m=1}^M \alpha_m G_m(x)$$&lt;/li&gt;
&lt;li&gt;得到最终的分类器
&lt;code&gt;$G(x) = sign(f(x))$&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;每轮迭代过程中，Adaboost会重点关注错误分类的样本，使得算法受噪声和离群点的影响较大。&lt;code&gt;$\alpha_m$&lt;/code&gt;随着 &lt;code&gt;$e_m$&lt;/code&gt; 的减小而增大，&lt;code&gt;$w_{mi}$&lt;/code&gt; 也随着&lt;code&gt;$\alpha_m$&lt;/code&gt; 而改变，不断增大误分类样本的权值，减小正确分类样本的权值。&lt;/p&gt;

&lt;p&gt;AdaBoost的一个解释是，模型为&lt;strong&gt;加法模型，损失函数为指数函数，学习算法为前向分步算法的二分类学习方法&lt;/strong&gt;。前向分步算法(forward stagewise algorithm)求解优化问题的思路是：从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式，就可以简化优化的复杂度。&lt;/p&gt;

&lt;h2 id=&#34;boosting-tree-提升树:d21755d9d058d9a9b44db3100ac0dea8&#34;&gt;Boosting Tree(提升树)&lt;/h2&gt;

&lt;p&gt;采用&lt;strong&gt;加法模型(基函数的线性组合)与前向分步算法，以决策树为基函数的提升方法&lt;/strong&gt;称为提升树，分类问题采用二叉分类树，回归问题采用二叉回归树。&lt;/p&gt;

&lt;p&gt;提升树表示为决策树的加法模型:
&lt;code&gt;$f_{M(x)} = \sum_{m=1}^M T(x; \Theta_m)$&lt;/code&gt;
其中，&lt;code&gt;$T(x;\Theta_m)$&lt;/code&gt;表示决策树，&lt;code&gt;$\Theta_m$&lt;/code&gt;为决策树的参数，M为树的个数&lt;/p&gt;

&lt;p&gt;不同问题的提升树学习算法，&lt;font color=#F08080&gt;主要区别在于使用的损失函数不同&lt;/font&gt;，包括用平方误差损失函数的回归问题，用指数损失函数的分类问题，以及用一般损失函数的一般决策问题。将Adaboost算法中的基本分类器限制为分类决策树，即为针对二分类问题的提升树算法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;提升树算法&lt;/strong&gt;(回归问题):&lt;br/&gt;
&lt;font color=#F08080&gt;输入&lt;/font&gt;:训练数据集\(T={(x_1,y_1),(x_2,y_2),&amp;hellip;,(x_N,y_N)}\)
&lt;br/&gt;
&lt;font color=#F08080&gt;输出&lt;/font&gt;:提升树$f_M(x)$&lt;br/&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;确定初始提升树&lt;code&gt;$f_0(x) = 0$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;对m = 1,2,&amp;hellip;,M

&lt;ul&gt;
&lt;li&gt;计算残差： &lt;code&gt;$r_{mi} = y_{i} - f_{m-1}(x_i), i=1,2,...,N$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;拟合残差，学习一个回归树，得到&lt;code&gt;$T(x;\Theta_m)$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;更新&lt;code&gt;$f_m(x)=f_{m-1}(x)+T(x;\Theta_m)$&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;得到最终提升树：&lt;code&gt;$f_M(x)=\sum_{m=1}^M T(x;\Theta_m)$&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;gradient-boosting-梯度提升:d21755d9d058d9a9b44db3100ac0dea8&#34;&gt;Gradient Boosting(梯度提升)&lt;/h2&gt;

&lt;p&gt;前向分布算法针对特殊的损失函数(平方和指数)，对于一般的损失函数，Freidman提出了梯度提升算法。它利用最速下降法的近似方法，其&lt;font color=#F08080&gt;&lt;strong&gt;关键&lt;/strong&gt;&lt;/font&gt;是利用损失函数的负梯度在当前模型的值最为回归问题提升树算法中的残差的近似值，拟合一个回归树。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;梯度提升算法&lt;/strong&gt;：&lt;br/&gt;
&lt;font color=#F08080&gt;输入&lt;/font&gt;:训练数据集\(T={(x_1,y_1),(x_2,y_2),&amp;hellip;,(x_N,y_N)}\)，损失函数$L(y,f(x))$&lt;br/&gt;
&lt;font color=#F08080&gt;输出&lt;/font&gt;:回归树$\hat{f} (x)$&lt;br\&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;初始化，估计是损失函数极小化的常数值，即只有一个根节点的树
&lt;code&gt;$f_0(x) = arg\mathop{min}\limits_{c} \sum_{i=1}^{N} L(y_i,c)$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;对m=1,2,&amp;hellip;,M

&lt;ul&gt;
&lt;li&gt;对i=1,2,&amp;hellip;,N,计算
&lt;code&gt;$r_{mi} = - {\left [\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)} \right]}_{f(x)=f_{m-1}(x)}$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;对于&lt;code&gt;$r_{mi}$&lt;/code&gt;拟合一个回归树，得到第m棵树的叶节点区域$R_{mj}$, j=1,2,&amp;hellip;,J（对于平方损失函数，就是通常所说的残差，对于一般损失函数，即为残差的近似值）&lt;/li&gt;
&lt;li&gt;对于j=1,2,&amp;hellip;,J,计算 &lt;code&gt;$c_{mj} = arg\mathop{min}\limits_{c} \sum_{x_i \in R_{mj}} L(y_i,f_{m-1}(x_i)+c)$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;更新&lt;code&gt;$f_m(x) = f_{m-1}(x)+\sum_{j=1}^J c_{mj}I(x\in R_mj)$&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;得到回归树 &lt;code&gt;$\hat{f}(x) = f_M(x) = \sum_{m=1}^M \sum_{j=1}^J c_{mj} I(x \in R_{mj})$&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>c&#43;&#43;_tips</title>
      <link>http://cherishzhang.github.io/post/cplus/c&#43;&#43;_tips/</link>
      <pubDate>Fri, 20 May 2016 17:04:03 +0800</pubDate>
      
      <guid>http://cherishzhang.github.io/post/cplus/c&#43;&#43;_tips/</guid>
      <description>

&lt;h2 id=&#34;summary-list:09633c670eef7b50cb8981f7657aaf97&#34;&gt;summary list&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://cherishzhang.github.io/post/template&#34;&gt;template&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cherishzhang.github.io/post/friend&#34;&gt;friend&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cherishzhang.github.io/post/ccode_tips.md&#34;&gt;code技巧点&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Data Quality in Recommendation</title>
      <link>http://cherishzhang.github.io/post/paper/Data%20Quality%20in%20Recommendation/</link>
      <pubDate>Wed, 16 Mar 2016 21:03:56 +0800</pubDate>
      
      <guid>http://cherishzhang.github.io/post/paper/Data%20Quality%20in%20Recommendation/</guid>
      <description>

&lt;h2 id=&#34;data-quality-matters-in-recommender-systems:c7e7f190dc4ad9c3cea8a960f18951d1&#34;&gt;Data Quality Matters in Recommender Systems&lt;/h2&gt;

&lt;h3 id=&#34;摘要:c7e7f190dc4ad9c3cea8a960f18951d1&#34;&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;数据质量好坏在信息系统中是一个很重要的因素，但在推荐系统中考虑得却不是很多。比较常用的方法是ad-hoc式的清洗，如，去除数据集中的噪声和不可靠的记录。显露出的缺点是,没有结合数据集本身的特性。&lt;/p&gt;

&lt;p&gt;论文的主要贡献是考虑推荐系统中两个核心的数据质量问题：&lt;strong&gt;sparsity&lt;/strong&gt; 和 &lt;strong&gt;redundancy&lt;/strong&gt;。主要是设计了数据集相关的阈值模型和采样等级模型，然后在一系列的公开数据集上做了验证试验。&lt;/p&gt;

&lt;h3 id=&#34;具体方法:c7e7f190dc4ad9c3cea8a960f18951d1&#34;&gt;&lt;strong&gt;具体方法&lt;/strong&gt;&lt;/h3&gt;

&lt;h4 id=&#34;关于数据的稀疏性:c7e7f190dc4ad9c3cea8a960f18951d1&#34;&gt;&lt;strong&gt;关于数据的稀疏性&lt;/strong&gt;&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;阈值模型(Threshold model)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;很多基于分数的推荐系统数据集中包含一部分&lt;strong&gt;冷启动&lt;/strong&gt;的用户和物品。一般的数据清洗操会去除这些用户和物品的打分信息。现实中的问题是，怎样得到最优的清洗阈值。最简单的暴力方法(brute-force)是评估所有可能的组合，但时间空间复杂度高，在实际中不可行。&lt;/p&gt;

&lt;p&gt;首先，明确目标:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The aim is to develop a heuristic method that predicts the optimal thresholds for a given user-item rating matrix, without building the model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然后，采用了一些合理的假设，如假设要预测的物品阈值是与&lt;strong&gt;物品向量&lt;/strong&gt;的平均长度&lt;code&gt;$\overline{r}_i$&lt;/code&gt;相关，在打分矩阵中，即为平均每个物品被打分的次数。由于推荐数据集中，只有一小部分的流行商品会被打分多次，大部分商品被打分的次数相对很少，这引入了模型的另外一个特征，即物品向量长度的&lt;strong&gt;power-law&lt;/strong&gt;分布,令H为物品向量长度的分布函数，拟合函数&lt;code&gt;$H={Ax}^{(-m)}$&lt;/code&gt;,其中x是单个物品向量的长度，m是个正数。最终可以得到数据集对应的m参数值。m越大，长尾分布中的尾部越向下，反映到数据集上，就是更加少的物品被多次打分过，也就是说，m越大，数据集本身越稀疏。&lt;/p&gt;

&lt;p&gt;得到以上两个特征因素，接下来就是构建阈值模型，得到公式: &lt;code&gt;${IT}_d = \gamma*\frac{log(\overline{r}_i)}{m^2}$&lt;/code&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;评价方法&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;总共24个数据集，包括10个开放数据集，如Movielens, Million Songs, Flixster, Moviepilot, Filmtipset, Yelp, Yahoo! Music(broken down into albums, artists, and tracks), and BookCrossing。以及14个从公司或站点获取的专业数据集(proprietary)。&lt;/p&gt;

&lt;p&gt;每个数据集9-1划分成训练集和测试集，使用*Precision@K*作为衡量测试集的标准(K为测试集的总记录数)，10折交叉验证，平均准确率作为最后的得分。&lt;/p&gt;

&lt;p&gt;实验过程：
- 首先，寻找每个数据集d的最优IT值。具体对于某个IT值，过滤掉低于IT的item，对剩余数据进行矩阵分解(Matrix Factorization),建立推荐模型，并通过测试集评测模型的准确率。不断迭代，增长IT，寻找使得测试集准确率最高的IT作为最优阈值&lt;code&gt;${IT}^{opt}_d$&lt;/code&gt;，对应的准确率为&lt;code&gt;${P}^{opt}_d$&lt;/code&gt;。
- 接下来，对24个数据集采用leave-one-out交叉验证。在23个数据集上训练阈值模型，然后预测剩余数据集d的阈值&lt;code&gt;${IT}^{pred}_d$&lt;/code&gt;,并在过滤后的数据上训练推荐模型，计算准确率&lt;code&gt;$P^{pred}_d$&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;两个评测指标：&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$NTE_d = |{IT}^{opt}_d - {IT}^{pred}_d|/{IT}^{opt}_d$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$AR_d = P^{pred}_d/P^{opt}_d$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;注：&lt;strong&gt;IT&lt;/strong&gt;(item threshold), &lt;strong&gt;AR&lt;/strong&gt;(accuracy ratio), &lt;strong&gt;NTE&lt;/strong&gt;(normalized threshold error)&lt;/p&gt;

&lt;p&gt;通过计算，&lt;code&gt;${NTE}_d$&lt;/code&gt;和&lt;code&gt;${AR}_d$&lt;/code&gt;之间的相关度为-0.54。这表明IT的误差越小，推荐模型的准确率越高。&lt;/p&gt;

&lt;h4 id=&#34;关于数据的冗余性:c7e7f190dc4ad9c3cea8a960f18951d1&#34;&gt;&lt;strong&gt;关于数据的冗余性&lt;/strong&gt;&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;模型&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;数据集可以通过随机采样，建立模型，使得尽可能得与在全部数据上建立模型的结果相似。这对于实际中的大规模数据是有效的。&lt;/p&gt;

&lt;p&gt;模型目标：
   - The aim is to pick the &lt;strong&gt;loweset sampling rate&lt;/strong&gt; that will still result in the recommendation model as close as possible to the model that would have been built using the complete data.&lt;/p&gt;

&lt;p&gt;与阈值模型不同的是，在采样率的取值上没有最优。因为在全体数据上构建推荐模型总是最优的。&lt;/p&gt;

&lt;p&gt;定义&lt;code&gt;$SR$&lt;/code&gt;为采样数据的推荐结果与全体数据的推荐结果相似度不低于&lt;code&gt;$\Delta$&lt;/code&gt;时的最低采样率。&lt;/p&gt;

&lt;p&gt;定义&lt;code&gt;$U_d, I_d, R_d$&lt;/code&gt;为数据集d中的用户数量，商品数量，和打分项个数，给定采样率&lt;code&gt;$SR$&lt;/code&gt;,从所有用户中随机选取&lt;code&gt;$SR*U_d$&lt;/code&gt;个用户，在采样的数据集上建立推荐模型，对一个固定的测试集产生预测结果。比较与全部数据预测的结果差异。&lt;/p&gt;

&lt;p&gt;以对用户采样举例，假设数据的冗余性与三个因素相关a.用户数 b.打分矩阵的稀疏性 c.V-structure，得到公式：&lt;/p&gt;

&lt;p&gt;&lt;code&gt;${SR}_d = tanh(\frac{1}{{V-structure}_d * \sqrt{U_d} * \frac{R_d}{I_d * U_d}})$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;其中V-structure的定义为平均相似度之比，分子中的每对用户至少有一个打分项目相同，分母中的每对用户为全体用户的所有组合可能。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;评价&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;采用19个专业数据集，9-1比例分训练集和测试集，10折交叉验证。实验主要验证了模型的合理性。&lt;/p&gt;

&lt;h3 id=&#34;我的思考:c7e7f190dc4ad9c3cea8a960f18951d1&#34;&gt;&lt;strong&gt;我的思考&lt;/strong&gt;&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;这篇论文提出了两个模型公式，并完成了实验验证。优点是模型公式与数据集本身的特性相关，不干涉推荐系统的建模过程。&lt;/li&gt;
&lt;li&gt;遗留下的问题：数据的稀疏性和冗余性只针对MF推荐模型，还可以扩展到与其他推荐模型相结合。&lt;/li&gt;
&lt;li&gt;考虑其他的评测指标，如覆盖率和多样性；同时，模型中还可以结合物品的内容属性和用户的位置属性。&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Telstra Network Disruptions</title>
      <link>http://cherishzhang.github.io/post/kaggle/Telstra/</link>
      <pubDate>Wed, 02 Mar 2016 21:03:56 +0800</pubDate>
      
      <guid>http://cherishzhang.github.io/post/kaggle/Telstra/</guid>
      <description>

&lt;h2 id=&#34;1-overview:43e8ca42a888262ca3ed3923385c58f2&#34;&gt;&lt;strong&gt;1. Overview&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The goal of the &lt;a href=&#34;https://www.kaggle.com/c/telstra-recruiting-network/&#34;&gt;problem&lt;/a&gt; is to predict Telstr network&amp;rsquo;s fault severity at a time at a particular location based on the &lt;em&gt;log data&lt;/em&gt; available.&lt;/p&gt;

&lt;p&gt;The target has 3 categories:0,1,2. It&amp;rsquo;s a multiclass classification problems.Different types of features are extracted from log files and other sources: event_type.csv, log_feature.csv, resource_type.csv,severity_type.csv.&lt;/p&gt;

&lt;p&gt;My final score is &lt;em&gt;0.44917(72 of 974)&lt;/em&gt; in private leaderboard. Here are my &lt;a href=&#34;https://github.com/Cherishzhang/kaggle/tree/master/Telstra&#34;&gt;code&lt;/a&gt;, and I will record my solution.&lt;/p&gt;

&lt;h2 id=&#34;2-feature-engineer:43e8ca42a888262ca3ed3923385c58f2&#34;&gt;&lt;strong&gt;2. Feature Engineer&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Selecting and designing good features is an important area in machine learning, which is called &lt;em&gt;feature engineering&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;At First, I just merge all files on Id. Here are five type of features. they are location, severity_type, resource_type, event_type, log_feature.
location and severity_type are just one-to-one variables, and others are many_to_one variables.
There are about 1200 locations, some are only in the test set. The correlation between location variable and target is 0.27, it gave me a hint that neighbouring locations may be similar in the network&amp;rsquo;s fault severity problem. I used one_hot encode to solve the many_to_one features. So there are about 400+ features in the initial stage.&lt;/p&gt;

&lt;p&gt;I read a paper about how to preprocess high-cardinality Categorical attributes in classification and regression problems, but it seemed to bring little help. I will tried it again after the competition.&lt;/p&gt;

&lt;p&gt;On the forum, there are a heated discussion about the magic feature. I spent much of my time to find it.
It is the order of the same location in the severity file which follows the order of fault occurrence.It&amp;rsquo;s called &lt;em&gt;Intra-location order&lt;/em&gt;.&lt;br /&gt;
It really did a big help to the final score which improved almost 0.06. To each record, I compute the target(&lt;em&gt;fault_severity&lt;/em&gt;) probabilities from the previous same location records, and used the &lt;em&gt;previous-target_probabilities&lt;/em&gt; as a feature to build the model.&lt;/p&gt;

&lt;h2 id=&#34;3-build-models-and-ensembling:43e8ca42a888262ca3ed3923385c58f2&#34;&gt;&lt;strong&gt;3. Build models and Ensembling&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;I tried many models, decision tree, random forests, svm and xgboost. The xgboost model is performed well.
On ensembling, I just average the random forest and xgboost result as the final result.&lt;/p&gt;

&lt;h2 id=&#34;4-what-i-learned-from-other-kagglers:43e8ca42a888262ca3ed3923385c58f2&#34;&gt;&lt;strong&gt;4. What I learned from other kagglers&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;On the platform, kagglers are willing to share their ideas. Here are some valuable ideas on the &lt;a href=&#34;https://www.kaggle.com/c/telstra-recruiting-network/forums/t/19239/it-s-been-fun-post-your-code-github-links-here-after-the-competition&#34;&gt;Competition Forum&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;some-useful-tips-on-feature-engineering:43e8ca42a888262ca3ed3923385c58f2&#34;&gt;Some useful tips on feature engineering&lt;/h3&gt;

&lt;p&gt;It&amp;rsquo;s the most important step in machine learning whatever models you used.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;About location&lt;br /&gt;
&lt;em&gt;Similar location numbers have similar fault severity.(Treat the location as numeric.)&lt;/em&gt;
&lt;em&gt;Don&amp;rsquo;t one-hot-encode the location, Tree-based classifiers are not good at handling huge sparse feature matrix.&lt;/em&gt;
&lt;em&gt;Frequency of LogType_203 = 0 &amp;amp; LogType_203 &amp;gt;0 per location&lt;/em&gt;
&lt;em&gt;The records in log data are arranged in the order of time.(the magic feature)&lt;/em&gt; Here are two ways to encode
the information, One is for each location, use the row number, which starts from 1 to the total number of rows for that locaiton. The other
is to normalize it between 0 and 1.
&lt;em&gt;percentile transformation of location counts&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;About log feature&lt;br /&gt;
&lt;em&gt;Pattern of log feature&lt;/em&gt;
&amp;ldquo;one hot&amp;rdquo; encoding for all log features with volume &amp;gt; 0, for all rows.
Each &amp;ldquo;one hot&amp;rdquo; encoded pattern treated as a string.
Assigned integer ID to each to each string, used as feature.
The log transform for the count of &amp;ldquo;pattern of log feature&amp;rdquo;, the log transform for counts as &amp;ldquo;pattern of event&amp;rdquo;
and &amp;ldquo;pattern of resource&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Common categorical variables&lt;br /&gt;
&lt;em&gt;For high-cardinality categorical variables, frequency works well.(Add the frequency of each location in both train and test set.)&lt;/em&gt;
&lt;em&gt;Summary statistics to reduce one-to-many relationship to one-to-one, and two-way or more-way interaction among multiple variables.&lt;/em&gt;
&lt;em&gt;Meta features(using Logistic regression to fit sparse matrix as predictors, then ensemble the model).&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;A useful solution&lt;br /&gt;
&amp;gt;a.The order of id on log_feature was frozen.&lt;br /&gt;
&amp;gt;b.Converting location, log_feature into numbers and generating count, mean, sum, etc. features (feature set A)&lt;br /&gt;
&amp;gt;c.Feature B was generated by shifting A forward by 1 row&lt;br /&gt;
&amp;gt;d.Feature C was generated by shifting B backward by 1 row&lt;br /&gt;
&amp;gt;Combining A, B, and C and training xgb, RF, GBM models. My final model is an ensemble model of these models.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>