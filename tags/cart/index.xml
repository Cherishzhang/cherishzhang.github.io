<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cart on Learn and record</title>
    <link>http://cherishzhang.github.io/tags/cart/</link>
    <description>Recent content in Cart on Learn and record</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 01 Aug 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://cherishzhang.github.io/tags/cart/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>决策树</title>
      <link>http://cherishzhang.github.io/post/ml/decision_tree/</link>
      <pubDate>Mon, 01 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/ml/decision_tree/</guid>
      <description>

&lt;h2 id=&#34;1-intro:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;1.Intro&lt;/h2&gt;

&lt;p&gt;决策树常用的算法有&lt;r&gt;ID3&lt;/r&gt;,&lt;r&gt;C4.5&lt;/r&gt;和&lt;r&gt;CART&lt;/r&gt;，学习过程分为三部分：&lt;r&gt;特征选择&lt;/r&gt;，&lt;r&gt;决策树生成&lt;/r&gt;和&lt;r&gt;决策树剪枝&lt;/r&gt;。&lt;/p&gt;

&lt;p&gt;决策树的&lt;r&gt;生成&lt;/r&gt;只考虑局部最优（&lt;r&gt;贪心算法&lt;/r&gt;），而决策树的&lt;r&gt;剪枝&lt;/r&gt;则考虑&lt;r&gt;全局最优&lt;/r&gt;。简而言之，这是&lt;strong&gt;一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对每个子数据集有一个最好的分类过程&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;They are easy to &lt;r&gt;interpret&lt;/r&gt;, &lt;r&gt;handle categorical features&lt;/r&gt;, extend to the &lt;r&gt;multiclass classification setting&lt;/r&gt;, do not require &lt;r&gt;feature scaling&lt;/r&gt;, and are able to &lt;r&gt;capture non-linearities and feature interactions&lt;/r&gt;.&lt;/p&gt;

&lt;h2 id=&#34;2-准则函数:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;2.准则函数&lt;/h2&gt;

&lt;p&gt;主要用来衡量节点数据集合的有序性，有&lt;r&gt;熵&lt;/r&gt;，&lt;r&gt;基尼指数(Gini)&lt;/r&gt;，&lt;r&gt;方差&lt;/r&gt;，其中前两种针对分类问题，而方差针对回归问题。寻找最好的分割点是通过量化&lt;strong&gt;分割后&lt;/strong&gt;类的纯度来确定的，计算方式总结如下表：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Impurity&lt;/th&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;formula&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;熵Entropy&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;随机变量X不确定性的度量，X的概率分布为\(P(X=x_i) = p_i\)&lt;/td&gt;
&lt;td&gt;\(H(X) =-\sum_{i=1}^{n} p_ilogp_i\)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;条件熵&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;已知随机变量X的条件下随机变量Y的不确定性&lt;/td&gt;
&lt;td&gt;\(H(Y|X)=\sum_{i=1}^n p_i H(Y|x=x_i)\)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;信息增益(互信息)&lt;/td&gt;
&lt;td&gt;分类ID3&lt;/td&gt;
&lt;td&gt;特征A对训练集D的信息增益，&lt;strong&gt;值越大，特征分类能力越强&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;\(g(D,A)=H(D)-H(D|A)\)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;信息增益比&lt;/td&gt;
&lt;td&gt;分类C4.5&lt;/td&gt;
&lt;td&gt;校正信息增益存在&lt;strong&gt;偏向选择取值较多的特征&lt;/strong&gt;的缺点，n是特征A的取值个数&lt;/td&gt;
&lt;td&gt;\(g_R(D,A) = \frac{g(D,A)}{H_A(D)}\), \(H_A(D)= \frac{|D_i|}{|D|}*log_2(\frac{|D_i|}{|D|})\)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;基尼&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;假设有K类，样本点属于第i类的概率为\(p_i\)，表示集合不确定性&lt;/td&gt;
&lt;td&gt;\(Gini(D) = \sum_{i=1}^{K} p_i*(1-p_i)\)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;基尼&lt;/td&gt;
&lt;td&gt;分类&lt;/td&gt;
&lt;td&gt;经过A=a分割后集合D的不确定性&lt;/td&gt;
&lt;td&gt;\(Gini(D,A)=\frac{|D1|}{|D|} Gini(D1) + \frac{|D2|}{|D|} Gini(D2)\)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;方差&lt;/td&gt;
&lt;td&gt;回归&lt;/td&gt;
&lt;td&gt;平方误差最小，求解最优输出值&lt;/td&gt;
&lt;td&gt;\(\frac{1}{N} \sum_{i=1}^N (y_i-\mu)^2\),\(\mu = \frac{1}{N} \sum_{i=1}^N y_i\)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;信息增益&lt;/strong&gt;&lt;br/&gt;
定义： 得知特征X的信息而使类Y的信息&lt;r&gt;不确定减少&lt;/r&gt;的程度。信息增益大的特征有更强的分类能力。&lt;br/&gt;
计算：\(g(D,A) = H(D)-H(D|A)\)。 即 &lt;r&gt;特征A的信息增益 = D的经验熵 - A对D的经验条件熵&lt;/r&gt;&lt;br/&gt;
方法： 对训练数据集（或子集）D，计算其每个特征的信息增益，比较大小，从中选择信息增益大的特征。&lt;/p&gt;

&lt;h2 id=&#34;3-生成算法:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;3 生成算法&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;具体方法&lt;/strong&gt;：从根节点开始，计算所有可能的特征的准则值(应用准则函数)，选择最优的特征作为节点的特征，由该特征的不同取值建立子节点；再对子节点递归调用以上方法，构建决策树。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;结束条件&lt;/strong&gt;：直到所有特征的信息增益均很小或没有特征可选择为止。&lt;/p&gt;

&lt;p&gt;ID3算法应用&lt;r&gt;信息增益&lt;/r&gt;选择特征；C4.5对此进行改进，应用&lt;r&gt;信息增益比&lt;/r&gt;选择特征。&lt;/p&gt;

&lt;h4 id=&#34;cart:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;CART&lt;/h4&gt;

&lt;p&gt;CART(classification and regression tree)，又称分类与回归树。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CART的生成是一个递归地构建&lt;r&gt;二叉树&lt;/r&gt;的过程&lt;/strong&gt;，对回归树应用平方误差最小化准则，对分类树用Gini指数最小化准则，进行特征选择，生成二叉树。预测时，先根据样本特征判断该样本点位于哪块区域，然后用该区域内的训练样本点集的&lt;r&gt;众数&lt;/r&gt; (分类) 或&lt;r&gt;均值&lt;/r&gt; (回归) 作为该样本点的预测值。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;算法描述&lt;/strong&gt;：CART树生成算法&lt;br/&gt;
&lt;strong&gt;输入&lt;/strong&gt;: 训练数据集D，停止条件&lt;br/&gt;
&lt;strong&gt;输出&lt;/strong&gt;: 决策树f(x)&lt;br/&gt;
&lt;strong&gt;(1)&lt;/strong&gt;在平方误差最小或Gini最小的准则下选择&lt;r&gt;最优切分变量j与切分点s&lt;/r&gt;，切分变量从可用特征集中选择。
回归树中，单元\(R_m\)下的最优值是\(R_m\)上所有输入实例对应的输出\(y_i\)的均值，即切分点。分类中，切分点是该变量所有可能的取值。&lt;br/&gt;
&lt;strong&gt;(2)&lt;/strong&gt;用选定的对(j,s)划分区域并决定相应的输出值&lt;br/&gt;
&lt;strong&gt;(3)&lt;/strong&gt;继续对子区域调用(1),(2),直至满足停止条件&lt;br/&gt;
&lt;strong&gt;(4)&lt;/strong&gt;将输入空间划分为M个区域\(R_1\),\(R_2\),&amp;hellip;,\(R_M\).生成如下决策树：&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;$$f(x) = \sum_{m=1}^{M} c_m*I(x \in R_m)$$&lt;/p&gt;

&lt;p&gt;算法的&lt;r&gt;停止条件&lt;/r&gt;是节点中的样本个数小于预定阈值，或样本集的基尼指数小于预订阈值（样本基本属于同一类），或没有更多特征。&lt;/p&gt;

&lt;h2 id=&#34;4-剪枝过程-pruning:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;4 剪枝过程(pruning)&lt;/h2&gt;

&lt;p&gt;将已生成的树进行简化的过程，通过极小化决策树整体的损失函数（loss function）来实现。 分为前向剪枝和后向剪枝。&lt;/p&gt;

&lt;h2 id=&#34;5-mllib实现:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;5.MLlib实现&lt;/h2&gt;

&lt;h4 id=&#34;连续值特征:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;连续值特征&lt;/h4&gt;

&lt;p&gt;对于连续值类型的特征，单个机器上选取全部唯一值作为切分点候选。加速计算的方法是对唯一值集合进行排序，用排序好的数组做候选集。&lt;/p&gt;

&lt;p&gt;在分布式中，对于规模大的数据集，排序是比较耗时的。于是先对整体数据按一定比例采样，然后&lt;r&gt;采用分位数(等频分箱)作为分割点的近似候选集&lt;/r&gt;。规定分箱总数不能超过训练样本的实例数(maxBins的默认值为32)。&lt;/p&gt;

&lt;h4 id=&#34;类别值特征:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;类别值特征&lt;/h4&gt;

&lt;p&gt;如果一个离散的特征，有m个值，那么最多产生\(2^{m-1}-1\)种划分可能。对于二分类或回归问题，可以将划分候选缩减到m-1种可能。&lt;/p&gt;

&lt;p&gt;举个例子，在二分类问题中，一个特征有A,B,C三个值。特征值为A的样本集中label是1的比例为0.2，B和C分别为0.6，0.4。则产生了一个类别值的有序序列ACB，共有两种划分可能：A|C,B，A,C|B。&lt;/p&gt;

&lt;p&gt;多分类问题中，离散特征会产生\(2^{m-1}-1\)种划分可能，当划分数大于maxBins，将m个类别值根据impurity排序，产生共m-1种划分可能。&lt;/p&gt;

&lt;h4 id=&#34;bin和split:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;bin和split&lt;/h4&gt;

&lt;p&gt;split是划分点，bin是划分区间。bin中预先计算样本的统计信息，节省计算开销。&lt;strong&gt;分类问题&lt;/strong&gt;，bin中保存训练样本个数 (count) 以及各 label 数目 (count for label)；&lt;strong&gt;回归问题&lt;/strong&gt;，bin中保存训练样本个数 (count)、 label 之和 (sum) 以及 label 的平方和 (squared sum)。特征选择时，统计同一分区下，选择不同切分变量时，对应bin中的信息（ImpurityAggregator），合并所有分区，确定最佳切分变量和切分点，从而实现并行化。&lt;/p&gt;

&lt;h4 id=&#34;终止条件:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;终止条件&lt;/h4&gt;

&lt;p&gt;为了防止过拟合，采用前向剪枝。当任一以下情况发生，节点就终止划分，形成叶子节点：
(1)树高度达到maxDepth &lt;br/&gt;
(2)minInfoGain，当前节点的所有属性分割带来的信息增益都比这个值要小 &lt;br/&gt;
(3)minInstancesPerNode，需要保证节点分割出的左右子节点的最少的样本数量达到这个值 &lt;br/&gt;&lt;/p&gt;

&lt;h4 id=&#34;关键优化点:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;关键优化点&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;查找最优split，是树的level级别的并行，处于同一层次的节点，查找是并行的。查找次数的时间复杂度为O(L）,其中L是树的层数。&lt;/li&gt;
&lt;li&gt;对于连续值特征，采用分位数（等频分箱）作为split的候选集，减少了大规模数据的计算开销。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;统计信息在bin中计算，节省开销。&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;参数:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;参数&lt;/h4&gt;

&lt;p&gt;&lt;r&gt;categoricalFeaturesInfo&lt;/r&gt;: 若特征是离散型，需要显示指定类别个数，否则会当作连续值处理。&lt;/p&gt;

&lt;p&gt;&lt;r&gt;maxBins&lt;/r&gt;: 值越大，算法越有可能产生更多的划分候选，但随之也会增加通信和计算开销。如果存在m个值的离散特征，maxBins至少为m。&lt;/p&gt;

&lt;p&gt;&lt;r&gt;maxMemoryInMB&lt;/r&gt;: 用于统计计算的内存空间大小。&lt;/p&gt;

&lt;p&gt;&lt;r&gt;subsamplingRate&lt;/r&gt;: 训练决策树时数据集的采样比例(随机森林和GBDT)。&lt;/p&gt;

&lt;h3 id=&#34;源码分析:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;源码分析&lt;/h3&gt;

&lt;p&gt;DecisionTree&lt;/p&gt;

&lt;p&gt;DecisionTreeModel&lt;/p&gt;

&lt;h2 id=&#34;参考链接:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;参考链接&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://spark.apache.org/docs/latest/mllib-decision-tree.html&#34;&gt;spark mllib - decision tree&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>