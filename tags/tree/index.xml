<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tree on Learn and record</title>
    <link>http://cherishzhang.github.io/tags/tree/</link>
    <description>Recent content in Tree on Learn and record</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 01 Aug 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://cherishzhang.github.io/tags/tree/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>[ml] Decision Tree模型</title>
      <link>http://cherishzhang.github.io/post/ml/decision_tree/</link>
      <pubDate>Mon, 01 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/ml/decision_tree/</guid>
      <description>

&lt;h2 id=&#34;1-intro:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;1.Intro&lt;/h2&gt;

&lt;p&gt;决策树常用的算法有&lt;r&gt;ID3&lt;/r&gt;,&lt;r&gt;C4.5&lt;/r&gt;和&lt;r&gt;CART&lt;/r&gt;，学习过程分为三部分：&lt;r&gt;特征选择&lt;/r&gt;，&lt;r&gt;决策树生成&lt;/r&gt;和&lt;r&gt;决策树剪枝&lt;/r&gt;。&lt;/p&gt;

&lt;p&gt;决策树的&lt;r&gt;生成&lt;/r&gt;只考虑局部最优（&lt;r&gt;贪心算法&lt;/r&gt;），而决策树的&lt;r&gt;剪枝&lt;/r&gt;则考虑&lt;r&gt;全局最优&lt;/r&gt;。简而言之，这是&lt;strong&gt;一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对每个子数据集有一个最好的分类过程&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;They are easy to &lt;r&gt;interpret&lt;/r&gt;, &lt;r&gt;handle categorical features&lt;/r&gt;, extend to the &lt;r&gt;multiclass classification setting&lt;/r&gt;, do not require &lt;r&gt;feature scaling&lt;/r&gt;, and are able to &lt;r&gt;capture non-linearities and feature interactions&lt;/r&gt;.&lt;/p&gt;

&lt;h2 id=&#34;2-准则函数:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;2.准则函数&lt;/h2&gt;

&lt;p&gt;主要用来衡量节点数据集合的有序性，有&lt;r&gt;熵(Entropy)&lt;/r&gt;，&lt;r&gt;基尼指数(Gini)&lt;/r&gt;，&lt;r&gt;方差(Variance)&lt;/r&gt;，其中前两种针对分类问题，而方差针对回归问题。寻找最好的分割点是通过量化&lt;strong&gt;分割后&lt;/strong&gt;类的纯度来确定的，计算方式总结如下表：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Impurity&lt;/th&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;formula&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;熵&lt;/td&gt;
&lt;td&gt;分类&lt;/td&gt;
&lt;td&gt;假设有K类，样本点属于第i类的概率为\(p_i\)，度量数据的不确定性&lt;/td&gt;
&lt;td&gt;\(E(D) =\sum_{i=1}^{K} -p_ilogp_i\)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;基尼&lt;/td&gt;
&lt;td&gt;分类&lt;/td&gt;
&lt;td&gt;同上&lt;/td&gt;
&lt;td&gt;\(Gini(D) = \sum_{i=1}^{K} p_i*(1-p_i)\)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;方差&lt;/td&gt;
&lt;td&gt;回归&lt;/td&gt;
&lt;td&gt;度量数据的离散程度&lt;/td&gt;
&lt;td&gt;\(Variance(D) = \frac{1}{N} \sum_{i=1}^N (y_i-\mu)^2\),\(\mu = \frac{1}{N} \sum_{i=1}^N y_i\)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;信息增益&lt;/strong&gt;: 得知某特征X的信息而使目标Y的信息&lt;r&gt;不确定减少&lt;/r&gt;的程度。信息增益大的特征有更强的分类能力。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;具体方法&lt;/strong&gt;：对训练集D，计算每个特征的信息增益，比较大小，选择信息增益大的特征列来分割集合。假设分割点将数据集分成左右两部分，则信息增益为 \( IG(D,s) = Impurity(D) - \frac{N_{left}}{N} Impurity(D_{left}) - \frac{N_{right}}{N} Impurity(D_{right})\)&lt;/p&gt;

&lt;p&gt;ID3s算法选择信息增益作分类，为了校正信息增益存在“偏向选择取值较多的特征”的缺点，C4.5采用&lt;r&gt;信息增益比&lt;/r&gt;\(IG_R(D,s) = \frac{IG(D,s)}{E_s(D)}\), \(E_s(D)= \sum_{i=1}^K -\frac{|D_i|}{|D|}*log_2(\frac{|D_i|}{|D|})\), 其中K是特征s的取值个数。&lt;/p&gt;

&lt;h2 id=&#34;3-生成算法:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;3 生成算法&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;具体方法&lt;/strong&gt;：从根节点开始，计算所有可能的特征的准则值(应用准则函数)，选择最优的特征作为节点的特征，由该特征的不同取值建立子节点；再对子节点递归调用以上方法，构建决策树。&lt;/p&gt;

&lt;h4 id=&#34;cart:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;CART&lt;/h4&gt;

&lt;p&gt;CART(classification and regression tree)，又称分类与回归树。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CART的生成是一个递归地构建&lt;r&gt;二叉树&lt;/r&gt;的过程&lt;/strong&gt;，对回归树应用平方误差最小化准则，对分类树用Gini指数最小化准则，进行特征选择，生成二叉树。预测时，先根据样本特征判断该样本点位于哪块区域，然后用该区域内的训练样本点集的&lt;r&gt;众数&lt;/r&gt; (分类) 或&lt;r&gt;均值&lt;/r&gt; (回归) 作为该样本点的预测值。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;算法描述&lt;/strong&gt;：CART树生成算法&lt;br/&gt;
&lt;strong&gt;输入&lt;/strong&gt;: 训练数据集D，停止条件&lt;br/&gt;
&lt;strong&gt;输出&lt;/strong&gt;: 决策树f(x)&lt;br/&gt;
&lt;strong&gt;(1)&lt;/strong&gt;在平方误差最小或Gini最小的准则下选择&lt;r&gt;最优切分变量j与切分点s&lt;/r&gt;，切分变量从可用特征集中选择。
回归树中，单元\(R_m\)下的最优值是\(R_m\)上所有输入实例对应的输出\(y_i\)的均值，即切分点。分类中，切分点是该变量所有可能的取值。&lt;br/&gt;
&lt;strong&gt;(2)&lt;/strong&gt;用选定的对(j,s)划分区域并决定相应的输出值&lt;br/&gt;
&lt;strong&gt;(3)&lt;/strong&gt;继续对子区域调用(1),(2),直至满足停止条件&lt;br/&gt;
&lt;strong&gt;(4)&lt;/strong&gt;将输入空间划分为M个区域\(R_1\),\(R_2\),&amp;hellip;,\(R_M\).生成如下决策树：&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;$$f(x) = \sum_{m=1}^{M} c_m*I(x \in R_m)$$&lt;/p&gt;

&lt;p&gt;算法的&lt;r&gt;停止条件&lt;/r&gt;是节点中的样本个数小于预定阈值，或样本集的基尼指数小于预订阈值（样本基本属于同一类），或没有更多特征。&lt;/p&gt;

&lt;h2 id=&#34;4-剪枝过程-pruning:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;4 剪枝过程(pruning)&lt;/h2&gt;

&lt;p&gt;将已生成的树进行简化的过程，通过极小化决策树整体的损失函数（loss function）来实现。 分为前向剪枝和后向剪枝。前向剪枝是在构造决策树的同时对树进行剪枝；后向剪枝是在决策树构建完成后，对树从根节点向上递归剪枝。&lt;/p&gt;

&lt;h2 id=&#34;5-mllib实现:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;5.MLlib实现&lt;/h2&gt;

&lt;h4 id=&#34;连续值特征:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;连续值特征&lt;/h4&gt;

&lt;p&gt;对于连续值类型的特征，单台机器上的做法是选取所有出现过的值作为切分点候选集。加速计算的方法是对该集合按值大小排序，用排序好的数组做候选集。&lt;/p&gt;

&lt;p&gt;在分布式中，对于规模大的数据集，排序是比较耗时的。于是先&lt;r&gt;对整体数据按比例采样&lt;/r&gt;，然后&lt;r&gt;采用分位数(等频分箱)作为分割点的近似候选集&lt;/r&gt;(&lt;gr&gt;&lt;strong&gt;优化点&lt;/strong&gt;&lt;/gr&gt;)。&lt;/p&gt;

&lt;p&gt;&lt;r&gt;采样策略&lt;/r&gt;为选取样本集中的&lt;strong&gt;&lt;r&gt;max&lt;/r&gt;(maxBins*maxBins, 10000)&lt;/strong&gt;条记录。&lt;r&gt;等频分箱策略&lt;/r&gt;有三种(&lt;strong&gt;Sort&lt;/strong&gt;,MinMax,ApproxHist)，目前只支持第一种。规定分箱总数不能超过训练样本的实例数(maxBins的默认值为32)。&lt;/p&gt;

&lt;h4 id=&#34;类别值特征:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;类别值特征&lt;/h4&gt;

&lt;p&gt;对于离散值类型的特征，有m个值，那么最多产生\(2^{m-1}-1\)种划分可能，\(2*(2^{m-1}-1)\)个分箱。对于二分类或回归问题，可以将划分候选缩减到m-1种可能。&lt;/p&gt;

&lt;p&gt;举个例子，在二分类问题中，一个特征有A,B,C三个值。特征值为A的样本集中label是1的比例为0.2，B和C分别为0.6，0.4。则产生了一个类别值的有序序列ACB，共有两种划分可能：A|C,B，A,C|B。&lt;/p&gt;

&lt;p&gt;多分类问题中，离散特征会产生\(2^{m-1}-1\)种划分可能，当划分数大于maxBins，将m个类别值根据impurity排序，产生共m-1种划分可能。&lt;/p&gt;

&lt;h4 id=&#34;bin和split:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;bin和split&lt;/h4&gt;

&lt;p&gt;split是划分点，bin是划分区间。每个特征都分别有一组bin和split。&lt;/p&gt;

&lt;p&gt;其中bin中&lt;r&gt;预先计算样本的统计信息&lt;/r&gt;，节省计算开销(&lt;gr&gt;&lt;strong&gt;优化点&lt;/strong&gt;&lt;/gr&gt;)。&lt;strong&gt;分类问题&lt;/strong&gt;，bin中保存训练样本个数 (count) 以及各 label 数目 (count for label)；&lt;strong&gt;回归问题&lt;/strong&gt;，bin中保存训练样本个数 (count)、 label 之和 (sum) 以及 label 的平方和 (squared sum)。&lt;/p&gt;

&lt;p&gt;特征选择时，对于某个特征，计算不同切分变量下信息增益的大小，确定出该特征下的最佳切分点。选择全部候选特征集中最优的切分点作为当前决策。特征选择是树的level级别的并行，对于同一层次的节点，查找可以并行，查找操作的时间复杂度为O(L),L为树的层数(&lt;gr&gt;&lt;strong&gt;优化点&lt;/strong&gt;&lt;/gr&gt;)。&lt;/p&gt;

&lt;h4 id=&#34;终止条件:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;终止条件&lt;/h4&gt;

&lt;p&gt;为了防止过拟合，采用前向剪枝。当任一以下情况发生，节点就终止划分，形成叶子节点：
(1)树高度达到maxDepth &lt;br/&gt;
(2)minInfoGain，当前节点的所有属性分割带来的信息增益都比这个值要小 &lt;br/&gt;
(3)minInstancesPerNode，需要保证节点分割出的左右子节点的最少的样本数量达到这个值 &lt;br/&gt;&lt;/p&gt;

&lt;h4 id=&#34;源码分析:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;源码分析&lt;/h4&gt;

&lt;p&gt;下面就一起揭开decision tree的真正面纱吧！&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/DecisionTree.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;gr&gt;输入 - RDD[LabelPoint]&lt;/gr&gt;：
LabelPoint类有两个属性label(标签值), features(特征向量)。将训练数据集转换为LabelPoint形式，作为决策树训练的输入之一。&lt;/p&gt;

&lt;p&gt;&lt;gr&gt;输入 - Strategy&lt;/gr&gt;:
这是关于模型训练过程中需要指定的参数集。&lt;/p&gt;

&lt;p&gt;&lt;gr&gt;buildMetadata()&lt;/gr&gt;:
DecisionTreeMetadata是一个决策树&lt;r&gt;元数据信息&lt;/r&gt;类，其中大部分属性和Strategy类重合。该函数的&lt;r&gt;功能&lt;/r&gt;: &lt;strong&gt;对于类别值特征，区分有序/无序；统计类别值特征的划分点数目和分箱数目。&lt;/strong&gt;是训练决策树模型的一个准备工作。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def buildMetadata(input:RDD[LabeledPoint], strategy:Strategy,
numTrees: Int, featureSubsetStrategy: String):DecisionTreeMetadata =
{...}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于每个类别值特征fi，&lt;r&gt;多分类情况下&lt;/r&gt;：首先计算最大可能分箱maxPossibleBins确定情况下，能够允许的最多类别值m，如果fi的类别值超过m，则当作有序处理；否则认为该特征是无序的，并计算分箱数。&lt;/p&gt;

&lt;p&gt;&lt;r&gt;二分类/回归情况下&lt;/r&gt;：当作有序处理，即numBins(fi) = numCategories(fi)。&lt;/p&gt;

&lt;h3&gt;findSplitsBins&lt;/h3&gt;

&lt;p&gt;&lt;sg&gt;这是DecisionTree.scala中一个方法，基于buildMetadata统计的划分点数目和分箱数，加上数据采样，确定每个特征的划分点值。&lt;/sg&gt;&lt;/p&gt;

&lt;p&gt;&lt;sg&gt;如果存在连续值特征，则对数据集进行无放回采样，采样个数为max(maxBins*maxBins, 10000)。接着进行分箱操作（见findSplitsBinsBySorting函数，findSplitsForContinuousFeature函数）。&lt;/sg&gt;&lt;/p&gt;

&lt;p&gt;&lt;sg&gt;&lt;strong&gt;treeRDD,BaggedRDD&lt;/strong&gt;:
treeRDD的类型是：RDD[TreePoint],treePoint类有两个属性：label和binnedFeatures。BaggedPoint类有两个属性datum和subsampleWeights(在各个采样后数据集中的权重，即出现次数)。
&lt;/sg&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def findSplitsBins(input:RDD[LabeledPoint],
metadata:DecisionTreeMetadata): (Array[Array[Split]], Array[Array[Bin]]) =
{ ... }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;h3&gt;findBestSplits&lt;/h3&gt;
&lt;sg&gt;对于当前节点，计算准则函数下的信息增益，选择最优切分特征和最优切分点。&lt;/sg&gt;&lt;/p&gt;

&lt;p&gt;&lt;gr&gt;其他&lt;/gr&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;configuration&lt;/strong&gt;:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;类名&lt;/th&gt;
&lt;th&gt;枚举值&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Algo&lt;/td&gt;
&lt;td&gt;Classification, Regression&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;FeatureType&lt;/td&gt;
&lt;td&gt;Continuous, Categorical&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;QuantileStrategy&lt;/td&gt;
&lt;td&gt;Sort, MinMax, ApproxHist&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;EnsembleCombiningStrategy&lt;/td&gt;
&lt;td&gt;Average, Sum, Vote&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;impurtiy,loss&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Impurity是准则函数的抽象类，其三个子类分别为Gini,Variance,Entropy。Impurities是Impurity的工厂类，负责解析用户参数，选择对应的准则函数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private[mllib] object Impurities {
  def fromString(name: String): Impurity = name match {
    case &amp;quot;gini&amp;quot; =&amp;gt; Gini
    case &amp;quot;entropy&amp;quot; =&amp;gt; Entropy
    case &amp;quot;variance&amp;quot; =&amp;gt; Variance
    case _ =&amp;gt; throw new IllegalArgumentException(s&amp;quot;Did not recognize
    Impurity name: $name&amp;quot;)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Loss与Impurity情况类似，它是损失函数的抽象类，其三个子类分别是AbsoluteError,SquaredError,LogLoss。Losses是Loss的工厂类，负责判断损失的所属类别。&lt;/p&gt;

&lt;h2 id=&#34;参考链接:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;参考链接&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://spark.apache.org/docs/latest/mllib-decision-tree.html&#34;&gt;spark mllib - decision tree&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;统计学习方法，李航著&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[ml] Boosting</title>
      <link>http://cherishzhang.github.io/post/ml/boosting/</link>
      <pubDate>Fri, 20 May 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/ml/boosting/</guid>
      <description>

&lt;h2 id=&#34;intro:d21755d9d058d9a9b44db3100ac0dea8&#34;&gt;Intro&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_boosting&#34;&gt;Boosting&lt;/a&gt; can be interpreted as &lt;strong&gt;an optimization algorithm&lt;/strong&gt; on a suitable cost function. The algorithms that optimize a cost &lt;strong&gt;function over function&lt;/strong&gt; space by &lt;r&gt;iteratively&lt;/r&gt; choosing a function(week hypothesis) that points in the &lt;strong&gt;negative gradient direction&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;在分类问题中，提升(Boosting)思想，通过改变训练数据的概率分布（权重分布），针对不同的训练数据分布，学习多个弱分类器，将这些分类器线性组合，提高分类的性能。&lt;/p&gt;

&lt;p&gt;围绕提升方法，有&lt;strong&gt;&lt;r&gt;两个问题&lt;/r&gt;&lt;/strong&gt;需要考虑：1.每一轮如何改变训练数据的概率或权值分布；2.如何将弱分类器组合成一个强分类器。&lt;/p&gt;

&lt;h2 id=&#34;adaboost-adaptive-boosting:d21755d9d058d9a9b44db3100ac0dea8&#34;&gt;AdaBoost(Adaptive Boosting)&lt;/h2&gt;

&lt;p&gt;AdaBoost提高那些被前几轮弱分类器线性组成的分类器&lt;r&gt;错误分类&lt;/r&gt;的样本权值，在下一轮的弱分类器中更加关注没有得到正确分类的数据；采用&lt;r&gt;加权多数表决&lt;/r&gt;的方法，误差率小的弱分类器权值大，误差率大的弱
分类器权值小。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;算法流程&lt;/strong&gt;：（二分类问题）&lt;br/&gt;
&lt;r&gt;输入&lt;/r&gt;：训练数据集\(T=\{(x_1,y_1),(x_2,y_2),&amp;hellip;,(x_N,y_N)\}\)和弱分类器算法 &lt;br/&gt;
&lt;r&gt;输出&lt;/r&gt;：最终分类器\(G(x)\)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;初始化训练样本的&lt;r&gt;权值分布&lt;/r&gt;(均匀分布)&lt;br/&gt;$$D_1=(w_{11},&amp;hellip;,w_{1i},&amp;hellip;,w_{1N}), w_{1i}=\frac{1}{N}, i=1,2,&amp;hellip;,N$$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对于\(m=1,2,&amp;hellip;,M\)(迭代)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用具有权值分布\(D_m\)的训练数据集学习，得到基本分类器\(G_m(x)\)&lt;/li&gt;
&lt;li&gt;计算\(G_m(x)\)在权值分布\(D_m\)的训练数据集上的&lt;strong&gt;分类误差率&lt;/strong&gt;
$$e_m = P(G_m(x_i) \neq y_i) = \sum_{i=1}^N w_{mi}I(G_m(x_i) \neq y_i)$$&lt;/li&gt;
&lt;li&gt;计算\(G_m(x)\)的系数，即&lt;r&gt;表决权&lt;/r&gt;
$$\alpha_{m} = \frac{1}{2} \log \frac{1-e_m}{e_m}$$&lt;/li&gt;
&lt;li&gt;更新训练数据集的权值分布
$$D_{m+1} = (w_{m+1,1},&amp;hellip;,w_{m+1,i},&amp;hellip;,w_{m+1,N})$$
$$w_{m+1,i} = \frac{w_{mi}}{Z_m} \exp (-\alpha_m y_i G_m(x_i))$$
其中\(Z_m\)是规范化因子,$$Z_m = \sum_{i=1}^N w_{mi} exp(-\alpha_m y_i G_m(x_i))$$&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;构建基本分类器的线性组合，得到&lt;r&gt;最终的分类器&lt;/r&gt;
$$f(x) = \sum_{m=1}^M \alpha_m G_m(x)$$
$$G(x) = sign(f(x))$$&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每轮迭代过程中，Adaboost会重点关注错误分类的样本，使得&lt;strong&gt;算法受噪声和离群点的影响较大&lt;/strong&gt;。\(\alpha_m\)随着 \(e_m\) 的减小而增大，\(w_{mi}\) 也随着\(\alpha_m\) 而改变，不断增大误分类样本的权值，减小正确分类样本的权值。&lt;/p&gt;

&lt;p&gt;AdaBoost的一个解释是，模型为&lt;strong&gt;加法模型，损失函数为指数函数，学习算法为前向分步算法的二分类学习方法&lt;/strong&gt;。前向分步算法(forward stagewise algorithm)求解优化问题的思路是：从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式，就可以简化优化的复杂度。&lt;/p&gt;

&lt;h2 id=&#34;boosting-tree-提升树:d21755d9d058d9a9b44db3100ac0dea8&#34;&gt;Boosting Tree(提升树)&lt;/h2&gt;

&lt;p&gt;采用&lt;strong&gt;加法模型(基函数的线性组合)与前向分步算法，以决策树为基函数的提升方法&lt;/strong&gt;称为提升树，分类问题采用二叉分类树，回归问题采用二叉回归树。&lt;/p&gt;

&lt;p&gt;提升树表示为决策树的加法模型:
$$f_{M(x)} = \sum_{m=1}^M T(x; \Theta_m)$$
其中，\(T(x;\Theta_m)\)表示决策树，\(\Theta_m\)为决策树的参数，M为树的个数&lt;/p&gt;

&lt;p&gt;不同问题的提升树学习算法，&lt;r&gt;主要区别在于使用的损失函数不同&lt;/r&gt;，包括用平方误差损失函数的回归问题，用指数损失函数的分类问题，以及用一般损失函数的一般决策问题。将Adaboost算法中的基本分类器限制为分类决策树，即为针对二分类问题的提升树算法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;提升树算法&lt;/strong&gt;(回归问题):&lt;br/&gt;
&lt;r&gt;输入&lt;/r&gt;:训练数据集\(T=\{(x_1,y_1),(x_2,y_2),&amp;hellip;,(x_N,y_N)\}\)
&lt;br/&gt;
&lt;r&gt;输出&lt;/r&gt;:提升树\(f_M(x)\)&lt;br/&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;确定初始提升树\(f_0(x) = 0\)&lt;/li&gt;
&lt;li&gt;对m = 1,2,&amp;hellip;,M

&lt;ul&gt;
&lt;li&gt;计算残差： \(r_{mi} = y_{i} - f_{m-1}(x_i), i=1,2,&amp;hellip;,N\)&lt;/li&gt;
&lt;li&gt;拟合残差，学习一个回归树，得到\(T(x;\Theta_m)\)&lt;/li&gt;
&lt;li&gt;更新\(f_m(x)=f_{m-1}(x)+T(x;\Theta_m)\)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;得到最终提升树：\(f_M(x)=\sum_{m=1}^M T(x;\Theta_m)\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;gradient-boosting-梯度提升:d21755d9d058d9a9b44db3100ac0dea8&#34;&gt;Gradient Boosting(梯度提升)&lt;/h2&gt;

&lt;p&gt;前向分布算法针对特殊的损失函数(平方和指数)，对于一般的损失函数，Freidman提出了梯度提升算法。它利用最速下降法的近似方法，其&lt;r&gt;&lt;strong&gt;关键&lt;/strong&gt;&lt;/r&gt;是利用损失函数的负梯度在当前模型的值最为回归问题提升树算法中的残差的近似值，拟合一个回归树。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;梯度提升算法&lt;/strong&gt;：&lt;br/&gt;
&lt;r&gt;输入&lt;/r&gt;:训练数据集\(T={(x_1,y_1),(x_2,y_2),&amp;hellip;,(x_N,y_N)}\)，损失函数\(L(y,f(x))\)&lt;br/&gt;
&lt;r&gt;输出&lt;/r&gt;:回归树\(\hat{f} (x)\)&lt;br\&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;初始化，估计是损失函数极小化的常数值，即只有一个根节点的树
\(f_0(x) = arg\mathop{min}\limits_{c} \sum_{i=1}^{N} L(y_i,c)\)&lt;/li&gt;
&lt;li&gt;对m=1,2,&amp;hellip;,M

&lt;ul&gt;
&lt;li&gt;对i=1,2,&amp;hellip;,N,计算
\(r_{mi} = - \{\left [\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)} \right]\}_{f(x)=f_{m-1}(x)}\)&lt;/li&gt;
&lt;li&gt;对于\(r_{mi}\)拟合一个回归树，得到第m棵树的叶节点区域\(R_{mj}$, j=1,2,&amp;hellip;,J\)（对于平方损失函数，就是通常所说的残差，对于一般损失函数，即为残差的近似值）&lt;/li&gt;
&lt;li&gt;对于j=1,2,&amp;hellip;,J,计算 $$c_{mj} = arg\mathop{min}\limits_{c} \sum_{x_i \in R_{mj}} L(y_i,f_{m-1}(x_i)+c)$$&lt;/li&gt;
&lt;li&gt;更新\(f_m(x) = f_{m-1}(x)+\sum_{j=1}^J c_{mj}I(x\in R_mj)\)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;得到回归树 $$\hat{f}(x) = f_M(x) = \sum_{m=1}^M \sum_{j=1}^J c_{mj} I(x \in R_{mj})$$&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>