<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Regresssion on Learn and record</title>
    <link>http://cheriszhang.github.io/tags/regresssion/</link>
    <description>Recent content in Regresssion on Learn and record</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Feb 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://cheriszhang.github.io/tags/regresssion/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>[ml] Stacking</title>
      <link>http://cheriszhang.github.io/post/ml/stacking/</link>
      <pubDate>Fri, 24 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>http://cheriszhang.github.io/post/ml/stacking/</guid>
      <description>

&lt;h2 id=&#34;intro:5d2f1f98b2cffb7fb28a28b223ce8215&#34;&gt;Intro&lt;/h2&gt;

&lt;p&gt;机器学习比赛中常常会用到的一个技巧：stacking，它通过组合不同的预测模型来提高最终结果的准确率。下面内容主要是阅读&lt;a href=&#34;https://burakhimmetoglu.com/2016/12/01/stacking-models-for-improved-predictions/&#34;&gt;文章&lt;/a&gt;后，总结学习到的一些东西。&lt;/p&gt;

&lt;h2 id=&#34;example:5d2f1f98b2cffb7fb28a28b223ce8215&#34;&gt;Example&lt;/h2&gt;

&lt;p&gt;以kaggle上&lt;a href=&#34;https://www.kaggle.com/c/house-prices-advanced-regression-techniques&#34;&gt;房屋价格预测&lt;/a&gt;为例，该挑战给出了有关房屋的多维特征，目标是通过构建模型，预测房屋的价格。stacking的可行做法如下：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;将训练集随机分成10 folds，留最后一份作为验证集(holdout data for validation)。前9份在level 1中使用；&lt;/p&gt;

&lt;p&gt;level 1: 选取XGBoost, neural network, support vector regression这三个模型，对于每个模型M，交叉验证的方式产生一个预测结果（用8份训练模型，1份预测结果）。&lt;/p&gt;

&lt;p&gt;level 2: 每个模型产生了一个9 folds大小的预测结果，将它们当成训练的feature，训练一个线性模型，并在最后一份数据上面做验证。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;some-points:5d2f1f98b2cffb7fb28a28b223ce8215&#34;&gt;Some points&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;out-of-example&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;第一层level中，通过交叉验证的方式构造第二层level中的输入训练集。第二层level中，用初始化预留的最后一折数据做模型验证。&lt;/p&gt;

&lt;p&gt;假设不按照如上方法，而是在level 1时，使用全部的训练数据对模型进行参数拟合，将预测结果用于level 2时，就会出现过拟合的问题，导致偏差增大（预测结果会偏向M个模型中效果最好的一个）。&lt;/p&gt;

&lt;p&gt;stacking方法通过使用out-of-example预测，有可能使level 2的预测结果好于level 1中的各个单模型。&lt;/p&gt;

&lt;p&gt;具体流程，如下图：
&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/stacking.png&#34; alt=&#34; &#34; title=&#34;stacking&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;work for small/medium dataset&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;stacking适合中小规模数据集&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;detail&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;重复10次，将训练集划分10-folds。这会产生10个level 2的预测结果，可以对该结果集取平均。&lt;/p&gt;

&lt;p&gt;一旦确定stacking可以提高预测结果的精度，将整个流程再次运行一次。此时不再保留第 N folds作为holdout data。用N-1 folds训练模型，得到剩余1折的预测结果。集齐训练集的全部预测结果，作为level 2的输入，训练模型，输出最终测试集的结果。&lt;/p&gt;

&lt;h2 id=&#34;ref:5d2f1f98b2cffb7fb28a28b223ce8215&#34;&gt;Ref&lt;/h2&gt;

&lt;p&gt;1.&lt;a href=&#34;https://burakhimmetoglu.com/2016/12/01/stacking-models-for-improved-predictions/&#34;&gt;Stacking models for improved predictions&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2.&lt;a href=&#34;http://mlwave.com/kaggle-ensembling-guide/&#34;&gt;KAGGLE ENSEMBLING GUIDE&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;3.&lt;a href=&#34;https://cran.r-project.org/web/packages/caretEnsemble/vignettes/caretEnsemble-intro.html&#34;&gt;A Brief Introduction to caretEnsemble&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>