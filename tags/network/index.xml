<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Network on Learn and record</title>
    <link>http://cherishzhang.github.io/tags/network/</link>
    <description>Recent content in Network on Learn and record</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Aug 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://cherishzhang.github.io/tags/network/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>word2vec模型和源码解析</title>
      <link>http://cherishzhang.github.io/post/word2vec/</link>
      <pubDate>Fri, 05 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/word2vec/</guid>
      <description>

&lt;h2 id=&#34;词向量:d45865320239d7818255e82b856775aa&#34;&gt;词向量&lt;/h2&gt;

&lt;p&gt;一种简单直观的词表示方法是&lt;r&gt;One-hot&lt;/r&gt;编码，用N位对N个词编码，每个词对应的N维向量中，只有一维为0。这种方式的缺陷是&lt;r&gt;词汇鸿沟&lt;/r&gt;，即词与词之间相互孤立，忽略了它们之间的联系。而word2vec中的词向量是一种distributed representation，它的&lt;r&gt;特点&lt;/r&gt;是&lt;strong&gt;利用距离刻画词之间的相似性&lt;/strong&gt;。&lt;/p&gt;

&lt;h2 id=&#34;统计语言模型:d45865320239d7818255e82b856775aa&#34;&gt;统计语言模型&lt;/h2&gt;

&lt;p&gt;统计语言模型是用来计算一个句子的概率的&lt;r&gt;概率模型&lt;/r&gt;，经典的有HAL,LSA,COALS等。通常它基于一个语料库来构建。&lt;/p&gt;

&lt;h3 id=&#34;句子的概率:d45865320239d7818255e82b856775aa&#34;&gt;句子的概率&lt;/h3&gt;

&lt;p&gt;给定一个由T个词\(w_1,w_2,&amp;hellip;,w_T\)按顺序构成的句子，根据Bayes，计算该句子的概率p(W)&lt;/p&gt;

&lt;p&gt;$$p(W) = p(w_1^T) = p(w_1,w_2,&amp;hellip;,w_T) = p(w_1)*p(w_2|w_1)*p(w_3|(w_1^2))&amp;hellip;p(w_T|(w_1^{T-1}))$$&lt;/p&gt;

&lt;p&gt;其中等式中的各个因子（条件概率）就是语言模型的&lt;strong&gt;参数&lt;/strong&gt;，常用的语言模型都是在近似求p(W)。如果给定一个词典大小为N的语料库，考虑长度为T的任意句子，理论上有\(N^T\)种可能组合，而每个句子需要计算T个参数，总共需要计算&lt;r&gt;\(T*N^T\)&lt;/r&gt;个参数。无论是计算或保存，都需要很大开销。&lt;/p&gt;

&lt;h3 id=&#34;n-gram模型:d45865320239d7818255e82b856775aa&#34;&gt;n-gram模型&lt;/h3&gt;

&lt;p&gt;针对参数繁多，该模型做了一个简化的假定，即一个词出现的概率&lt;strong&gt;只与&lt;/strong&gt;它前面的n-1个词相关。实际中最多采用&lt;r&gt;n=3&lt;/r&gt;的三元模型。&lt;/p&gt;

&lt;p&gt;$$(p(w_k|w^{k-1}_1) = p(w_k|w^{k-1}_{k-n+1}) = \frac{count(w_{k-n+1}^k)}{count(w_{k-n+1}^{k-1})}$$&lt;/p&gt;

&lt;p&gt;如果过程中出现\(count(w_{k-n+1}^k)\) = \(count(w_{k-n+1}^{k-1})\) 或者 \(count(w_{k-n+1}^{k-1}) = 0\)，则需要做&lt;r&gt;平滑处理&lt;/r&gt;。&lt;/p&gt;

&lt;h3 id=&#34;最大似然:d45865320239d7818255e82b856775aa&#34;&gt;最大似然&lt;/h3&gt;

&lt;p&gt;对句子模型的另一种转化是利用机器学习，构造一个目标函数，并最大似然该函数。优化目标求得最优参数，利用最优参数预测未知句子的概率。&lt;strong&gt;&lt;r&gt;目标函数&lt;/r&gt;&lt;/strong&gt;一般为: $$\prod_{w \in C} p(w|Context(w))$$&lt;/p&gt;

&lt;p&gt;其中C表示&lt;r&gt;语料集&lt;/r&gt;，Context(w)表示词w的&lt;r&gt;上下文&lt;/r&gt;，即w周边词的集合。实际中采用最大对数似然，即\(\sum\limits_{w \in C} log p(w|Context(w))\)最大化。&lt;strong&gt;增加log是为了将乘法运算转换为加法运算(便于求导的常用方法)&lt;/strong&gt;。问题关键在于如何构造F函数。&lt;/p&gt;

&lt;p&gt;$$p(w|Context(w)) = F(w,Context(w),\theta)$$&lt;/p&gt;

&lt;h2 id=&#34;神经概率语言模型:d45865320239d7818255e82b856775aa&#34;&gt;神经概率语言模型&lt;/h2&gt;

&lt;p&gt;Bengio(03) - 《A Neural Probabilistic Language Model》&lt;/p&gt;

&lt;p&gt;神经网络包含四层：&lt;r&gt;输入层&lt;/r&gt;，&lt;r&gt;投影层&lt;/r&gt;，&lt;r&gt;隐藏层&lt;/r&gt;和&lt;r&gt;输出层&lt;/r&gt;。W,U为权值矩阵，p,q为偏置向量。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/word2vec-neural.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;取&lt;r&gt;二元对(Context(w),w)&lt;/r&gt;作为一个训练样本，其中Context(w)取前面n-1个词，每个词\(v_i\)用一个m维向量表示。投影层是一个(n-1)*m长度的向量，输出层的规模为词库中词表的长度D。&lt;/p&gt;

&lt;p&gt;选用双曲正切函数作为隐藏层的激活函数，计算过程为：&lt;/p&gt;

&lt;p&gt;$$z_w = tanh(Wx_w + p), y_w = Uz_w + q$$&lt;/p&gt;

&lt;p&gt;输出为一个长度为D的向量\(y_w = (y_{w,1}, y_{w,2},&amp;hellip;,y_{w,D})^T\)，通过softmax归一化，每个分量表示上下文为Context(w)时下一词为词典第i个词的概率。&lt;/p&gt;

&lt;p&gt;&lt;r&gt;待确定的参数集\(\Theta\)&lt;/r&gt;包括：&lt;strong&gt;词向量和填充向量，神经网络参数W,U,p,q&lt;/strong&gt;。其中大部分&lt;strong&gt;计算集中&lt;/strong&gt;在&lt;r&gt;隐藏层和输出层之间的矩阵向量运算&lt;/r&gt;和&lt;r&gt;softmax归一化&lt;/r&gt;上。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;关于softmax归一化：
形式为：$$ P(y=i) = \frac{exp(\sum_d w_{id}x_{d})}{\sum_j exp(\sum_d w_{jd} x_{d})} $$
主要是针对多分类问题，归一化输出概率值。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;word2vec核心:d45865320239d7818255e82b856775aa&#34;&gt;Word2vec核心&lt;/h2&gt;

&lt;p&gt;word2vec中用到了两个重要模型: &lt;r&gt;CBOW&lt;/r&gt;和&lt;r&gt;Skip-gram&lt;/r&gt;，对于这两个模型，分别都给出了基于&lt;r&gt;Hierarchical Softmax&lt;/r&gt;和&lt;r&gt;Negative Sampling&lt;/r&gt;两套框架的实现。&lt;/p&gt;

&lt;p&gt;CBOW和Skip-gram包括三层：&lt;r&gt;输入层&lt;/r&gt;，&lt;r&gt;投影层&lt;/r&gt;，&lt;r&gt;输出层&lt;/r&gt;。前者是已知词的上下文预测当前词，后者是通过当前词预测上下文。&lt;/p&gt;

&lt;h3 id=&#34;1-cbow:d45865320239d7818255e82b856775aa&#34;&gt;1. CBOW&lt;/h3&gt;

&lt;p&gt;CBOW = Continuous Bag-of-Words, 目标函数为：&lt;/p&gt;

&lt;p&gt;$$\sum_{w \in c} log p(w|Context(w)) = \sum_{t=1}^T log p(w_t|\tau(w_{t-k},w_{t-k-1},&amp;hellip;,w_{t+k-1},w_{t+k}))$$&lt;/p&gt;

&lt;p&gt;其中T表示整个词典的大小，模型的目标是&lt;r&gt;最大化&lt;/r&gt;这个目标函数值。\(w_t\)表示词典中的一个词，即通过和\(w_t\)相邻的窗口大小为k的词来预测\(w_t\)出现的概率。其中\(\tau(w_1,w_2,&amp;hellip;,w_k)\)函数表示以\(w_i(i&amp;lt;=i&amp;lt;=k)\)为参数进行某种运算。在Word2vec里，这种运算是向量加和运算。即&lt;r&gt;把窗口相邻的所有词的向量加和&lt;/r&gt;。&lt;/p&gt;

&lt;h4 id=&#34;hierarchical-softmax:d45865320239d7818255e82b856775aa&#34;&gt;Hierarchical Softmax&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/word2vec-hs-cbow.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;相比于之前的神经概率语言模型，主要有&lt;r&gt;三处改动&lt;/r&gt;：&lt;strong&gt;去掉隐藏层；首尾拼接改为累加求和；输出为树结构。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;================================================================&lt;/p&gt;

&lt;p&gt;关于模型的&lt;r&gt;推导&lt;/r&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;结点的带权路径长度&lt;/strong&gt;: 从根结点到该结点的路径长度乘以该结点的权值&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;树的带权路径长度&lt;/strong&gt;: 所有叶结点的带权路径长度之和&lt;/p&gt;

&lt;p&gt;Huffman树是在给定n个叶子结点和其对应权值的前提下，构造一棵二叉树，使该树的带权路径长度最小。已知语料库中各词的词频，可以构建一棵二叉树，每个词作为树的叶子节点，并保证词频越大的词离根节点越近。每个叶子对应的&lt;r&gt;huffman编码&lt;/r&gt;就是该词的编码。约定左子结点编码为0，右子结点编码为1。&lt;/p&gt;

&lt;p&gt;此外为每个非叶子结点设置一个&lt;r&gt;辅助向量&lt;/r&gt;\(\theta\)。&lt;/p&gt;

&lt;p&gt;对于词w，从根结点到该叶结点的路径中，假设有\(l^w\)层，经历了\(l^w-1\)次二分类，则条件概率\( p(w|Context(w)) = \prod\limits_{j=2}^{l^w} p(d_j^w|X_w,\theta_{j-1}^w) \), \(d_j^w\)表示第j个结点对应的编码，\(\theta_j^w\)表示第j个结点对应的辅助向量。&lt;/p&gt;

&lt;p&gt;第一步，&lt;strong&gt;目标函数对数化&lt;/strong&gt;:
$$F = \sum_{w \in C} log p(w|Context(w)) = \sum_{w \in C} log \prod\limits_{j=2}^{l^w} \sigma(\theta_{j-1}^w X_w)^{1-{d_j^w}} (1-\sigma(\theta_{j-1}^w X_w))^{d_j^w} $$&lt;/p&gt;

&lt;p&gt;第二步，&lt;strong&gt;取内部项F(w,u)&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;$$ F(w,u) =  (1-{d_j^w}) log(\sigma(\theta_{j-1}^w)) + {d_j^w} log(1-\sigma(\theta_{j-1}^w X_w))$$&lt;/p&gt;

&lt;p&gt;第三步，&lt;strong&gt;随机梯度上升&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;\(\theta_{j-1}^w\)的更新公式为：&lt;/p&gt;

&lt;p&gt;$$ \frac{\partial F(w,u)}{\partial \theta_{j-1}^w} = (1 - d_j^w - \sigma(X_w^T*\theta_{j-1}^w)) X_w$$&lt;/p&gt;

&lt;p&gt;$$ \theta_{j-1}^w := \theta_{j-1}^w + \eta (1 - d_j^w - \sigma(X_w^T*\theta_{j-1}^w)) X_w $$&lt;/p&gt;

&lt;p&gt;\(X_w\)和\(\theta_{j-1}^w\)具有对称性，更新公式类似，这里注意更新的\(v(\tilde{w})\)而非加和后的\(X_w\)：&lt;/p&gt;

&lt;p&gt;$$v(\tilde{w}) := v(\tilde{w}) +  \eta \sum\limits_{j=2}^{l^w}(1 - d_j^w - \sigma(X_w^T*\theta_{j-1}^w)) \theta_{j-1}^w, \tilde{w} \in Context(w)$$&lt;/p&gt;

&lt;p&gt;================================================================&lt;/p&gt;

&lt;h4 id=&#34;negative-sampling:d45865320239d7818255e82b856775aa&#34;&gt;Negative Sampling&lt;/h4&gt;

&lt;p&gt;huffman树中从根节点到叶子节点路径上的各个中间结点，分为正负两类，构造好huffman树后，每个词w对应的采样样本（中间结点）也就确定。训练的目标是最大化似然函数值。由此衍生出，用简单的&lt;r&gt;随机负采样&lt;/r&gt;代替复杂的huffman树。已知词w的上下文，预测词w，可将w看作正样本，负样本的抽样策略为&lt;r&gt;带权采样&lt;/r&gt;，即被选中的概率与词频成正比。模型训练的&lt;r&gt;目标&lt;/r&gt;是&lt;strong&gt;最大化正样本的概率同时最小化负样本的概率&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;设NEG(w)是关于词w的一个负样本集，对已一个给定的&lt;r&gt;二元对&lt;/r&gt;(Context(w),w),&lt;r&gt;优化目标&lt;/r&gt;是&lt;/p&gt;

&lt;p&gt;$$ g(w) = \prod\limits_{u \in w \cup NEG(w)} p(u|Context(w))$$&lt;/p&gt;

&lt;p&gt;=================================================================&lt;/p&gt;

&lt;p&gt;关于模型的&lt;r&gt;推导&lt;/r&gt;:&lt;/p&gt;

&lt;p&gt;为每个词设置一个&lt;r&gt;辅助向量\(\theta^u\)&lt;/r&gt;，\(X_w^T\)表示为Context(w)中的k个词进行加和后的向量，\(L^w(u)\)表示词u的样本标签，正样本取1，反之取0。&lt;/p&gt;

&lt;p&gt;第一步，&lt;strong&gt;转换目标函数&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;$$g(w) = \prod\limits_{u \in w \cup NEG(w)} [\sigma(\theta^u X_w^T)]^{L^w(u)} [1 - \sigma(\theta^u X_w^T)] ^ {(1-L^w(u))}$$&lt;/p&gt;

&lt;p&gt;因为正样本只有一个，故可简化为\(g(w) =\sigma(\theta^u X_w^T) \prod\limits_{u \cup NEG(w)} [1 - \sigma(\theta^u X_w^T)] ^ {1-L^w(u)}\)&lt;/p&gt;

&lt;p&gt;第二步，&lt;strong&gt;考虑整个语料库&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;$$F = log G = log \prod\limits_{w \in C} g(w)$$
$$ = \sum\limits_{w \in C} \sum\limits_{u \in w \cup NEG(w)} {L^w(u)}log[\sigma(\theta^u X_w^T)] + (1-L^w(u))log[1 - \sigma(\theta^u X_w^T)]$$
$$ = \sum\limits_{w \in C} \bigg (log(\sigma(\theta^u X_w^T)) + \sum\limits_{u \in NEG(w)} log(\sigma(-\theta^u X_w^T))\bigg)$$&lt;/p&gt;

&lt;p&gt;第三步，&lt;strong&gt;随机梯度上升&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;$$ F(w,u) = L^w(u) log[\sigma(\theta^u X_w^T)] + (1-L^w(u)) log[1 - \sigma(\theta^u X_w^T)] $$&lt;/p&gt;

&lt;p&gt;\(\theta^u\)的&lt;r&gt;更新公式&lt;/r&gt;：&lt;/p&gt;

&lt;p&gt;$$\frac{\partial F(w,u)}{\partial \theta^u}  = L^w(u) (1 -\sigma(\theta^u X_w^T)) X_w - (1-L^w(u)) \sigma(\theta^u X_w^T) X_w $$
$$  =  L^w(u) X_w - \sigma(\theta^u X_w^T) X_w $$&lt;/p&gt;

&lt;p&gt;$$\theta^u := \theta^u + \eta (L^w(u) - \sigma(\theta^u X_w^T)) X_w$$&lt;/p&gt;

&lt;p&gt;根据\(\theta^u\)和\(X_w\)的&lt;r&gt;对称性&lt;/r&gt;，w上下文中的每个词向量的&lt;r&gt;更新公式&lt;/r&gt;为：&lt;/p&gt;

&lt;h1 id=&#34;v-tilde-w-v-tilde-w-sum-limits-u-in-w-cup-neg-w-eta-l-w-u-sigma-theta-u-x-w-t-theta-u-tilde-w-in-context-w:d45865320239d7818255e82b856775aa&#34;&gt;$$v(\tilde{w}) := v(\tilde{w}) + \sum\limits_{u \in w \cup NEG(w)} \eta (L^w(u) - \sigma(\theta^u X_w^T))\theta^u, \tilde{w} \in Context(w)$$&lt;/h1&gt;

&lt;h3 id=&#34;2-skip-gram:d45865320239d7818255e82b856775aa&#34;&gt;2. SKIP-GRAM&lt;/h3&gt;

&lt;h4 id=&#34;hierarchical-softmax-1:d45865320239d7818255e82b856775aa&#34;&gt;Hierarchical Softmax&lt;/h4&gt;

&lt;p&gt;与CBOW不同是，输入层只有一个词向量w。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/word2vec-hs-sg.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;关键在于\(p(Context(w)|w)\)的定义：&lt;/p&gt;

&lt;p&gt;$$ p(Context(w)|w) = \prod\limits_{u \in Context(w)} p(u|w) $$，其他推导类似CBOW。&lt;/p&gt;

&lt;h4 id=&#34;negative-sampling-1:d45865320239d7818255e82b856775aa&#34;&gt;Negative Sampling&lt;/h4&gt;

&lt;p&gt;已知词w，预测词w的上下文，则针对上下文中的每个词，将该词看作正样本，选取一定数量的负样本。目标函数为：&lt;/p&gt;

&lt;p&gt;$$F = log G = log \prod\limits_{w \in C} g(w)$$
$$ = \sum\limits_{w \in C} \sum\limits_{u \in Context(w)} \sum\limits_{z \in u \cup NEG(u)} {L^u(z)}log[\sigma(\theta^z X_w^T)] + (1-L^u(z))log[1 - \sigma(\theta^z X_w^T)]$$&lt;/p&gt;

&lt;p&gt;公式中对于一个样本(w, Context(w)), 对Context(w)中的每个词，都做了一次负采样。而word2vec源码对此作了改动，只对词w进行了|Context(w)|次负采样。&lt;/p&gt;

&lt;h2 id=&#34;c-源码阅读:d45865320239d7818255e82b856775aa&#34;&gt;c++源码阅读&lt;/h2&gt;

&lt;h3 id=&#34;命令行参数:d45865320239d7818255e82b856775aa&#34;&gt;命令行参数&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;参数&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;size&lt;/td&gt;
&lt;td&gt;Projection Layer的长度，即词向量的长度&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;train&lt;/td&gt;
&lt;td&gt;语料库地址，即训练集&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;save-vocab&lt;/td&gt;
&lt;td&gt;词典保存地址&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;read-vocab&lt;/td&gt;
&lt;td&gt;词典读取地址，如果没指定，从语料库中学习词典&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;debug&lt;/td&gt;
&lt;td&gt;debug_mode，debug信息打印控制&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;td&gt;是否保存为二进制&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;cbow&lt;/td&gt;
&lt;td&gt;确定是cbow，还是skip-gram模型&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;alpha&lt;/td&gt;
&lt;td&gt;学习率&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;模型保存地址&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;window&lt;/td&gt;
&lt;td&gt;窗口大小，默认值5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;sample&lt;/td&gt;
&lt;td&gt;采样百分比&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;hs&lt;/td&gt;
&lt;td&gt;是否进行Hierarchical Softmax&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;negative&lt;/td&gt;
&lt;td&gt;是否进行负采样&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;threads&lt;/td&gt;
&lt;td&gt;线程数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;min-count&lt;/td&gt;
&lt;td&gt;有效词频的最小值，默认值5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;classes&lt;/td&gt;
&lt;td&gt;聚类个数&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;数据结构:d45865320239d7818255e82b856775aa&#34;&gt;数据结构&lt;/h3&gt;

&lt;p&gt;每个词用结构体&lt;r&gt;vocab_word&lt;/r&gt;保存信息:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct vocab_word{
    char *word; //词本身
    long long cn; //语料集中的词频
    int *point; //huffman树中，从根节点到该词所在叶节点的中间结点列表
    char *code, codelen; //该词对应的huffman编码和编码长度
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;整个词表用一个数组vocab表示，为了&lt;strong&gt;快速查找&lt;/strong&gt;给定词的信息，建立了一个&lt;r&gt;hash表&lt;/r&gt;，存放的元素是该词在vocab中的下标。&lt;/p&gt;

&lt;p&gt;哈希的策略是&lt;strong&gt;线性探测的开放定址法&lt;/strong&gt;，计算词w的哈希值hv(w)作为下标索引，若该位置被占用，则顺序往下找，直到找到一个未被占用的位置。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vocab = (struct vocab_word *)calloc(vocab_max_size, sizeof(struct vocab_word));
vocab_hash = (int *)calloc(vocab_hash_size, sizeof(int));
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;模型实现:d45865320239d7818255e82b856775aa&#34;&gt;模型实现&lt;/h3&gt;

&lt;h4 id=&#34;1-关于net:d45865320239d7818255e82b856775aa&#34;&gt;1. 关于Net&lt;/h4&gt;

&lt;p&gt;定义了&lt;r&gt;syn0&lt;/r&gt;,&lt;r&gt;syn1&lt;/r&gt;,&lt;r&gt;syn1neg&lt;/r&gt;三个浮点数组，大小都为vocab_size*layer1_size。其中vocab_size为词表的长度，layer1_size为词向量的长度。&lt;/p&gt;

&lt;p&gt;syn0表示输入层需要的每个词的词向量\(v\), syn1表示huffman树中每个中间结点的辅助向量\(\theta\), syn1neg表示negative sampling中为每个词设置的辅助向量\(theta\)。代码中Hierarchical Softmax和Negative Sampling可以同时训练。neu1代表\(X_w\),即输入词向量加和后的向量；neu1e代表\(e\)，即huffman树每一条路径（从根到叶子节点）梯度更新的求和，用来更新相应的词向量。&lt;/p&gt;

&lt;h4 id=&#34;2-sigmoid函数值计算:d45865320239d7818255e82b856775aa&#34;&gt;2. sigmoid函数值计算&lt;/h4&gt;

&lt;p&gt;建模过程中用到&lt;r&gt;逻辑回归&lt;/r&gt;做二分类问题，程序中采用表格的方式，存储\(x \in [-6,6]\区间（等分成1000份）的sigmoid函数值，小于-6，默认为0；大于6，默认为1。每次计算时，查表即可。&lt;/p&gt;

&lt;h4 id=&#34;3-创建huffman树:d45865320239d7818255e82b856775aa&#34;&gt;3. 创建huffman树&lt;/h4&gt;

&lt;p&gt;简单来说，用一个vocab_size*2+1大小的数组存储树的&lt;r&gt;全部节点&lt;/r&gt;，其中前vocab_size个元素为全部的&lt;r&gt;叶子节点&lt;/r&gt;且按词频降序排列；后面的元素表示&lt;r&gt;中间结点&lt;/r&gt;，且初始化词频为1e15。用两个变量&lt;r&gt;pos1&lt;/r&gt;,&lt;r&gt;pos2&lt;/r&gt;记录构造树的情况，其中pos1指向vocab_size-1，pos2指向vocab_size。每次从pos1,pos2选取词频较小的那个结点，并相应的向前或向后移动，更新pos。选择两次，合并两个选中的结点为一棵新树。在构建树的过程中，还需要记录各个节点的编码。&lt;/p&gt;

&lt;p&gt;详见&lt;r&gt;CreateBinaryTree&lt;/r&gt;函数。&lt;/p&gt;

&lt;h4 id=&#34;4-负采样:d45865320239d7818255e82b856775aa&#34;&gt;4. 负采样&lt;/h4&gt;

&lt;p&gt;假设词典中的每个词w对应一个线段l(w),其长度为：&lt;/p&gt;

&lt;p&gt;$$len(w) = \frac{[counter(w)]^{0.75}}{\sum\limits_{u \in D} [counter(u)]^{0.75}} $$&lt;/p&gt;

&lt;p&gt;将这些线段首尾相接拼接在一起，形成一个长度为1的线段。随机往这个线段上打点，则高频词被打中的概率大。将这个单位线段等距离分割成M份(M&amp;gt;&amp;gt;N)，每次生成一个[0,M-1]的随机数r，r对应的词即为采样。&lt;/p&gt;

&lt;p&gt;详见&lt;r&gt;InitUnigramTable&lt;/r&gt;函数。&lt;/p&gt;

&lt;h4 id=&#34;5-学习率:d45865320239d7818255e82b856775aa&#34;&gt;5. 学习率&lt;/h4&gt;

&lt;p&gt;自适应&lt;/p&gt;

&lt;h4 id=&#34;6-多线程:d45865320239d7818255e82b856775aa&#34;&gt;6. 多线程&lt;/h4&gt;

&lt;p&gt;整个工作流程是：首先读取语料库中词表和词频信息，做一些模型初始化工作，然后多线程训练word2vec，判断classes，如果为0，则保存词向量；反之，对得到的词向量进行K-means聚类，聚类个数为classes，并保存聚类结果。&lt;/p&gt;

&lt;p&gt;其中多线程的部分是模型训练的核心，它根据&lt;r&gt;线程数划分文件&lt;/r&gt;，每个线程负责语料库的一部分。首先根据文件指针读取一个句子，用一个数组保存下句子中的每个词；然后根据命令行参数，选择训练CBOW或Skip-gram，其中每个模型都可以采用hs或negative的框架训练。&lt;/p&gt;

&lt;h2 id=&#34;spark-mllib-1-6-实现:d45865320239d7818255e82b856775aa&#34;&gt;Spark MLlib 1.6 实现&lt;/h2&gt;

&lt;h3 id=&#34;可调参数:d45865320239d7818255e82b856775aa&#34;&gt;可调参数&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;参数&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;vectorSize&lt;/td&gt;
&lt;td&gt;词向量的长度&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;learningRate&lt;/td&gt;
&lt;td&gt;学习率&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;numPartitions&lt;/td&gt;
&lt;td&gt;分区数，默认为1，越小准确率越高&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;numIterations&lt;/td&gt;
&lt;td&gt;迭代次数，默认为1，不大于numPartitions&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;seed&lt;/td&gt;
&lt;td&gt;随机数种子&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;minCount&lt;/td&gt;
&lt;td&gt;有效词频的最小值，默认值5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;window&lt;/td&gt;
&lt;td&gt;上下文的窗口大小，默认值5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;主要实现了基于hierarchical softmax框架下的skip-gram模型。&lt;/p&gt;

&lt;h3 id=&#34;广播:d45865320239d7818255e82b856775aa&#34;&gt;广播&lt;/h3&gt;

&lt;p&gt;对于sigmod函数值表格，词信息数组，词的hash表三个变量，通过广播机制，变成全局变量。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val sc = dataset.context //dataset: RDD[Iterable[String]]
val expTable = sc.broadcast(createExpTable)
val bcVocab = sc.broadcast(vocab)
val bcVocabHash = sc.broadcast(vocabHash)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;分布式:d45865320239d7818255e82b856775aa&#34;&gt;分布式&lt;/h3&gt;

&lt;p&gt;根据语料集文件，读取文件中的全部句子，将句子按分区数分发。&lt;/p&gt;

&lt;h2 id=&#34;参考链接:d45865320239d7818255e82b856775aa&#34;&gt;参考链接&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.cnblogs.com/peghoty/p/3857839.html&#34;&gt;word2vec 中的数学原理详解&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1301.3781.pdf&#34;&gt;论文：Efficient Estimation of Word Representations in Vector Space&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&#34;&gt;论文：Distributed Representations of Words and Phrases and their Compositionality&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>