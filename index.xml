<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Learn and record</title>
    <link>http://cherishzhang.github.io/</link>
    <description>Recent content on Learn and record</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 22 Sep 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://cherishzhang.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>计算机基础知识</title>
      <link>http://cherishzhang.github.io/post/cplus/basic/</link>
      <pubDate>Thu, 22 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/cplus/basic/</guid>
      <description>

&lt;h2 id=&#34;tcp-ip参考模型:9f043c0d0d320a105e644fbd6515da42&#34;&gt;TCP/IP参考模型&lt;/h2&gt;

&lt;p&gt;该网络协议共分为四层：&lt;r&gt;网络访问层&lt;/r&gt;、&lt;r&gt;互联网层&lt;/r&gt;、&lt;r&gt;传输层和应用层&lt;/r&gt;。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;互联网层&lt;/strong&gt;使用因特网协议(&lt;r&gt;IP&lt;/r&gt;)，IP地址分别三类，A类(&lt;r&gt;1.0.0.0-126.0.0.0&lt;/r&gt;,共126个)，B类(&lt;r&gt;128.0.0.0-191.255.255.255&lt;/r&gt;,共16382个)，C类(&lt;r&gt;192.0.0.0-223.255.255.255&lt;/r&gt;,达209万余个)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;传输层&lt;/strong&gt;定义了两个&lt;r&gt;端到端&lt;/r&gt;的协议：TCP是面向连接的协议，它提供可靠的报文传输和对上层应用的连接服务；UDP是面向无连接的不可靠传输的协议，主要用于不需要TCP的排序和流量控制等功能的应用程序。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;应用层&lt;/strong&gt;包含所有的高层协议，包括：&lt;strong&gt;虚拟终端协议&lt;/strong&gt;(&lt;r&gt;TELNET&lt;/r&gt;，TELecommunications NETwork)、&lt;strong&gt;文件传输协议&lt;/strong&gt;(&lt;r&gt;FTP&lt;/r&gt;，File Transfer Protocol)、&lt;strong&gt;电子邮件传输协议&lt;/strong&gt;(&lt;r&gt;SMTP&lt;/r&gt;，Simple Mail Transfer Protocol)、&lt;strong&gt;域名服务&lt;/strong&gt;(&lt;r&gt;DNS&lt;/r&gt;，Domain Name Service)、&lt;strong&gt;网上新闻传输协议&lt;/strong&gt;(&lt;r&gt;NNTP&lt;/r&gt;，Net News Transfer Protocol)和&lt;strong&gt;超文本传送协议&lt;/strong&gt;(&lt;r&gt;HTTP&lt;/r&gt;，HyperText Transfer Protocol)等。TELNET允许一台机器上的用户登录到远程机器上，并进行工作；FTP提供有效地将文件从一台机器上移到另一台机器上的方法；SMTP用于电子邮件的收发；DNS用于把主机名映射到网络地址；NNTP用于新闻的发布、检索和获取；HTTP用于在WWW上获取主页。&lt;/p&gt;

&lt;p&gt;&lt;r&gt;ARP(Address Resolution Protocol)表&lt;/r&gt;：IP到MAC的映射，通过目标设备的IP地址，查询目标设备的MAC地址，以保证通信的顺利进行；发送的是广播包。&lt;/p&gt;

&lt;p&gt;&lt;r&gt;路由表&lt;/r&gt;是IP地址可达范围的一张表，相当于是网络里的地图，负责三层的数据转发；&lt;/p&gt;

&lt;p&gt;&lt;r&gt;MAC地址表&lt;/r&gt;是MAC地址和交换机接口的逻辑关系表，负责二层的数据转发。&lt;/p&gt;

&lt;h2 id=&#34;socket通信:9f043c0d0d320a105e644fbd6515da42&#34;&gt;Socket通信&lt;/h2&gt;

&lt;p&gt;本地的进程间通信（IPC）分为：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;消息传递（管道、FIFO、消息队列）&lt;/p&gt;

&lt;p&gt;同步（互斥量、条件变量、读写锁、文件和写记录锁、信号量）&lt;/p&gt;

&lt;p&gt;共享内存（匿名的和具名的）&lt;/p&gt;

&lt;p&gt;远程过程调用（Solaris门和Sun RPC）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;网络中进程通信，使用ip地址(网络层) + 协议(传输层) + 端口(传输层)唯一标识一个进程。&lt;/p&gt;

&lt;h4 id=&#34;tcp连接:9f043c0d0d320a105e644fbd6515da42&#34;&gt;tcp连接&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/tcp.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;三次握手 - 建立连接&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;客户端向服务器发送一个SYN J (请求建立连接)
服务器向客户端响应一个SYN K，并对SYN J进行确认ACK J+1 (确认收到请求)
客户端再向服务器发一个确认ACK K+1 (确认收到确认请求)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;四次握手 - 释放连接&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;一端调用close主动关闭连接，TCP发送一个FIN M；
另一端接收到FIN M，执行被动关闭。它的接收也作为文件结束符传递给应用进程。(FIN的接收意味着应用进程在相应的连接上再也接收不到额外数据)
一段时间后，接收到文件结束符的应用进程调用close关闭它的socket。TCP也发送一个FIN N；
接收到这个FIN的源发送端TCP对它进行确认。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;r&gt;为什么建立连接是3次，释放连接是4次？&lt;/r&gt;&lt;/p&gt;

&lt;p&gt;服务端在LISTEN状态下的SOCKET当收到SYN报文的建连请求后，&lt;r&gt;它可以把ACK和SYN（ACK起&lt;strong&gt;应答&lt;/strong&gt;作用，而SYN起&lt;strong&gt;同步&lt;/strong&gt;作用）放在一个报文里来发送&lt;/r&gt;。但关闭连接时，当收到对方的FIN报文通知时，它仅仅表示对方没有数据发送给你了；但未必你所有的数据都全部发送给对方了，所以你可以未必会马上会关闭SOCKET,也即你可能还需要发送一些数据给对方之后，再发送FIN报文给对方来表示你同意现在可以关闭连接了，所以它这里的ACK报文和FIN报文多数情况下都是分开发送的。&lt;/p&gt;

&lt;h2 id=&#34;fork:9f043c0d0d320a105e644fbd6515da42&#34;&gt;fork&lt;/h2&gt;

&lt;p&gt;一个进程，包括代码、数据和分配给进程的资源。fork通过系统调用创建一个与原来进程几乎完全相同的新进程，系统为新进程分配资源，相当于克隆了一个自己。&lt;/p&gt;

&lt;p&gt;fork调用的一个奇妙之处就是它仅仅被调用一次，却能够返回两次，它可能有三种不同的返回值：&lt;/p&gt;

&lt;p&gt;1）在父进程中，fork返回新创建子进程的进程ID；&lt;br/&gt;
2）在子进程中，fork返回0；&lt;br/&gt;
3）如果出现错误，fork返回一个负值； &lt;br/&gt;&lt;/p&gt;

&lt;h2 id=&#34;浏览器通信:9f043c0d0d320a105e644fbd6515da42&#34;&gt;浏览器通信&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;同源策略&lt;/strong&gt;（Same Origin Policy，SOP）是指浏览器请求的资源必须是同域名、同协议、同端口。不同源的客户端脚本（js，actionscript）在没有明确的授权的情况下不能读取对方的资源。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;跨源资源&lt;/strong&gt;（CrossOrigin Resources Sharing，CORS ）是一种允许多种资源在一个web页面请求域之外的另一个域的资源的协议，是为了让AJAX能够跨域而生的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;内容安全策略&lt;/strong&gt;（Content Security Policy，CSP）是一种白名单机制，限制网站中是否可以包含某来源的内容。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Oauth&lt;/strong&gt;是一个关于授权的开放网络标准，相当于在客户端与服务器之间添加了一个授权层。&lt;/p&gt;

&lt;h2 id=&#34;排序算法:9f043c0d0d320a105e644fbd6515da42&#34;&gt;排序算法&lt;/h2&gt;

&lt;p&gt;稳定排序:(N是数组长度，K是不同键值的数量)&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;名称&lt;/th&gt;
&lt;th&gt;时间复杂度&lt;/th&gt;
&lt;th&gt;空间复杂度&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;bubble&lt;/td&gt;
&lt;td&gt;\(O(N^2)\)&lt;/td&gt;
&lt;td&gt;\(O(1)\)&lt;/td&gt;
&lt;td&gt;比较相邻的两个元素，每次将当前排列中最大的元素冒泡到最后&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Cocktail&lt;/td&gt;
&lt;td&gt;\(O(N^2)\)&lt;/td&gt;
&lt;td&gt;\(O(1)\)&lt;/td&gt;
&lt;td&gt;双向的冒泡排序&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;insertion&lt;/td&gt;
&lt;td&gt;\(O(N^2)\)&lt;/td&gt;
&lt;td&gt;\(O(1)\)&lt;/td&gt;
&lt;td&gt;在一个有序数组中，插入一个给定的数到指定位置&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;bucket&lt;/td&gt;
&lt;td&gt;\(O(N)\)&lt;/td&gt;
&lt;td&gt;需要\(O(K)\)的额外空间&lt;/td&gt;
&lt;td&gt;将数组分到有限数量的桶中，每个桶在分别排序，非比较排序，适合海量的均匀分布数据排序&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;merge&lt;/td&gt;
&lt;td&gt;\(O(NlogN)\)&lt;/td&gt;
&lt;td&gt;需要\(O(N)\)的额外空间&lt;/td&gt;
&lt;td&gt;将两个（或两个以上）有序表合并成一个新的有序表&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;binary tree&lt;/td&gt;
&lt;td&gt;期望时间\(O(NlogN)\)，最坏\(O(N^2)\)&lt;/td&gt;
&lt;td&gt;\(O(N)\)&lt;/td&gt;
&lt;td&gt;二叉搜索树,左子树的所有结点值小于根结点 ,右子树所有结点值大于根结点,左右子树分别为二叉搜索树&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;不稳定排序:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;名称&lt;/th&gt;
&lt;th&gt;时间复杂度&lt;/th&gt;
&lt;th&gt;空间复杂度&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;selection&lt;/td&gt;
&lt;td&gt;\(O(N^2)\)&lt;/td&gt;
&lt;td&gt;\(O(1)\)&lt;/td&gt;
&lt;td&gt;每次从待排序数据中选出最大/小的元素，放在序列的起始位置&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;shell&lt;/td&gt;
&lt;td&gt;\(O(NlogN)\)&lt;/td&gt;
&lt;td&gt;\(O(1)\)&lt;/td&gt;
&lt;td&gt;插入排序的一种，也称缩小增量排序&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;heapsort&lt;/td&gt;
&lt;td&gt;\(O(NlogN)\)&lt;/td&gt;
&lt;td&gt;\(O(1)\)&lt;/td&gt;
&lt;td&gt;利用堆积树，是选择排序的一种&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;quicksort&lt;/td&gt;
&lt;td&gt;期望时间\(O(NlogN)\)，最坏\(O(N^2)\)&lt;/td&gt;
&lt;td&gt;\(O(1)\)&lt;/td&gt;
&lt;td&gt;适合于大的，乱数列表;将数据分成两部分,一部分的所有数据比另一部所有数据都要小,递归排列,直到整组数据有序&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;参考链接:9f043c0d0d320a105e644fbd6515da42&#34;&gt;参考链接&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://baike.baidu.com/link?url=tMNzUW-jPiPum3wRSbZb3mvvLp7BnXqHFVID4cN3TcYo2YGre8uZu80XK1qhvg3lbd4tDrzcxupe_af6XRhNUq&#34;&gt;TCP/IP协议&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.csdn.net/xiaoweige207/article/details/6211577/&#34;&gt;Socket通信简介&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://baike.baidu.com/link?url=2KJOOFEzbpB20KrSK4QVHs01QN7D9zVuf6Fesb4AZiTxGKwlS-vVZzADhIbOhzSNdxoqAREtyVE6FtZysGtJywCYThzSFn-Q3IAWCTLBtfDhyLiY4btPxS5AfjQcGbqR&#34;&gt;排序算法&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[rec] 推荐系统总结</title>
      <link>http://cherishzhang.github.io/post/rec/recommender/</link>
      <pubDate>Thu, 08 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/rec/recommender/</guid>
      <description>

&lt;p&gt;本文是对推荐系统相关知识的一个梳理和总结。主要是参考阿里的一篇技术分享ppt。&lt;/p&gt;

&lt;p&gt;推荐系统的核心问题是&lt;strong&gt;&lt;r&gt;如何评估一个用户(user)对一个物品(item)的评分(喜欢程度)&lt;/r&gt;&lt;/strong&gt;。&lt;/p&gt;

&lt;h2 id=&#34;场景和指标:0ddb80b7e0dcbf864c099ae18a623ad9&#34;&gt;场景和指标&lt;/h2&gt;

&lt;p&gt;推荐系统主要用来解决信息过量负载(&lt;r&gt;information overload&lt;/r&gt;)的问题，可以应用&lt;strong&gt;&lt;r&gt;多种场景&lt;/r&gt;&lt;/strong&gt;中。如:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;音乐，电影&lt;/p&gt;

&lt;p&gt;电子商务中商品推荐&lt;/p&gt;

&lt;p&gt;个性化阅读&lt;/p&gt;

&lt;p&gt;社交好友推荐、朋友圈推荐&lt;/p&gt;

&lt;p&gt;基于位置的服务推荐&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;&lt;r&gt;评价标准&lt;/r&gt;&lt;/strong&gt;：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;名称&lt;/th&gt;
&lt;th&gt;作用&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;用户满意度(&lt;r&gt;user satisfaction&lt;/r&gt;)&lt;/td&gt;
&lt;td&gt;调研或用户反馈；点击率、转化率等&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;准确率(&lt;r&gt;Accuracy&lt;/r&gt;)&lt;/td&gt;
&lt;td&gt;precision/recall/F-score&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;覆盖率(&lt;r&gt;Coverage&lt;/r&gt;)&lt;/td&gt;
&lt;td&gt;长尾分布，尾部物品和用户&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;多样性(&lt;r&gt;Diversity&lt;/r&gt;)&lt;/td&gt;
&lt;td&gt;两两之间不相似&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;新颖性(&lt;r&gt;Novelty&lt;/r&gt;)&lt;/td&gt;
&lt;td&gt;没听过的物品&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;惊喜性(&lt;r&gt;Serendipity&lt;/r&gt;)&lt;/td&gt;
&lt;td&gt;？&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;用户信任度(&lt;r&gt;Trust&lt;/r&gt;)/可解释性(&lt;r&gt;explanation&lt;/r&gt;)&lt;/td&gt;
&lt;td&gt;推荐理由&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;鲁棒性/健壮性(&lt;r&gt;Robustness&lt;/r&gt;)&lt;/td&gt;
&lt;td&gt;哈利波特现象；抗攻击、反作弊&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;实时性(&lt;r&gt;Real-time/online&lt;/r&gt;)&lt;/td&gt;
&lt;td&gt;新加入的物品；新的用户行为(实时意图)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;商业目标(&lt;r&gt;business target&lt;/r&gt;)&lt;/td&gt;
&lt;td&gt;一个用户带来多少盈利&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;&lt;r&gt;影响推荐效果的因素&lt;/r&gt;&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;用户交互界面(User Interface)（用户感知）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数据（收集and处理）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;领域知识（产品定位）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;算法迭代&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;非个性化-推荐-热度排行-popularity:0ddb80b7e0dcbf864c099ae18a623ad9&#34;&gt;&lt;strong&gt;非个性化&lt;/strong&gt;推荐：热度排行(Popularity)&lt;/h2&gt;

&lt;p&gt;排行榜算法&lt;r&gt;主要分为&lt;/r&gt;：&lt;/p&gt;

&lt;p&gt;单一维度的投票；多个维度的综合打分；考虑时间因素(引入衰减权重，如半衰期、冷却定律)；考虑反馈信息(Reddit)；考虑置信度(威尔逊区间)；防止马太效应(MBA)。&lt;/p&gt;

&lt;p&gt;这个方法容易实现，可以解决&lt;r&gt;新用户的冷启动&lt;/r&gt;问题。但无法对用户做出个性化推荐。通常情况下，可以先对新用户做热度排行推荐，然后根据用户的互动历史，做个性化推荐。&lt;/p&gt;

&lt;h2 id=&#34;协同过滤-collaborative-filtering:0ddb80b7e0dcbf864c099ae18a623ad9&#34;&gt;&lt;strong&gt;协同过滤&lt;/strong&gt;(Collaborative Filtering)&lt;/h2&gt;

&lt;p&gt;它的来源很简单，即朋友之间会互相推荐自己喜爱的产品。基本组成元素有：&lt;r&gt;评分矩阵&lt;/r&gt;和&lt;r&gt;相似度度量&lt;/r&gt;。&lt;/p&gt;

&lt;p&gt;主要分为两大类：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Memory-based(neighborhood-based)方法：Item-based/User-based CF&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Model-based方法：频繁项挖掘/聚类/分类/回归/矩阵分解/RBM/图模型&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;h4 id=&#34;user-based:0ddb80b7e0dcbf864c099ae18a623ad9&#34;&gt;User-based&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;计算目标用户的(前k个)相似用户(相似度度量：Pearson,Jaccard,cosine)&lt;/p&gt;

&lt;p&gt;找出相似用户喜欢的物品，并预测目标用户对这些物品的评分(knn, regression)&lt;/p&gt;

&lt;p&gt;过滤掉目标用户已经消费过的物品&lt;/p&gt;

&lt;p&gt;将剩余物品按照预测评分排序，返回top-N物品&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;item-based:0ddb80b7e0dcbf864c099ae18a623ad9&#34;&gt;Item-based&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;计算item之间的两两相似性&lt;/p&gt;

&lt;p&gt;假设预测用户对item的评分，首先找出用户评分过的物品，根据它们与item的相似度预测用户的的评分&lt;/p&gt;

&lt;p&gt;选取用户评分最高的前N个商品进行推荐。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;问题1&lt;/strong&gt;： 比较user-based和item-based？&lt;/p&gt;

&lt;p&gt;Item之间的相似性是静态稳定的；而user之间的相似性是动态复杂的。&lt;/br&gt;
user-based: 可以帮助用户&lt;r&gt;发现新商品&lt;/r&gt;，带来惊喜性。但需要复杂的在线计算，而且无法处理新用户问题。&lt;/br&gt;
item-based: &lt;r&gt;准确性&lt;/r&gt;好，便于离线计算；但推荐缺乏多样性。&lt;/br&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;问题2&lt;/strong&gt;：协同过滤的优缺点？&lt;/p&gt;

&lt;p&gt;优点：模型通用性好，实现简单。&lt;/br&gt;
缺点：&lt;r&gt;冷启动&lt;/r&gt;问题，&lt;r&gt;数据稀疏性&lt;/r&gt;；假定“过去的行为决定现在”，没有考虑&lt;r&gt;具体情景&lt;/r&gt;的差异；&lt;r&gt;热门倾向性&lt;/r&gt;(Popularity Bias)，很难推荐出小众偏好。&lt;/p&gt;

&lt;h4 id=&#34;关联规则:0ddb80b7e0dcbf864c099ae18a623ad9&#34;&gt;关联规则&lt;/h4&gt;

&lt;p&gt;基于物品之间的&lt;r&gt;共现性&lt;/r&gt;挖掘频繁项，应用场景为：买了又买，看了又看。主要有A-priori和FP-growth两种算法。&lt;/p&gt;

&lt;p&gt;支持度：\(s(X,Y) = \frac{包含X,Y的记录数}{总记录数}\)&lt;/p&gt;

&lt;p&gt;置信度：\(c(X,Y) = \frac{包含X,Y的记录数}{包含X的记录数}\)&lt;/p&gt;

&lt;p&gt;关联规则实现简单，通用性较强，适合&lt;strong&gt;商品搭配场景&lt;/strong&gt;；
在相似商品上的推荐效果往往不如协同过滤好；
注意在&lt;strong&gt;变量间关联&lt;/strong&gt;的解释上，由于一些隐含因素的影响，可能得出完全相反的结论(&lt;r&gt;辛普森悖论&lt;/r&gt;)。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;ps: 辛普森悖论，两组数据分别讨论时满足某种性质，一旦合并考虑，却可能导致相反的结论。&lt;r&gt;主要原因&lt;/r&gt;是数据的不同分组之间基数差异很大（通常一个组相对于其他组来说数量占绝对优势）。一个&lt;r&gt;结论&lt;/r&gt;：分析数据时合理地将数据分组很重要。&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&#34;聚类-clustering:0ddb80b7e0dcbf864c099ae18a623ad9&#34;&gt;聚类(Clustering)&lt;/h4&gt;

&lt;p&gt;可以对用户按爱好分群，商品按相似度聚类。常用的算法有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;k-means, 层次聚类&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Louvain, 基于密度的聚类方法&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;聚类可以一定程度上解决&lt;strong&gt;数据稀疏性问题&lt;/strong&gt;，但精准度往往不如协同过滤好。&lt;/p&gt;

&lt;h4 id=&#34;分类-回归:0ddb80b7e0dcbf864c099ae18a623ad9&#34;&gt;分类/回归&lt;/h4&gt;

&lt;p&gt;它的基本思想是把&lt;r&gt;评分预测&lt;/r&gt;看作一个多分类(回归)问题。常用的分类器有LR,Navie Bayes，把item的特征向量作为模型的输入。&lt;/p&gt;

&lt;p&gt;比较通用，可以和其他方法组合，提高预测的准确性；需要大量训练数据，防止过拟合现象。&lt;/p&gt;

&lt;h4 id=&#34;svd:0ddb80b7e0dcbf864c099ae18a623ad9&#34;&gt;SVD&lt;/h4&gt;

&lt;p&gt;它的原理是将(用户,物品)的评分矩阵R，分解成三个矩阵U,S,V。SVD本身要求矩阵必须是稠密矩阵，而评分矩阵非常稀疏；此外，SVD的计算复杂度很高。&lt;/p&gt;

&lt;p&gt;于是，就出现了关于SVD的一系列变形，如FunkSVD,BiasSVD,SVD++。具体可参见开源算法包&lt;r&gt;svdfeture&lt;/r&gt;。&lt;/p&gt;

&lt;h4 id=&#34;item的向量化:0ddb80b7e0dcbf864c099ae18a623ad9&#34;&gt;Item的向量化&lt;/h4&gt;

&lt;p&gt;向量化的目的是将Item进行知识、概念层次上的表达。常用方法有pLSA,LDA,&lt;a href=&#34;http://cherishzhang.github.io/post/word2vec/&#34;&gt;Word2Vec&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;向量化时大多基于行为数据，&lt;r&gt;行为少的item向量化效果不好&lt;/r&gt;。&lt;/p&gt;

&lt;h4 id=&#34;玻尔兹曼机:0ddb80b7e0dcbf864c099ae18a623ad9&#34;&gt;玻尔兹曼机&lt;/h4&gt;

&lt;p&gt;Boltzmann机：两层神经网络(无输出层)；对称、全连接；每个神经元有0/1(激活/未激活)两种状态，某个时刻的状态是随机的，由一定概率确定。&lt;/p&gt;

&lt;p&gt;受限Boltzmann机：受限，层内无连接。&lt;/p&gt;

&lt;p&gt;如何对(user,item)进行建模？一个用户一个RBM，共享隐层，权重W和偏置b。&lt;/p&gt;

&lt;h4 id=&#34;图模型:0ddb80b7e0dcbf864c099ae18a623ad9&#34;&gt;图模型&lt;/h4&gt;

&lt;p&gt;多用于社交网络中人与人的关系挖掘。方法有SimRank,SimRank++,Markov模型。&lt;/p&gt;

&lt;p&gt;借助图的结构的传导性，可以发现协同过滤发现不了的弱相似性，给推荐带来一定的惊喜性。但该模型仍面临数据稀疏性和冷启动问题。&lt;/p&gt;

&lt;h2 id=&#34;基于-内容-content-based:0ddb80b7e0dcbf864c099ae18a623ad9&#34;&gt;基于&lt;strong&gt;内容&lt;/strong&gt;(Content-based)&lt;/h2&gt;

&lt;p&gt;内容包括：&lt;r&gt;文本描述&lt;/r&gt;（通常用NLP技术挖掘关键词）、&lt;r&gt;Item属性&lt;/r&gt;(如电影的主题，职位的行业)、&lt;r&gt;Item特征&lt;/r&gt;(如语音信号表示、图像向量表示)&lt;/p&gt;

&lt;p&gt;基本组成：&lt;r&gt;item特征向量&lt;/r&gt;(如文本的TF-IDF向量)，&lt;r&gt;用户profile向量&lt;/r&gt;(根据用户偏好的items来提取)、&lt;r&gt;匹配分&lt;/r&gt;(cosine,分类/回归模型)&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;方面&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;优点&lt;/td&gt;
&lt;td&gt;能够推荐出用户独有的&lt;r&gt;小众偏好&lt;/r&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;可以一定程度解决&lt;r&gt;数据稀疏&lt;/r&gt;问题和&lt;r&gt;item冷启动&lt;/r&gt;问题&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;具有很好的&lt;r&gt;解释性&lt;/r&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;缺点&lt;/td&gt;
&lt;td&gt;如何提取出有意义的特征&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;很难将不同item的特征组合在一起(邻域思想)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;惊喜性&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;依赖用户profile的准确性&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;混合-方法-hybrid-approaches:0ddb80b7e0dcbf864c099ae18a623ad9&#34;&gt;&lt;strong&gt;混合&lt;/strong&gt;方法(Hybrid Approaches)&lt;/h2&gt;

&lt;p&gt;通常比单个算法好，需要在不同算法之间、理论效果和实际可行性之间权衡。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;方法&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;weighted&lt;/td&gt;
&lt;td&gt;加权&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;switching&lt;/td&gt;
&lt;td&gt;切换，确定一个合理的跳进&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Mixed&lt;/td&gt;
&lt;td&gt;混合&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;feature combination&lt;/td&gt;
&lt;td&gt;特征组合，不同特征组合在一起&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;feature augmentation&lt;/td&gt;
&lt;td&gt;特征扩展，一个模型的输出作为另一个的特征&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;cascade&lt;/td&gt;
&lt;td&gt;级联，粗排 -&amp;gt; 精排&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;一些新进展:0ddb80b7e0dcbf864c099ae18a623ad9&#34;&gt;一些新进展&lt;/h2&gt;

&lt;h3 id=&#34;learning-to-rank:0ddb80b7e0dcbf864c099ae18a623ad9&#34;&gt;Learning to rank&lt;/h3&gt;

&lt;p&gt;将排序看做一个ML问题，评价准则为NDCG(normalized discounted cumulative gain)或MRR(mean reciprocal rank)。算法分为三类：Pointwise, Pairwise, Listwise。&lt;/p&gt;

&lt;h3 id=&#34;页面整体优化-page-optimization:0ddb80b7e0dcbf864c099ae18a623ad9&#34;&gt;页面整体优化(Page Optimization)&lt;/h3&gt;

&lt;p&gt;关于用户注意力建模的一篇文章Modeling User Attention and Interaction on the Web。&lt;/p&gt;

&lt;h3 id=&#34;情景推荐-context-aware:0ddb80b7e0dcbf864c099ae18a623ad9&#34;&gt;情景推荐(Context-aware)&lt;/h3&gt;

&lt;p&gt;针对不同推荐场景，情景也有所不同。看电影，是周末还是工作日？订餐，餐厅距离？网购，用户心情？母婴：孩子年龄？&lt;/p&gt;

&lt;p&gt;算法主要有&lt;r&gt;张量分解(Tensor Factorization)&lt;/r&gt;，&lt;r&gt;分解机(Factorization Machine)&lt;/r&gt;。其中后者的效果更好。&lt;/p&gt;

&lt;h3 id=&#34;深度学习:0ddb80b7e0dcbf864c099ae18a623ad9&#34;&gt;深度学习&lt;/h3&gt;

&lt;p&gt;主要用到的有卷积神经网络(CNN)和循环神经网络(RNN)。&lt;/p&gt;

&lt;h2 id=&#34;参考链接:0ddb80b7e0dcbf864c099ae18a623ad9&#34;&gt;参考链接&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://mp.weixin.qq.com/s?__biz=MzIzOTU0NTQ0MA==&amp;amp;mid=2247483811&amp;amp;idx=1&amp;amp;sn=fc3ee4ddfc4a8d6014a4cd90cdb5983c&#34;&gt;常用推荐算法&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://mp.weixin.qq.com/s?__biz=MzA5NzkxMzg1Nw==&amp;amp;mid=2653160714&amp;amp;idx=1&amp;amp;sn=7e53c70b3567f2277206dd5ba5347591&#34;&gt;百分点亿级个性化推荐系统的发展历程和实践架构&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spark之RDD</title>
      <link>http://cherishzhang.github.io/post/spark/sparkRDD/</link>
      <pubDate>Sat, 20 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/spark/sparkRDD/</guid>
      <description>

&lt;h3 id=&#34;rdd:1676227db8a33b6ba8c0a321eafd5577&#34;&gt;RDD&lt;/h3&gt;

&lt;p&gt;RDD，全称为Resilient Distributed Datasets（弹性分布式数据集）。它是一个&lt;r&gt;容错&lt;/r&gt;且&lt;r&gt;并行&lt;/r&gt;的数据结构，可以让用户&lt;r&gt;显式&lt;/r&gt;地将数据存储到磁盘和内存中，并能控制数据的&lt;r&gt;分区&lt;/r&gt;。它只能基于在稳定物理存储中的数据集和其他已有的RDD上执行确定性操作(transformation)来创建。&lt;/p&gt;

&lt;p&gt;举个例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//用普通数组[1,2,...,9]创建RDD，数据分别在3个分区中
val rdd1 = sc.parallelize(1 to 9, 3)
//读取文件创建RDD，每一行就是RDD的一个元素
val rdd2 = sc.textFile(&amp;quot;README.md&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由此可见，RDD&lt;r&gt;本质&lt;/r&gt;是一个&lt;r&gt;只读的分区记录集合&lt;/r&gt;。一个RDD包含多个分区，每个分区是一个dataset片段。&lt;/p&gt;

&lt;p&gt;相关操作函数如下表：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val rdd1 = sc.parallelize(1 to 9, 3)
val rdd2 = sc.parallelize(list(&amp;quot;dog&amp;quot;,&amp;quot;tiger&amp;quot;,&amp;quot;lion&amp;quot;,&amp;quot;cat&amp;quot;,&amp;quot;parther&amp;quot;,&amp;quot;eagle&amp;quot;),2)
val rdd3 = sc.parallelize(List(1,2),(3,4),(3,6)))
&lt;/code&gt;&lt;/pre&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;函数名&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;输入与输出&lt;/th&gt;
&lt;th&gt;Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;map&lt;/td&gt;
&lt;td&gt;对RDD中的每个元素执行一个指定的函数来产生一个新的RDD，映射函数的参数为RDD中的每一个元素&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;一对一&lt;/td&gt;
&lt;td&gt;val a = rdd1.map(x =&amp;gt; x*2)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;mapValues&lt;/td&gt;
&lt;td&gt;对象是元素为kv对的RDD，key保持不变，对value进行映射&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;一对一&lt;/td&gt;
&lt;td&gt;val b = rdd2.map(x =&amp;gt; (x.length, x)) &lt;br\&gt; b.mapValues(&amp;ldquo;x&amp;rdquo; + _ + &amp;ldquo;x&amp;rdquo;).collect&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;flatMap&lt;/td&gt;
&lt;td&gt;扁平化map后的结果，多个集合合并为一个集合&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;一对一&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;mapPartitions&lt;/td&gt;
&lt;td&gt;映射函数的参数为RDD中&lt;strong&gt;每一个分区&lt;/strong&gt;的迭代器&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;一对一&lt;/td&gt;
&lt;td&gt;可用于为RDD中数据按分区创建连接&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;reduce&lt;/td&gt;
&lt;td&gt;将RDD中元素两两传递给输入函数，产生一个新值，新产生的值与RDD中的下一个元素再被传递给输入函数，&lt;strong&gt;直到最后只有一个值为止&lt;/strong&gt;。&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;多对一&lt;/td&gt;
&lt;td&gt;rdd1.reduce((x,y) =&amp;gt; x+y) //结果为45&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;reduceByKey&lt;/td&gt;
&lt;td&gt;将key相同的元素的value进行reduce，组成一个新的KV对&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;多对一&lt;/td&gt;
&lt;td&gt;rdd3.reduceByKey((x,y) =&amp;gt; x+y).collect //结果为Array((1,2),(3,10))&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;filter&lt;/td&gt;
&lt;td&gt;对每个元素应用f函数，只保留返回值为true的元素&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;子集型&lt;/td&gt;
&lt;td&gt;val d = rdd3.filter{ case(x,y) =&amp;gt; equal(x,y) }.count() //结果为0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;sample&lt;/td&gt;
&lt;td&gt;对元素采样，获取子集&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;子集型&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;cache&lt;/td&gt;
&lt;td&gt;将元素从磁盘缓存到内存&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;cache型&lt;/td&gt;
&lt;td&gt;rdd1.cache()&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;persist&lt;/td&gt;
&lt;td&gt;对RDD缓存，位置可指定&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;cache型&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;通常来讲，针对数据处理有几种常见模型，包括：&lt;r&gt;Iterative Algorithms&lt;/r&gt;,&lt;r&gt;Relational Queries&lt;/r&gt;,&lt;r&gt;MapReduce&lt;/r&gt;,&lt;r&gt;Stream Processing&lt;/r&gt;。例如Hadoop MapReduce采用了MapReduces模型，Storm则采用了Stream Processing模型。RDD混合了这四种模型，使得Spark可以应用于各种大数据处理场景。&lt;/p&gt;

&lt;p&gt;Spark将依赖分为narrow与wide。如果RDD的每个分区最多只能被一个Child RDD的一个分区使用，则称之为narrow dependency；若多个Child RDD分区都可以依赖，则称之为wide dependency。不同的操作依据其特性，可能会产生不同的依赖。例如map操作会产生narrow dependency，而join操作则产生wide dependency。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/rdd.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图说明了narrow和wide之间的区别。 narrow dependencies可以支持在同一个cluster node上以管道形式执行多条命令，而且它的失败恢复更有效，只需要重新计算丢失的parent partition即可；wide dependencies需要所有的父分区都是可用的，牵涉到RDD各级的多个Parent Partitions。&lt;/p&gt;

&lt;h3 id=&#34;共享变量:1676227db8a33b6ba8c0a321eafd5577&#34;&gt;共享变量&lt;/h3&gt;

&lt;p&gt;&lt;r&gt;broadcast&lt;/r&gt;变量：只读的共享变量 每个节点上都有一个拷贝。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val broadcastVar = sc.broadcast(&amp;quot;string test&amp;quot;) //broadcast variable is readonly
val v = broadcastVar.value
println(v)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;r&gt;accumulator&lt;/r&gt;变量：做累加器用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val accum = sc.accumulator(0, &amp;quot;My Accumulator&amp;quot;) //value and name
sc.parallelize(1 to 1000000).foreach(x =&amp;gt; accum+= 1)
println(accum.name + &amp;quot;:&amp;quot; + accum.value)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>[ml] word2vec模型和源码解析</title>
      <link>http://cherishzhang.github.io/post/ml/word2vec/</link>
      <pubDate>Fri, 05 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/ml/word2vec/</guid>
      <description>

&lt;h2 id=&#34;词向量:d45865320239d7818255e82b856775aa&#34;&gt;词向量&lt;/h2&gt;

&lt;p&gt;一种简单直观的词表示方法是&lt;r&gt;One-hot&lt;/r&gt;编码，用N位对N个词编码，每个词对应的N维向量中，只有一维为0。这种方式的缺陷是&lt;r&gt;词汇鸿沟&lt;/r&gt;，即词与词之间相互孤立，忽略了它们之间的联系。而word2vec中的词向量是一种distributed representation，它的&lt;r&gt;特点&lt;/r&gt;是&lt;strong&gt;利用距离刻画词之间的相似性&lt;/strong&gt;。&lt;/p&gt;

&lt;h2 id=&#34;统计语言模型:d45865320239d7818255e82b856775aa&#34;&gt;统计语言模型&lt;/h2&gt;

&lt;p&gt;统计语言模型是用来计算一个句子的概率的&lt;r&gt;概率模型&lt;/r&gt;，经典的有HAL,LSA,COALS等。通常它基于一个语料库来构建。&lt;/p&gt;

&lt;h3 id=&#34;句子的概率:d45865320239d7818255e82b856775aa&#34;&gt;句子的概率&lt;/h3&gt;

&lt;p&gt;给定一个由T个词\(w_1,w_2,&amp;hellip;,w_T\)按顺序构成的句子，根据Bayes，计算该句子的概率p(W)&lt;/p&gt;

&lt;p&gt;$$p(W) = p(w_1^T) = p(w_1,w_2,&amp;hellip;,w_T) = p(w_1)*p(w_2|w_1)*p(w_3|(w_1^2))&amp;hellip;p(w_T|(w_1^{T-1}))$$&lt;/p&gt;

&lt;p&gt;其中等式中的各个因子（条件概率）就是语言模型的&lt;strong&gt;参数&lt;/strong&gt;，常用的语言模型都是在近似求p(W)。如果给定一个词典大小为N的语料库，考虑长度为T的任意句子，理论上有\(N^T\)种可能组合，而每个句子需要计算T个参数，总共需要计算&lt;r&gt;\(T*N^T\)&lt;/r&gt;个参数。无论是计算或保存，都需要很大开销。&lt;/p&gt;

&lt;h3 id=&#34;n-gram模型:d45865320239d7818255e82b856775aa&#34;&gt;n-gram模型&lt;/h3&gt;

&lt;p&gt;针对参数繁多，该模型做了一个简化的假定，即一个词出现的概率&lt;strong&gt;只与&lt;/strong&gt;它前面的n-1个词相关。实际中最多采用&lt;r&gt;n=3&lt;/r&gt;的三元模型。&lt;/p&gt;

&lt;p&gt;$$(p(w_k|w^{k-1}_1) = p(w_k|w^{k-1}_{k-n+1}) = \frac{count(w_{k-n+1}^k)}{count(w_{k-n+1}^{k-1})}$$&lt;/p&gt;

&lt;p&gt;如果过程中出现\(count(w_{k-n+1}^k)\) = \(count(w_{k-n+1}^{k-1})\) 或者 \(count(w_{k-n+1}^{k-1}) = 0\)，则需要做&lt;r&gt;平滑处理&lt;/r&gt;。&lt;/p&gt;

&lt;p&gt;n-gram模型的缺陷是无法建模更远的关系，语料的不足使得无法训练更高阶的语言模型；而且无法建模出词之间的相似度。&lt;/p&gt;

&lt;h3 id=&#34;最大似然:d45865320239d7818255e82b856775aa&#34;&gt;最大似然&lt;/h3&gt;

&lt;p&gt;对句子模型的另一种转化是利用机器学习，构造一个目标函数，并最大似然该函数。优化目标求得最优参数，利用最优参数预测未知句子的概率。&lt;strong&gt;&lt;r&gt;目标函数&lt;/r&gt;&lt;/strong&gt;一般为: $$\prod_{w \in C} p(w|Context(w))$$&lt;/p&gt;

&lt;p&gt;其中C表示&lt;r&gt;语料集&lt;/r&gt;，Context(w)表示词w的&lt;r&gt;上下文&lt;/r&gt;，即w周边词的集合。实际中采用最大对数似然，即\(\sum\limits_{w \in C} log p(w|Context(w))\)最大化。&lt;strong&gt;增加log是为了将乘法运算转换为加法运算(便于求导的常用方法)&lt;/strong&gt;。问题关键在于如何构造F函数。&lt;/p&gt;

&lt;p&gt;$$p(w|Context(w)) = F(w,Context(w),\theta)$$&lt;/p&gt;

&lt;h2 id=&#34;神经概率语言模型:d45865320239d7818255e82b856775aa&#34;&gt;神经概率语言模型&lt;/h2&gt;

&lt;p&gt;Bengio(03) - 《A Neural Probabilistic Language Model》&lt;/p&gt;

&lt;p&gt;神经网络包含四层：&lt;r&gt;输入层&lt;/r&gt;，&lt;r&gt;投影层&lt;/r&gt;，&lt;r&gt;隐藏层&lt;/r&gt;和&lt;r&gt;输出层&lt;/r&gt;。W,U为权值矩阵，p,q为偏置向量。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/word2vec-neural.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;取&lt;r&gt;二元对(Context(w),w)&lt;/r&gt;作为一个训练样本，其中Context(w)取前面n-1个词，每个词\(v_i\)用一个m维向量表示。投影层是一个(n-1)*m长度的向量，输出层的规模为词库中词表的长度D。&lt;/p&gt;

&lt;p&gt;选用双曲正切函数作为隐藏层的激活函数，计算过程为：&lt;/p&gt;

&lt;p&gt;$$z_w = tanh(Wx_w + p), y_w = Uz_w + q$$&lt;/p&gt;

&lt;p&gt;输出为一个长度为D的向量\(y_w = (y_{w,1}, y_{w,2},&amp;hellip;,y_{w,D})^T\)，通过softmax归一化，每个分量表示上下文为Context(w)时下一词为词典第i个词的概率。&lt;/p&gt;

&lt;p&gt;&lt;r&gt;待确定的参数集\(\Theta\)&lt;/r&gt;包括：&lt;strong&gt;词向量和填充向量，神经网络参数W,U,p,q&lt;/strong&gt;。其中大部分&lt;strong&gt;计算集中&lt;/strong&gt;在&lt;r&gt;隐藏层和输出层之间的矩阵向量运算&lt;/r&gt;和&lt;r&gt;softmax归一化&lt;/r&gt;上。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;关于softmax归一化：
形式为：$$ P(y=i) = \frac{exp(\sum_d w_{id}x_{d})}{\sum_j exp(\sum_d w_{jd} x_{d})} $$
主要是针对多分类问题，归一化输出概率值。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;word2vec核心:d45865320239d7818255e82b856775aa&#34;&gt;Word2vec核心&lt;/h2&gt;

&lt;p&gt;word2vec中用到了两个重要模型: &lt;r&gt;CBOW&lt;/r&gt;和&lt;r&gt;Skip-gram&lt;/r&gt;，对于这两个模型，分别都给出了基于&lt;r&gt;Hierarchical Softmax&lt;/r&gt;和&lt;r&gt;Negative Sampling&lt;/r&gt;两套框架的实现。而word2vec的高效具体体现在这两个策略上。&lt;/p&gt;

&lt;p&gt;CBOW和Skip-gram包括三层：&lt;r&gt;输入层&lt;/r&gt;，&lt;r&gt;投影层&lt;/r&gt;，&lt;r&gt;输出层&lt;/r&gt;。前者是已知词的上下文预测当前词，后者是通过当前词预测上下文。&lt;/p&gt;

&lt;h3 id=&#34;1-cbow:d45865320239d7818255e82b856775aa&#34;&gt;1. CBOW&lt;/h3&gt;

&lt;p&gt;CBOW = Continuous Bag-of-Words, 目标函数为：&lt;/p&gt;

&lt;p&gt;$$\sum_{w \in c} log p(w|Context(w)) = \sum_{t=1}^T log p(w_t|\tau(w_{t-k},w_{t-k-1},&amp;hellip;,w_{t+k-1},w_{t+k}))$$&lt;/p&gt;

&lt;p&gt;其中T表示整个词典的大小，模型的目标是&lt;r&gt;最大化&lt;/r&gt;这个目标函数值。\(w_t\)表示词典中的一个词，即通过和\(w_t\)相邻的窗口大小为k的词来预测\(w_t\)出现的概率。其中\(\tau(w_1,w_2,&amp;hellip;,w_k)\)函数表示以\(w_i(i&amp;lt;=i&amp;lt;=k)\)为参数进行某种运算。在Word2vec里，这种运算是向量加和运算。即&lt;r&gt;把窗口相邻的所有词的向量加和&lt;/r&gt;。&lt;/p&gt;

&lt;h4 id=&#34;hierarchical-softmax:d45865320239d7818255e82b856775aa&#34;&gt;Hierarchical Softmax&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/word2vec-hs-cbow.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;相比于之前的神经概率语言模型，主要有&lt;r&gt;三处改动&lt;/r&gt;：&lt;strong&gt;去掉隐藏层；首尾拼接改为累加求和；输出为树结构。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;问题1&lt;/strong&gt;：huffman树的应用原理，及hierarchical softmax到底指什么？&lt;/p&gt;

&lt;p&gt;hs可以保证叶节点输出的概率值是归一化的。将二叉树中的每个中间结点看作一个隐含的二分类器，一个词的huffman编码就是多个隐含二分类器作用的结果。最后输出层得到的是在当前上下文（输入层）的前提下，各个词的概率，softmax是指这些概率之和为1，即归一化。证明如下：当树的高度为2，即根节点连接两个叶子节点，设左叶子节点的概率为p，则右叶子节点概率为1-p。它们的概率和为1；当树的高度为k时，从根到叶子节点的一个路径可以看成k个独立事件的一种发生可能，所有的叶子节点刚好包括了全部可能情况，概率总和为1。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;问题2&lt;/strong&gt;：简单描述一下模型流程？&lt;/p&gt;

&lt;p&gt;根据上图，首先从训练集中选取一个sentence，对于其中的一个词w，计算其上下文词向量的加和，得到\(X_w\)向量(长度为词向量的维度)，即投影层。对于huffman树中的每个非叶子节点，添加一个辅助向量\(\theta\)，\(X_w\)和\(\theta\)层层作用后，可以得到每个叶子节点的概率。计算输出概率与真实值的残差，然后用随机梯度优化参数，使残差尽可能小。在模型训练过程中，可以得到最优参数下的词向量集合。这些词向量就是word2vec。&lt;/p&gt;

&lt;p&gt;================================================================&lt;/p&gt;

&lt;p&gt;关于模型的&lt;r&gt;推导&lt;/r&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;结点的带权路径长度&lt;/strong&gt;: 从根结点到该结点的路径长度乘以该结点的权值&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;树的带权路径长度&lt;/strong&gt;: 所有叶结点的带权路径长度之和&lt;/p&gt;

&lt;p&gt;Huffman树是在给定n个叶子结点和其对应权值的前提下，构造一棵二叉树，使该树的带权路径长度最小。已知语料库中各词的词频，可以构建一棵二叉树，每个词作为树的叶子节点，并保证词频越大的词离根节点越近。每个叶子对应的&lt;r&gt;huffman编码&lt;/r&gt;就是该词的编码。约定左子结点编码为0，右子结点编码为1。&lt;/p&gt;

&lt;p&gt;此外为每个非叶子结点设置一个&lt;r&gt;辅助向量&lt;/r&gt;\(\theta\)。&lt;/p&gt;

&lt;p&gt;对于词w，从根结点到该叶结点的路径中，假设有\(l^w\)层，经历了\(l^w-1\)次二分类，则条件概率\( p(w|Context(w)) = \prod\limits_{j=2}^{l^w} p(d_j^w|X_w,\theta_{j-1}^w) \), \(d_j^w\)表示第j个结点对应的编码，\(\theta_j^w\)表示第j个结点对应的辅助向量。&lt;/p&gt;

&lt;p&gt;第一步，&lt;strong&gt;目标函数对数化&lt;/strong&gt;:
$$F = \sum_{w \in C} log p(w|Context(w)) = \sum_{w \in C} log \prod\limits_{j=2}^{l^w} \sigma(\theta_{j-1}^w X_w)^{1-{d_j^w}} (1-\sigma(\theta_{j-1}^w X_w))^{d_j^w} $$&lt;/p&gt;

&lt;p&gt;第二步，&lt;strong&gt;取内部项F(w,u)&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;$$ F(w,u) =  (1-{d_j^w}) log(\sigma(\theta_{j-1}^w)) + {d_j^w} log(1-\sigma(\theta_{j-1}^w X_w))$$&lt;/p&gt;

&lt;p&gt;第三步，&lt;strong&gt;随机梯度上升&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;\(\theta_{j-1}^w\)的更新公式为：&lt;/p&gt;

&lt;p&gt;$$ \frac{\partial F(w,u)}{\partial \theta_{j-1}^w} = (1 - d_j^w - \sigma(X_w^T*\theta_{j-1}^w)) X_w$$&lt;/p&gt;

&lt;p&gt;$$ \theta_{j-1}^w := \theta_{j-1}^w + \eta (1 - d_j^w - \sigma(X_w^T*\theta_{j-1}^w)) X_w $$&lt;/p&gt;

&lt;p&gt;\(X_w\)和\(\theta_{j-1}^w\)具有对称性，更新公式类似，这里注意更新的\(v(\tilde{w})\)而非加和后的\(X_w\)：&lt;/p&gt;

&lt;p&gt;$$v(\tilde{w}) := v(\tilde{w}) +  \eta \sum\limits_{j=2}^{l^w}(1 - d_j^w - \sigma(X_w^T*\theta_{j-1}^w)) \theta_{j-1}^w, \tilde{w} \in Context(w)$$&lt;/p&gt;

&lt;p&gt;================================================================&lt;/p&gt;

&lt;h4 id=&#34;negative-sampling:d45865320239d7818255e82b856775aa&#34;&gt;Negative Sampling&lt;/h4&gt;

&lt;p&gt;huffman树中从根节点到叶子节点路径上的各个中间结点，分为正负两类，构造好huffman树后，每个词w对应的采样样本（中间结点）也就确定。训练的目标是最大化似然函数值。由此衍生出，用简单的&lt;r&gt;随机负采样&lt;/r&gt;代替复杂的huffman树。已知词w的上下文，预测词w，可将w看作正样本，负样本的抽样策略为&lt;r&gt;带权采样&lt;/r&gt;，即被选中的概率与词频成正比。模型训练的&lt;r&gt;目标&lt;/r&gt;是&lt;strong&gt;最大化正样本的概率同时最小化负样本的概率&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;设NEG(w)是关于词w的一个负样本集，对已一个给定的&lt;r&gt;二元对&lt;/r&gt;(Context(w),w),&lt;r&gt;优化目标&lt;/r&gt;是&lt;/p&gt;

&lt;p&gt;$$ g(w) = \prod\limits_{u \in w \cup NEG(w)} p(u|Context(w))$$&lt;/p&gt;

&lt;p&gt;=================================================================&lt;/p&gt;

&lt;p&gt;关于模型的&lt;r&gt;推导&lt;/r&gt;:&lt;/p&gt;

&lt;p&gt;为每个词设置一个&lt;r&gt;辅助向量\(\theta^u\)&lt;/r&gt;，\(X_w^T\)表示为Context(w)中的k个词进行加和后的向量，\(L^w(u)\)表示词u的样本标签，正样本取1，反之取0。&lt;/p&gt;

&lt;p&gt;第一步，&lt;strong&gt;转换目标函数&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;$$g(w) = \prod\limits_{u \in w \cup NEG(w)} [\sigma(\theta^u X_w^T)]^{L^w(u)} [1 - \sigma(\theta^u X_w^T)] ^ {(1-L^w(u))}$$&lt;/p&gt;

&lt;p&gt;因为正样本只有一个，故可简化为\(g(w) =\sigma(\theta^u X_w^T) \prod\limits_{u \cup NEG(w)} [1 - \sigma(\theta^u X_w^T)] ^ {1-L^w(u)}\)&lt;/p&gt;

&lt;p&gt;第二步，&lt;strong&gt;考虑整个语料库&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;$$F = log G = log \prod\limits_{w \in C} g(w)$$
$$ = \sum\limits_{w \in C} \sum\limits_{u \in w \cup NEG(w)} {L^w(u)}log[\sigma(\theta^u X_w^T)] + (1-L^w(u))log[1 - \sigma(\theta^u X_w^T)]$$
$$ = \sum\limits_{w \in C} \bigg (log(\sigma(\theta^u X_w^T)) + \sum\limits_{u \in NEG(w)} log(\sigma(-\theta^u X_w^T))\bigg)$$&lt;/p&gt;

&lt;p&gt;第三步，&lt;strong&gt;随机梯度上升&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;$$ F(w,u) = L^w(u) log[\sigma(\theta^u X_w^T)] + (1-L^w(u)) log[1 - \sigma(\theta^u X_w^T)] $$&lt;/p&gt;

&lt;p&gt;\(\theta^u\)的&lt;r&gt;更新公式&lt;/r&gt;：&lt;/p&gt;

&lt;p&gt;$$\frac{\partial F(w,u)}{\partial \theta^u}  = L^w(u) (1 -\sigma(\theta^u X_w^T)) X_w - (1-L^w(u)) \sigma(\theta^u X_w^T) X_w $$
$$  =  L^w(u) X_w - \sigma(\theta^u X_w^T) X_w $$&lt;/p&gt;

&lt;p&gt;$$\theta^u := \theta^u + \eta (L^w(u) - \sigma(\theta^u X_w^T)) X_w$$&lt;/p&gt;

&lt;p&gt;根据\(\theta^u\)和\(X_w\)的&lt;r&gt;对称性&lt;/r&gt;，w上下文中的每个词向量的&lt;r&gt;更新公式&lt;/r&gt;为：&lt;/p&gt;

&lt;h1 id=&#34;v-tilde-w-v-tilde-w-sum-limits-u-in-w-cup-neg-w-eta-l-w-u-sigma-theta-u-x-w-t-theta-u-tilde-w-in-context-w:d45865320239d7818255e82b856775aa&#34;&gt;$$v(\tilde{w}) := v(\tilde{w}) + \sum\limits_{u \in w \cup NEG(w)} \eta (L^w(u) - \sigma(\theta^u X_w^T))\theta^u, \tilde{w} \in Context(w)$$&lt;/h1&gt;

&lt;h3 id=&#34;2-skip-gram:d45865320239d7818255e82b856775aa&#34;&gt;2. SKIP-GRAM&lt;/h3&gt;

&lt;h4 id=&#34;hierarchical-softmax-1:d45865320239d7818255e82b856775aa&#34;&gt;Hierarchical Softmax&lt;/h4&gt;

&lt;p&gt;与CBOW不同是，输入层只有一个词向量w。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/word2vec-hs-sg.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;关键在于\(p(Context(w)|w)\)的定义：&lt;/p&gt;

&lt;p&gt;$$ p(Context(w)|w) = \prod\limits_{u \in Context(w)} p(u|w) $$，其他推导类似CBOW。&lt;/p&gt;

&lt;h4 id=&#34;negative-sampling-1:d45865320239d7818255e82b856775aa&#34;&gt;Negative Sampling&lt;/h4&gt;

&lt;p&gt;已知词w，预测词w的上下文，则针对上下文中的每个词，将该词看作正样本，选取一定数量的负样本。目标函数为：&lt;/p&gt;

&lt;p&gt;$$F = log G = log \prod\limits_{w \in C} g(w)$$
$$ = \sum\limits_{w \in C} \sum\limits_{u \in Context(w)} \sum\limits_{z \in u \cup NEG(u)} {L^u(z)}log[\sigma(\theta^z X_w^T)] + (1-L^u(z))log[1 - \sigma(\theta^z X_w^T)]$$&lt;/p&gt;

&lt;p&gt;公式中对于一个样本(w, Context(w)), 对Context(w)中的每个词，都做了一次负采样。而word2vec源码对此作了改动，只对词w进行了|Context(w)|次负采样。&lt;/p&gt;

&lt;h2 id=&#34;c源码阅读:d45865320239d7818255e82b856775aa&#34;&gt;c源码阅读&lt;/h2&gt;

&lt;h3 id=&#34;命令行参数:d45865320239d7818255e82b856775aa&#34;&gt;命令行参数&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;参数&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;size&lt;/td&gt;
&lt;td&gt;Projection Layer的长度，即词向量的长度&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;train&lt;/td&gt;
&lt;td&gt;语料库地址，即训练集&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;save-vocab&lt;/td&gt;
&lt;td&gt;词典保存地址&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;read-vocab&lt;/td&gt;
&lt;td&gt;词典读取地址，如果没指定，从语料库中学习词典&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;debug&lt;/td&gt;
&lt;td&gt;debug_mode，debug信息打印控制&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;td&gt;是否保存为二进制&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;cbow&lt;/td&gt;
&lt;td&gt;确定是cbow，还是skip-gram模型&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;alpha&lt;/td&gt;
&lt;td&gt;学习率&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;模型保存地址&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;window&lt;/td&gt;
&lt;td&gt;窗口大小，默认值5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;sample&lt;/td&gt;
&lt;td&gt;采样百分比&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;hs&lt;/td&gt;
&lt;td&gt;是否进行Hierarchical Softmax&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;negative&lt;/td&gt;
&lt;td&gt;是否进行负采样&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;threads&lt;/td&gt;
&lt;td&gt;线程数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;min-count&lt;/td&gt;
&lt;td&gt;有效词频的最小值，默认值5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;classes&lt;/td&gt;
&lt;td&gt;聚类个数&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;数据结构:d45865320239d7818255e82b856775aa&#34;&gt;数据结构&lt;/h3&gt;

&lt;p&gt;每个词用结构体&lt;r&gt;vocab_word&lt;/r&gt;保存信息:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct vocab_word{
    char *word; //词本身
    long long cn; //语料集中的词频
    int *point; //huffman树中，从根节点到该词所在叶节点的中间结点列表
    char *code, codelen; //该词对应的huffman编码和编码长度
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;整个词表用一个数组vocab表示，为了&lt;strong&gt;快速查找&lt;/strong&gt;给定词的信息，建立了一个&lt;r&gt;hash表&lt;/r&gt;，存放的元素是该词在vocab中的下标。&lt;/p&gt;

&lt;p&gt;哈希的策略是&lt;strong&gt;线性探测的开放定址法&lt;/strong&gt;，计算词w的哈希值hv(w)作为下标索引，若该位置被占用，则顺序往下找，直到找到一个未被占用的位置。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vocab = (struct vocab_word *)calloc(vocab_max_size, sizeof(struct vocab_word));
vocab_hash = (int *)calloc(vocab_hash_size, sizeof(int));
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;模型实现:d45865320239d7818255e82b856775aa&#34;&gt;模型实现&lt;/h3&gt;

&lt;h4 id=&#34;1-关于net:d45865320239d7818255e82b856775aa&#34;&gt;1. 关于Net&lt;/h4&gt;

&lt;p&gt;对于hs来说，参数主要有&lt;r&gt;词向量&lt;/r&gt;(syn0)，&lt;r&gt;huffman树中间结点的辅助向量&lt;/r&gt;(syn1)；对于skip-gram来说，参数主要有&lt;r&gt;词向量&lt;/r&gt;(syn0),&lt;r&gt;负采样中每个词的辅助向量&lt;/r&gt;(syn1neg)。&lt;/p&gt;

&lt;p&gt;关于初始化，syn0采用&lt;strong&gt;零初始化&lt;/strong&gt;，syn1,syn1neg采用&lt;strong&gt;随机初始化&lt;/strong&gt;([\(-\frac{-0.5}{size}, \frac{0.5}{size}\)])，其中size为词向量的长度。&lt;/p&gt;

&lt;p&gt;代码中Hierarchical Softmax和Negative Sampling可以同时训练。neu1代表\(X_w\),即输入词向量加和后的向量；neu1e代表\(e\)，即huffman树每一条路径（从根到叶子节点）梯度更新的求和，用来更新相应的词向量。&lt;/p&gt;

&lt;h4 id=&#34;2-sigmoid函数值计算:d45865320239d7818255e82b856775aa&#34;&gt;2. sigmoid函数值计算&lt;/h4&gt;

&lt;p&gt;建模过程中用到&lt;r&gt;逻辑回归&lt;/r&gt;做二分类问题，需要计算sigmod函数值。程序中采用的&lt;r&gt;近似计算&lt;/r&gt;的方法，将区间&lt;r&gt;(-6,-6)&lt;/r&gt;等分成若干份(1000)，事先将每个结点处的函数值保存起来，对于某个x，判断x的范围，如果小于-6，返回0；如果大于6；返回1；否则对x取整查表，返回对应值。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;sigmoid函数中涉及指数运算，指数运算利用幂级数展开式实现，如果展开项数过多，则计算比较耗时。&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&#34;3-词频和窗口:d45865320239d7818255e82b856775aa&#34;&gt;3. 词频和窗口&lt;/h4&gt;

&lt;p&gt;删除小于阈值(min_count)的低频词，对高频词采用&lt;r&gt;subsampling&lt;/r&gt;，可以提高训练速度和词向量精度。具体做法：给定一个阈值t，对于词w，以\(prob(w) = 1-(\sqrt{(\frac{t}{f(w)})} + \frac{t}{f(w)})\)的概率舍弃。&lt;/p&gt;

&lt;p&gt;模型训练以行为单位进行，对于一个给定行，包含T个词，每个词为一个训练样本，对于某个词w，定义context(w)为：设置一个窗口阈值参数window(5)，每次构造context(w)时，生成一个随机数\(c \in [1, window]\)，w前后各取c个词构成context。&lt;/p&gt;

&lt;h4 id=&#34;4-创建huffman树:d45865320239d7818255e82b856775aa&#34;&gt;4. 创建huffman树&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/huffman.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;简单来说，用一个vocab_size*2+1大小的数组存储树的&lt;r&gt;全部节点&lt;/r&gt;，其中前vocab_size个元素为全部的&lt;r&gt;叶子节点&lt;/r&gt;且按词频降序排列；后面的元素表示&lt;r&gt;中间结点&lt;/r&gt;，且初始化词频为1e15。用两个变量&lt;r&gt;pos1&lt;/r&gt;,&lt;r&gt;pos2&lt;/r&gt;记录构造树的情况，其中pos1指向vocab_size-1，pos2指向vocab_size。每次从pos1,pos2选取词频较小的那个结点，并相应的向前或向后移动，更新pos。选择两次，合并两个选中的结点为一棵新树。&lt;/p&gt;

&lt;p&gt;在构建树的过程中，不断更新中间结点的词频，并保存各个节点的编码(0/1)，各个节点的父节点(层次信息)。从叶子节点(词)回溯，得出该词在树中的&lt;r&gt;路径&lt;/r&gt;和&lt;r&gt;huffman编码&lt;/r&gt;。&lt;/p&gt;

&lt;p&gt;详见&lt;r&gt;CreateBinaryTree&lt;/r&gt;函数。&lt;/p&gt;

&lt;h4 id=&#34;5-负采样:d45865320239d7818255e82b856775aa&#34;&gt;5. 负采样&lt;/h4&gt;

&lt;p&gt;假设词典中的每个词w对应一个线段l(w),其长度为：&lt;/p&gt;

&lt;p&gt;$$len(w) = \frac{[counter(w)]^{0.75}}{\sum\limits_{u \in D} [counter(u)]^{0.75}} $$&lt;/p&gt;

&lt;p&gt;将这些线段首尾相接拼接在一起，形成一个长度为1的线段。随机往这个线段上打点，则高频词被打中的概率大。将这个单位线段等距离分割成M份(M远大于N,\(M=10^8\))，每次生成一个[0,M-1]的随机数r，r对应的词即为采样。有点类似k-means++的做法。&lt;/p&gt;

&lt;p&gt;详见&lt;r&gt;InitUnigramTable&lt;/r&gt;函数。&lt;/p&gt;

&lt;h4 id=&#34;6-学习率-eta:d45865320239d7818255e82b856775aa&#34;&gt;6. 学习率\(\eta\)&lt;/h4&gt;

&lt;p&gt;学习率采用&lt;r&gt;自适应&lt;/r&gt;的方法，具体为：预先设置一个初始学习率(0.025)，每处理完固定数目(10000)个词，对学习率进行一次调整。&lt;/p&gt;

&lt;p&gt;$$\eta = \eta_{0}(1-\frac{word\_count\_actual}{train\_words+1})$$&lt;/p&gt;

&lt;p&gt;随着训练的进行\(\eta\)会逐渐变小，当小于阈值\(\eta_{min}\)时，\(\eta = \eta_{min}\)&lt;/p&gt;

&lt;h4 id=&#34;7-多线程并行:d45865320239d7818255e82b856775aa&#34;&gt;7. 多线程并行&lt;/h4&gt;

&lt;p&gt;整个工作流程是：首先读取语料库中词表并统计词频，做一些模型初始化工作，然后多线程训练word2vec，判断classes，如果为0，则保存词向量；反之，对得到的词向量进行K-means聚类，聚类个数为classes，并保存聚类结果。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pthread_t *pt = (pthread_t *)malloc(num_threads * sizeof(pthread_t));
for (a = 0; a &amp;lt; num_threads; a++)
    pthread_create(&amp;amp;pt[a], NULL, TrainModelThread, (void *)a);
for (a = 0; a &amp;lt; num_threads; a++)
    pthread_join(pt[a], NULL);
//平衡划分语料文件
fseek(fi, file_size / (long long)num_threads * (long long)id, SEEK_SET);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并行的&lt;r&gt;关键&lt;/r&gt;在于&lt;strong&gt;如何分割好并行的任务和如何达成任务之间的良好通信&lt;/strong&gt;。word2vec根据&lt;r&gt;线程数将语料划分成若干份&lt;/r&gt;，每个线程负责语料库的一部分，但训练的神经网络、词向量和哈夫曼树是共享的。首先根据文件指针读取一个句子，用一个数组保存下句子中的每个词；然后根据命令行参数，选择训练CBOW或Skip-gram，其中每个模型都可以采用hs或negative的框架训练。&lt;/p&gt;

&lt;h2 id=&#34;spark-mllib-1-6-实现:d45865320239d7818255e82b856775aa&#34;&gt;Spark MLlib 1.6 实现&lt;/h2&gt;

&lt;h3 id=&#34;可调参数:d45865320239d7818255e82b856775aa&#34;&gt;可调参数&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;参数&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;vectorSize&lt;/td&gt;
&lt;td&gt;词向量的长度&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;learningRate&lt;/td&gt;
&lt;td&gt;学习率&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;numPartitions&lt;/td&gt;
&lt;td&gt;分区数，默认为1，越小准确率越高&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;numIterations&lt;/td&gt;
&lt;td&gt;迭代次数，默认为1，不大于numPartitions&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;seed&lt;/td&gt;
&lt;td&gt;随机数种子&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;minCount&lt;/td&gt;
&lt;td&gt;有效词频的最小值，默认值5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;window&lt;/td&gt;
&lt;td&gt;上下文的窗口大小，默认值5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;主要实现了基于hierarchical softmax框架下的skip-gram模型。&lt;/p&gt;

&lt;h3 id=&#34;广播:d45865320239d7818255e82b856775aa&#34;&gt;广播&lt;/h3&gt;

&lt;p&gt;对于sigmod函数值表格，词信息数组，词的hash表三个变量，通过广播机制，变成全局变量。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val sc = dataset.context //dataset: RDD[Iterable[String]]
val expTable = sc.broadcast(createExpTable)
val bcVocab = sc.broadcast(vocab)
val bcVocabHash = sc.broadcast(vocabHash)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;分布式:d45865320239d7818255e82b856775aa&#34;&gt;分布式&lt;/h3&gt;

&lt;p&gt;根据语料集文件，读取文件中的全部句子，将句子按分区数分发。&lt;/p&gt;

&lt;h2 id=&#34;参考链接:d45865320239d7818255e82b856775aa&#34;&gt;参考链接&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.cnblogs.com/peghoty/p/3857839.html&#34;&gt;word2vec 中的数学原理详解&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1301.3781.pdf&#34;&gt;论文：Efficient Estimation of Word Representations in Vector Space&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&#34;&gt;论文：Distributed Representations of Words and Phrases and their Compositionality&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[ml] Decision Tree模型</title>
      <link>http://cherishzhang.github.io/post/ml/decision_tree/</link>
      <pubDate>Mon, 01 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/ml/decision_tree/</guid>
      <description>

&lt;h2 id=&#34;1-intro:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;1.Intro&lt;/h2&gt;

&lt;p&gt;决策树常用的算法有&lt;r&gt;ID3&lt;/r&gt;,&lt;r&gt;C4.5&lt;/r&gt;和&lt;r&gt;CART&lt;/r&gt;，学习过程分为三部分：&lt;r&gt;特征选择&lt;/r&gt;，&lt;r&gt;决策树生成&lt;/r&gt;和&lt;r&gt;决策树剪枝&lt;/r&gt;。&lt;/p&gt;

&lt;p&gt;决策树的&lt;r&gt;生成&lt;/r&gt;只考虑局部最优（&lt;r&gt;贪心算法&lt;/r&gt;），而决策树的&lt;r&gt;剪枝&lt;/r&gt;则考虑&lt;r&gt;全局最优&lt;/r&gt;。简而言之，这是&lt;strong&gt;一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对每个子数据集有一个最好的分类过程&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;They are easy to &lt;r&gt;interpret&lt;/r&gt;, &lt;r&gt;handle categorical features&lt;/r&gt;, extend to the &lt;r&gt;multiclass classification setting&lt;/r&gt;, do not require &lt;r&gt;feature scaling&lt;/r&gt;, and are able to &lt;r&gt;capture non-linearities and feature interactions&lt;/r&gt;.&lt;/p&gt;

&lt;h2 id=&#34;2-准则函数:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;2.准则函数&lt;/h2&gt;

&lt;p&gt;主要用来衡量节点数据集合的有序性，有&lt;r&gt;熵(Entropy)&lt;/r&gt;，&lt;r&gt;基尼指数(Gini)&lt;/r&gt;，&lt;r&gt;方差(Variance)&lt;/r&gt;，其中前两种针对分类问题，而方差针对回归问题。寻找最好的分割点是通过量化&lt;strong&gt;分割后&lt;/strong&gt;类的纯度来确定的，计算方式总结如下表：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Impurity&lt;/th&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;formula&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;熵&lt;/td&gt;
&lt;td&gt;分类&lt;/td&gt;
&lt;td&gt;假设有K类，样本点属于第i类的概率为\(p_i\)，度量数据的不确定性&lt;/td&gt;
&lt;td&gt;\(E(D) =\sum_{i=1}^{K} -p_ilogp_i\)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;基尼&lt;/td&gt;
&lt;td&gt;分类&lt;/td&gt;
&lt;td&gt;同上&lt;/td&gt;
&lt;td&gt;\(Gini(D) = \sum_{i=1}^{K} p_i*(1-p_i)\)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;方差&lt;/td&gt;
&lt;td&gt;回归&lt;/td&gt;
&lt;td&gt;度量数据的离散程度&lt;/td&gt;
&lt;td&gt;\(Variance(D) = \frac{1}{N} \sum_{i=1}^N (y_i-\mu)^2\),\(\mu = \frac{1}{N} \sum_{i=1}^N y_i\)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;信息增益&lt;/strong&gt;: 得知某特征X的信息而使目标Y的信息&lt;r&gt;不确定减少&lt;/r&gt;的程度。信息增益大的特征有更强的分类能力。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;具体方法&lt;/strong&gt;：对训练集D，计算每个特征的信息增益，比较大小，选择信息增益大的特征列来分割集合。假设分割点将数据集分成左右两部分，则信息增益为 \( IG(D,s) = Impurity(D) - \frac{N_{left}}{N} Impurity(D_{left}) - \frac{N_{right}}{N} Impurity(D_{right})\)&lt;/p&gt;

&lt;p&gt;ID3s算法选择信息增益作分类，为了校正信息增益存在“偏向选择取值较多的特征”的缺点，C4.5采用&lt;r&gt;信息增益比&lt;/r&gt;\(IG_R(D,s) = \frac{IG(D,s)}{E_s(D)}\), \(E_s(D)= \sum_{i=1}^K -\frac{|D_i|}{|D|}*log_2(\frac{|D_i|}{|D|})\), 其中K是特征s的取值个数。&lt;/p&gt;

&lt;h2 id=&#34;3-生成算法:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;3 生成算法&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;具体方法&lt;/strong&gt;：从根节点开始，计算所有可能的特征的准则值(应用准则函数)，选择最优的特征作为节点的特征，由该特征的不同取值建立子节点；再对子节点递归调用以上方法，构建决策树。&lt;/p&gt;

&lt;h4 id=&#34;cart:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;CART&lt;/h4&gt;

&lt;p&gt;CART(classification and regression tree)，又称分类与回归树。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CART的生成是一个递归地构建&lt;r&gt;二叉树&lt;/r&gt;的过程&lt;/strong&gt;，对回归树应用平方误差最小化准则，对分类树用Gini指数最小化准则，进行特征选择，生成二叉树。预测时，先根据样本特征判断该样本点位于哪块区域，然后用该区域内的训练样本点集的&lt;r&gt;众数&lt;/r&gt; (分类) 或&lt;r&gt;均值&lt;/r&gt; (回归) 作为该样本点的预测值。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;算法描述&lt;/strong&gt;：CART树生成算法&lt;br/&gt;
&lt;strong&gt;输入&lt;/strong&gt;: 训练数据集D，停止条件&lt;br/&gt;
&lt;strong&gt;输出&lt;/strong&gt;: 决策树f(x)&lt;br/&gt;
&lt;strong&gt;(1)&lt;/strong&gt;在平方误差最小或Gini最小的准则下选择&lt;r&gt;最优切分变量j与切分点s&lt;/r&gt;，切分变量从可用特征集中选择。
回归树中，单元\(R_m\)下的最优值是\(R_m\)上所有输入实例对应的输出\(y_i\)的均值，即切分点。分类中，切分点是该变量所有可能的取值。&lt;br/&gt;
&lt;strong&gt;(2)&lt;/strong&gt;用选定的对(j,s)划分区域并决定相应的输出值&lt;br/&gt;
&lt;strong&gt;(3)&lt;/strong&gt;继续对子区域调用(1),(2),直至满足停止条件&lt;br/&gt;
&lt;strong&gt;(4)&lt;/strong&gt;将输入空间划分为M个区域\(R_1\),\(R_2\),&amp;hellip;,\(R_M\).生成如下决策树：&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;$$f(x) = \sum_{m=1}^{M} c_m*I(x \in R_m)$$&lt;/p&gt;

&lt;p&gt;算法的&lt;r&gt;停止条件&lt;/r&gt;是节点中的样本个数小于预定阈值，或样本集的基尼指数小于预订阈值（样本基本属于同一类），或没有更多特征。&lt;/p&gt;

&lt;h2 id=&#34;4-剪枝过程-pruning:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;4 剪枝过程(pruning)&lt;/h2&gt;

&lt;p&gt;将已生成的树进行简化的过程，通过极小化决策树整体的损失函数（loss function）来实现。 分为前向剪枝和后向剪枝。前向剪枝是在构造决策树的同时对树进行剪枝；后向剪枝是在决策树构建完成后，对树从根节点向上递归剪枝。&lt;/p&gt;

&lt;h2 id=&#34;5-mllib实现:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;5.MLlib实现&lt;/h2&gt;

&lt;h4 id=&#34;连续值特征:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;连续值特征&lt;/h4&gt;

&lt;p&gt;对于连续值类型的特征，单台机器上的做法是选取所有出现过的值作为切分点候选集。加速计算的方法是对该集合按值大小排序，用排序好的数组做候选集。&lt;/p&gt;

&lt;p&gt;在分布式中，对于规模大的数据集，排序是比较耗时的。于是先&lt;r&gt;对整体数据按比例采样&lt;/r&gt;，然后&lt;r&gt;采用分位数(等频分箱)作为分割点的近似候选集&lt;/r&gt;(&lt;gr&gt;&lt;strong&gt;优化点&lt;/strong&gt;&lt;/gr&gt;)。&lt;/p&gt;

&lt;p&gt;&lt;r&gt;采样策略&lt;/r&gt;为选取样本集中的&lt;strong&gt;&lt;r&gt;max&lt;/r&gt;(maxBins*maxBins, 10000)&lt;/strong&gt;条记录。&lt;r&gt;等频分箱策略&lt;/r&gt;有三种(&lt;strong&gt;Sort&lt;/strong&gt;,MinMax,ApproxHist)，目前只支持第一种。规定分箱总数不能超过训练样本的实例数(maxBins的默认值为32)。&lt;/p&gt;

&lt;h4 id=&#34;类别值特征:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;类别值特征&lt;/h4&gt;

&lt;p&gt;对于离散值类型的特征，有m个值，那么最多产生\(2^{m-1}-1\)种划分可能，\(2*(2^{m-1}-1)\)个分箱。对于二分类或回归问题，可以将划分候选缩减到m-1种可能。&lt;/p&gt;

&lt;p&gt;举个例子，在二分类问题中，一个特征有A,B,C三个值。特征值为A的样本集中label是1的比例为0.2，B和C分别为0.6，0.4。则产生了一个类别值的有序序列ACB，共有两种划分可能：A|C,B，A,C|B。&lt;/p&gt;

&lt;p&gt;多分类问题中，离散特征会产生\(2^{m-1}-1\)种划分可能，当划分数大于maxBins，将m个类别值根据impurity排序，产生共m-1种划分可能。&lt;/p&gt;

&lt;h4 id=&#34;bin和split:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;bin和split&lt;/h4&gt;

&lt;p&gt;split是划分点，bin是划分区间。每个特征都分别有一组bin和split。&lt;/p&gt;

&lt;p&gt;其中bin中&lt;r&gt;预先计算样本的统计信息&lt;/r&gt;，节省计算开销(&lt;gr&gt;&lt;strong&gt;优化点&lt;/strong&gt;&lt;/gr&gt;)。&lt;strong&gt;分类问题&lt;/strong&gt;，bin中保存训练样本个数 (count) 以及各 label 数目 (count for label)；&lt;strong&gt;回归问题&lt;/strong&gt;，bin中保存训练样本个数 (count)、 label 之和 (sum) 以及 label 的平方和 (squared sum)。&lt;/p&gt;

&lt;p&gt;特征选择时，对于某个特征，计算不同切分变量下信息增益的大小，确定出该特征下的最佳切分点。选择全部候选特征集中最优的切分点作为当前决策。特征选择是树的level级别的并行，对于同一层次的节点，查找可以并行，查找操作的时间复杂度为O(L),L为树的层数(&lt;gr&gt;&lt;strong&gt;优化点&lt;/strong&gt;&lt;/gr&gt;)。&lt;/p&gt;

&lt;h4 id=&#34;终止条件:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;终止条件&lt;/h4&gt;

&lt;p&gt;为了防止过拟合，采用前向剪枝。当任一以下情况发生，节点就终止划分，形成叶子节点：
(1)树高度达到maxDepth &lt;br/&gt;
(2)minInfoGain，当前节点的所有属性分割带来的信息增益都比这个值要小 &lt;br/&gt;
(3)minInstancesPerNode，需要保证节点分割出的左右子节点的最少的样本数量达到这个值 &lt;br/&gt;&lt;/p&gt;

&lt;h4 id=&#34;源码分析:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;源码分析&lt;/h4&gt;

&lt;p&gt;下面就一起揭开decision tree的真正面纱吧！&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/DecisionTree.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;gr&gt;输入 - RDD[LabelPoint]&lt;/gr&gt;：
LabelPoint类有两个属性label(标签值), features(特征向量)。将训练数据集转换为LabelPoint形式，作为决策树训练的输入之一。&lt;/p&gt;

&lt;p&gt;&lt;gr&gt;输入 - Strategy&lt;/gr&gt;:
这是关于模型训练过程中需要指定的参数集。&lt;/p&gt;

&lt;p&gt;&lt;gr&gt;buildMetadata()&lt;/gr&gt;:
DecisionTreeMetadata是一个决策树&lt;r&gt;元数据信息&lt;/r&gt;类，其中大部分属性和Strategy类重合。该函数的&lt;r&gt;功能&lt;/r&gt;: &lt;strong&gt;对于类别值特征，区分有序/无序；统计类别值特征的划分点数目和分箱数目。&lt;/strong&gt;是训练决策树模型的一个准备工作。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def buildMetadata(input:RDD[LabeledPoint], strategy:Strategy,
numTrees: Int, featureSubsetStrategy: String):DecisionTreeMetadata =
{...}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于每个类别值特征fi，&lt;r&gt;多分类情况下&lt;/r&gt;：首先计算最大可能分箱maxPossibleBins确定情况下，能够允许的最多类别值m，如果fi的类别值超过m，则当作有序处理；否则认为该特征是无序的，并计算分箱数。&lt;/p&gt;

&lt;p&gt;&lt;r&gt;二分类/回归情况下&lt;/r&gt;：当作有序处理，即numBins(fi) = numCategories(fi)。&lt;/p&gt;

&lt;p&gt;&lt;gr&gt;findSplitsBins&lt;/gr&gt;:
这是DecisionTree.scala中一个方法，基于buildMetadata统计的划分点数目和分箱数，加上数据采样，确定每个特征的划分点值。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def findSplitsBins(input:RDD[LabeledPoint],
metadata:DecisionTreeMetadata): (Array[Array[Split]], Array[Array[Bin]]) =
{ ... }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果存在连续值特征，则对数据集进行无放回采样，采样个数为max(maxBins*maxBins, 10000)。接着进行分箱操作（见findSplitsBinsBySorting函数，findSplitsForContinuousFeature函数）。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;treeRDD,BaggedRDD&lt;/strong&gt;:
treeRDD的类型是：RDD[TreePoint],treePoint类有两个属性：label和binnedFeatures。BaggedPoint类有两个属性datum和subsampleWeights(在各个采样后数据集中的权重，即出现次数)。&lt;/p&gt;

&lt;p&gt;&lt;gr&gt;findBestSplits&lt;/gr&gt;:
对于当前节点，计算准则函数下的信息增益，选择最优切分特征和最优切分点。&lt;/p&gt;

&lt;p&gt;&lt;gr&gt;其他&lt;/gr&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;configuration&lt;/strong&gt;:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;类名&lt;/th&gt;
&lt;th&gt;枚举值&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Algo&lt;/td&gt;
&lt;td&gt;Classification, Regression&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;FeatureType&lt;/td&gt;
&lt;td&gt;Continuous, Categorical&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;QuantileStrategy&lt;/td&gt;
&lt;td&gt;Sort, MinMax, ApproxHist&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;EnsembleCombiningStrategy&lt;/td&gt;
&lt;td&gt;Average, Sum, Vote&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;impurtiy,loss&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Impurity是准则函数的抽象类，其三个子类分别为Gini,Variance,Entropy。Impurities是Impurity的工厂类，负责解析用户参数，选择对应的准则函数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private[mllib] object Impurities {
  def fromString(name: String): Impurity = name match {
    case &amp;quot;gini&amp;quot; =&amp;gt; Gini
    case &amp;quot;entropy&amp;quot; =&amp;gt; Entropy
    case &amp;quot;variance&amp;quot; =&amp;gt; Variance
    case _ =&amp;gt; throw new IllegalArgumentException(s&amp;quot;Did not recognize
    Impurity name: $name&amp;quot;)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Loss与Impurity情况类似，它是损失函数的抽象类，其三个子类分别是AbsoluteError,SquaredError,LogLoss。Losses是Loss的工厂类，负责判断损失的所属类别。&lt;/p&gt;

&lt;h2 id=&#34;参考链接:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;参考链接&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://spark.apache.org/docs/latest/mllib-decision-tree.html&#34;&gt;spark mllib - decision tree&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;统计学习方法，李航著&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[ml] Boosting</title>
      <link>http://cherishzhang.github.io/post/ml/boosting/</link>
      <pubDate>Fri, 20 May 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/ml/boosting/</guid>
      <description>

&lt;h2 id=&#34;intro:d21755d9d058d9a9b44db3100ac0dea8&#34;&gt;Intro&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_boosting&#34;&gt;Boosting&lt;/a&gt; can be interpreted as &lt;strong&gt;an optimization algorithm&lt;/strong&gt; on a suitable cost function. The algorithms that optimize a cost &lt;strong&gt;function over function&lt;/strong&gt; space by &lt;r&gt;iteratively&lt;/r&gt; choosing a function(week hypothesis) that points in the &lt;strong&gt;negative gradient direction&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;在分类问题中，提升(Boosting)思想，通过改变训练数据的概率分布（权重分布），针对不同的训练数据分布，学习多个弱分类器，将这些分类器线性组合，提高分类的性能。&lt;/p&gt;

&lt;p&gt;围绕提升方法，有&lt;strong&gt;&lt;r&gt;两个问题&lt;/r&gt;&lt;/strong&gt;需要考虑：1.每一轮如何改变训练数据的概率或权值分布；2.如何将弱分类器组合成一个强分类器。&lt;/p&gt;

&lt;h2 id=&#34;adaboost-adaptive-boosting:d21755d9d058d9a9b44db3100ac0dea8&#34;&gt;AdaBoost(Adaptive Boosting)&lt;/h2&gt;

&lt;p&gt;AdaBoost提高那些被前几轮弱分类器线性组成的分类器&lt;r&gt;错误分类&lt;/r&gt;的样本权值，在下一轮的弱分类器中更加关注没有得到正确分类的数据；采用&lt;r&gt;加权多数表决&lt;/r&gt;的方法，误差率小的弱分类器权值大，误差率大的弱
分类器权值小。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;算法流程&lt;/strong&gt;：（二分类问题）&lt;br/&gt;
&lt;r&gt;输入&lt;/r&gt;：训练数据集\(T=\{(x_1,y_1),(x_2,y_2),&amp;hellip;,(x_N,y_N)\}\)和弱分类器算法 &lt;br/&gt;
&lt;r&gt;输出&lt;/r&gt;：最终分类器\(G(x)\)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;初始化训练样本的&lt;r&gt;权值分布&lt;/r&gt;(均匀分布)&lt;br/&gt;$$D_1=(w_{11},&amp;hellip;,w_{1i},&amp;hellip;,w_{1N}), w_{1i}=\frac{1}{N}, i=1,2,&amp;hellip;,N$$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对于\(m=1,2,&amp;hellip;,M\)(迭代)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用具有权值分布\(D_m\)的训练数据集学习，得到基本分类器\(G_m(x)\)&lt;/li&gt;
&lt;li&gt;计算\(G_m(x)\)在权值分布\(D_m\)的训练数据集上的&lt;strong&gt;分类误差率&lt;/strong&gt;
$$e_m = P(G_m(x_i) \neq y_i) = \sum_{i=1}^N w_{mi}I(G_m(x_i) \neq y_i)$$&lt;/li&gt;
&lt;li&gt;计算\(G_m(x)\)的系数，即&lt;r&gt;表决权&lt;/r&gt;
$$\alpha_{m} = \frac{1}{2} \log \frac{1-e_m}{e_m}$$&lt;/li&gt;
&lt;li&gt;更新训练数据集的权值分布
$$D_{m+1} = (w_{m+1,1},&amp;hellip;,w_{m+1,i},&amp;hellip;,w_{m+1,N})$$
$$w_{m+1,i} = \frac{w_{mi}}{Z_m} \exp (-\alpha_m y_i G_m(x_i))$$
其中\(Z_m\)是规范化因子,$$Z_m = \sum_{i=1}^N w_{mi} exp(-\alpha_m y_i G_m(x_i))$$&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;构建基本分类器的线性组合，得到&lt;r&gt;最终的分类器&lt;/r&gt;
$$f(x) = \sum_{m=1}^M \alpha_m G_m(x)$$
$$G(x) = sign(f(x))$$&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每轮迭代过程中，Adaboost会重点关注错误分类的样本，使得&lt;strong&gt;算法受噪声和离群点的影响较大&lt;/strong&gt;。\(\alpha_m\)随着 \(e_m\) 的减小而增大，\(w_{mi}\) 也随着\(\alpha_m\) 而改变，不断增大误分类样本的权值，减小正确分类样本的权值。&lt;/p&gt;

&lt;p&gt;AdaBoost的一个解释是，模型为&lt;strong&gt;加法模型，损失函数为指数函数，学习算法为前向分步算法的二分类学习方法&lt;/strong&gt;。前向分步算法(forward stagewise algorithm)求解优化问题的思路是：从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式，就可以简化优化的复杂度。&lt;/p&gt;

&lt;h2 id=&#34;boosting-tree-提升树:d21755d9d058d9a9b44db3100ac0dea8&#34;&gt;Boosting Tree(提升树)&lt;/h2&gt;

&lt;p&gt;采用&lt;strong&gt;加法模型(基函数的线性组合)与前向分步算法，以决策树为基函数的提升方法&lt;/strong&gt;称为提升树，分类问题采用二叉分类树，回归问题采用二叉回归树。&lt;/p&gt;

&lt;p&gt;提升树表示为决策树的加法模型:
$$f_{M(x)} = \sum_{m=1}^M T(x; \Theta_m)$$
其中，\(T(x;\Theta_m)\)表示决策树，\(\Theta_m\)为决策树的参数，M为树的个数&lt;/p&gt;

&lt;p&gt;不同问题的提升树学习算法，&lt;r&gt;主要区别在于使用的损失函数不同&lt;/r&gt;，包括用平方误差损失函数的回归问题，用指数损失函数的分类问题，以及用一般损失函数的一般决策问题。将Adaboost算法中的基本分类器限制为分类决策树，即为针对二分类问题的提升树算法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;提升树算法&lt;/strong&gt;(回归问题):&lt;br/&gt;
&lt;r&gt;输入&lt;/r&gt;:训练数据集\(T=\{(x_1,y_1),(x_2,y_2),&amp;hellip;,(x_N,y_N)\}\)
&lt;br/&gt;
&lt;r&gt;输出&lt;/r&gt;:提升树\(f_M(x)\)&lt;br/&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;确定初始提升树\(f_0(x) = 0\)&lt;/li&gt;
&lt;li&gt;对m = 1,2,&amp;hellip;,M

&lt;ul&gt;
&lt;li&gt;计算残差： \(r_{mi} = y_{i} - f_{m-1}(x_i), i=1,2,&amp;hellip;,N\)&lt;/li&gt;
&lt;li&gt;拟合残差，学习一个回归树，得到\(T(x;\Theta_m)\)&lt;/li&gt;
&lt;li&gt;更新\(f_m(x)=f_{m-1}(x)+T(x;\Theta_m)\)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;得到最终提升树：\(f_M(x)=\sum_{m=1}^M T(x;\Theta_m)\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;gradient-boosting-梯度提升:d21755d9d058d9a9b44db3100ac0dea8&#34;&gt;Gradient Boosting(梯度提升)&lt;/h2&gt;

&lt;p&gt;前向分布算法针对特殊的损失函数(平方和指数)，对于一般的损失函数，Freidman提出了梯度提升算法。它利用最速下降法的近似方法，其&lt;r&gt;&lt;strong&gt;关键&lt;/strong&gt;&lt;/r&gt;是利用损失函数的负梯度在当前模型的值最为回归问题提升树算法中的残差的近似值，拟合一个回归树。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;梯度提升算法&lt;/strong&gt;：&lt;br/&gt;
&lt;r&gt;输入&lt;/r&gt;:训练数据集\(T={(x_1,y_1),(x_2,y_2),&amp;hellip;,(x_N,y_N)}\)，损失函数\(L(y,f(x))\)&lt;br/&gt;
&lt;r&gt;输出&lt;/r&gt;:回归树\(\hat{f} (x)\)&lt;br\&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;初始化，估计是损失函数极小化的常数值，即只有一个根节点的树
\(f_0(x) = arg\mathop{min}\limits_{c} \sum_{i=1}^{N} L(y_i,c)\)&lt;/li&gt;
&lt;li&gt;对m=1,2,&amp;hellip;,M

&lt;ul&gt;
&lt;li&gt;对i=1,2,&amp;hellip;,N,计算
\(r_{mi} = - \{\left [\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)} \right]\}_{f(x)=f_{m-1}(x)}\)&lt;/li&gt;
&lt;li&gt;对于\(r_{mi}\)拟合一个回归树，得到第m棵树的叶节点区域\(R_{mj}$, j=1,2,&amp;hellip;,J\)（对于平方损失函数，就是通常所说的残差，对于一般损失函数，即为残差的近似值）&lt;/li&gt;
&lt;li&gt;对于j=1,2,&amp;hellip;,J,计算 $$c_{mj} = arg\mathop{min}\limits_{c} \sum_{x_i \in R_{mj}} L(y_i,f_{m-1}(x_i)+c)$$&lt;/li&gt;
&lt;li&gt;更新\(f_m(x) = f_{m-1}(x)+\sum_{j=1}^J c_{mj}I(x\in R_mj)\)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;得到回归树 $$\hat{f}(x) = f_M(x) = \sum_{m=1}^M \sum_{j=1}^J c_{mj} I(x \in R_{mj})$$&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Data Quality in Recommendation</title>
      <link>http://cherishzhang.github.io/post/paper/Data%20Quality%20in%20Recommendation/</link>
      <pubDate>Wed, 16 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/paper/Data%20Quality%20in%20Recommendation/</guid>
      <description>

&lt;p&gt;论文名称：&lt;a href=&#34;http://dl.acm.org/citation.cfm?id=2799670&amp;amp;preflayout=tabs&#34;&gt;Data Quality Matters in Recommender Systems, RecSys &amp;lsquo;15&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;摘要:c7e7f190dc4ad9c3cea8a960f18951d1&#34;&gt;摘要&lt;/h2&gt;

&lt;p&gt;&lt;r&gt;数据质量好坏&lt;/r&gt;在信息系统中是一个很重要的因素，但在推荐系统中考虑得却不是很多。比较常用的方法是ad-hoc式的清洗，如，去除数据集中的噪声和不可靠的记录。显露出的缺点是,没有结合数据集本身的特性。&lt;/p&gt;

&lt;p&gt;论文中的主要贡献是考虑推荐系统中两个核心的数据质量问题：&lt;strong&gt;&lt;r&gt;sparsity&lt;/r&gt;&lt;/strong&gt; 和 &lt;strong&gt;&lt;r&gt;redundancy&lt;/r&gt;&lt;/strong&gt;，并设计了数据集相关的&lt;r&gt;阈值模型&lt;/r&gt;和&lt;r&gt;采样等级模型&lt;/r&gt;，然后在一系列的公开数据集上做了验证试验。&lt;/p&gt;

&lt;h2 id=&#34;具体方法:c7e7f190dc4ad9c3cea8a960f18951d1&#34;&gt;具体方法&lt;/h2&gt;

&lt;h3 id=&#34;关于数据的稀疏性:c7e7f190dc4ad9c3cea8a960f18951d1&#34;&gt;关于数据的稀疏性&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;阈值模型(Threshold model)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;很多基于分数的推荐系统数据集中包含一部分&lt;strong&gt;冷启动&lt;/strong&gt;的用户和物品。一般的数据清洗操会去除这些用户和物品的打分信息。现实中的问题是，怎样得到最优的清洗阈值。最简单的暴力方法(brute-force)是评估所有可能的组合，但时间空间复杂度高，在实际中不可行。&lt;/p&gt;

&lt;p&gt;首先，明确目标:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The aim is to develop a heuristic method that predicts the optimal thresholds for a given user-item rating matrix, without building the model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然后，采用了一些合理的假设，如假设要预测的物品阈值是与&lt;strong&gt;物品向量&lt;/strong&gt;的平均长度&lt;code&gt;$\overline{r}_i$&lt;/code&gt;相关，在打分矩阵中，即为平均每个物品被打分的次数。由于推荐数据集中，只有一小部分的流行商品会被打分多次，大部分商品被打分的次数相对很少，这引入了模型的另外一个特征，即物品向量长度的&lt;strong&gt;power-law&lt;/strong&gt;分布,令H为物品向量长度的分布函数，拟合函数\(H={Ax}^{(-m)}\),其中x是单个物品向量的长度，m是个正数。最终可以得到数据集对应的m参数值。m越大，长尾分布中的尾部越向下，反映到数据集上，就是更加少的物品被多次打分过，也就是说，m越大，数据集本身越稀疏。&lt;/p&gt;

&lt;p&gt;得到以上两个特征因素，接下来就是构建阈值模型，得到公式: \({IT}_d = \gamma*\frac{log(\overline{r}_i)}{m^2}\)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;评价方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;总共24个数据集，包括10个开放数据集，如Movielens, Million Songs, Flixster, Moviepilot, Filmtipset, Yelp, Yahoo! Music(broken down into albums, artists, and tracks), and BookCrossing。以及14个从公司或站点获取的专业数据集(proprietary)。&lt;/p&gt;

&lt;p&gt;每个数据集9-1划分成训练集和测试集，使用*Precision@K*作为衡量测试集的标准(K为测试集的总记录数)，10折交叉验证，平均准确率作为最后的得分。&lt;/p&gt;

&lt;p&gt;实验过程：
- 首先，寻找每个数据集d的最优IT值。具体对于某个IT值，过滤掉低于IT的item，对剩余数据进行矩阵分解(Matrix Factorization),建立推荐模型，并通过测试集评测模型的准确率。不断迭代，增长IT，寻找使得测试集准确率最高的IT作为最优阈值\({IT}^{opt}_d\)，对应的准确率为\({P}^{opt}_d\)。
- 接下来，对24个数据集采用leave-one-out交叉验证。在23个数据集上训练阈值模型，然后预测剩余数据集d的阈值\({IT}^{pred}_d\),并在过滤后的数据上训练推荐模型，计算准确率\(P^{pred}_d\)。&lt;/p&gt;

&lt;p&gt;两个评测指标：&lt;/p&gt;

&lt;p&gt;$$ NTE_d = |{IT}^{opt}_d - {IT}^{pred}_d|/{IT}^{opt}_d $$&lt;/p&gt;

&lt;p&gt;$$ AR_d = P^{pred}_d/P^{opt}_d $$&lt;/p&gt;

&lt;p&gt;注：&lt;strong&gt;IT&lt;/strong&gt;(item threshold), &lt;strong&gt;AR&lt;/strong&gt;(accuracy ratio), &lt;strong&gt;NTE&lt;/strong&gt;(normalized threshold error)&lt;/p&gt;

&lt;p&gt;通过计算，\({NTE}_d\)和\({AR}_d\)之间的相关度为-0.54。这表明IT的误差越小，推荐模型的准确率越高。&lt;/p&gt;

&lt;h4 id=&#34;关于数据的冗余性:c7e7f190dc4ad9c3cea8a960f18951d1&#34;&gt;&lt;strong&gt;关于数据的冗余性&lt;/strong&gt;&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;模型&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;数据集可以通过随机采样，建立模型，使得尽可能得与在全部数据上建立模型的结果相似。这对于实际中的大规模数据是有效的。&lt;/p&gt;

&lt;p&gt;模型目标：
   - The aim is to pick the &lt;strong&gt;loweset sampling rate&lt;/strong&gt; that will still result in the recommendation model as close as possible to the model that would have been built using the complete data.&lt;/p&gt;

&lt;p&gt;与阈值模型不同的是，在采样率的取值上没有最优。因为在全体数据上构建推荐模型总是最优的。&lt;/p&gt;

&lt;p&gt;定义\(SR\)为采样数据的推荐结果与全体数据的推荐结果相似度不低于\(\Delta\)时的最低采样率。&lt;/p&gt;

&lt;p&gt;定义\(U_d, I_d, R_d\)为数据集d中的用户数量，商品数量，和打分项个数，给定采样率\(SR\),从所有用户中随机选取\(SR*U_d\)个用户，在采样的数据集上建立推荐模型，对一个固定的测试集产生预测结果。比较与全部数据预测的结果差异。&lt;/p&gt;

&lt;p&gt;以对用户采样举例，假设数据的冗余性与三个因素相关a.用户数 b.打分矩阵的稀疏性 c.V-structure，得到公式：&lt;/p&gt;

&lt;p&gt;$${SR}_d = tanh(\frac{1}{{V-structure}_d * \sqrt{U_d} * \frac{R_d}{I_d * U_d}})$$&lt;/p&gt;

&lt;p&gt;其中V-structure的定义为平均相似度之比，分子中的每对用户至少有一个打分项目相同，分母中的每对用户为全体用户的所有组合可能。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;评价&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;采用19个专业数据集，9-1比例分训练集和测试集，10折交叉验证。实验主要验证了模型的合理性。&lt;/p&gt;

&lt;h3 id=&#34;我的思考:c7e7f190dc4ad9c3cea8a960f18951d1&#34;&gt;&lt;strong&gt;我的思考&lt;/strong&gt;&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;这篇论文提出了两个模型公式，并完成了实验验证。优点是模型公式与数据集本身的特性相关，不干涉推荐系统的建模过程。&lt;/li&gt;
&lt;li&gt;遗留下的问题：数据的稀疏性和冗余性只针对MF推荐模型，还可以扩展到与其他推荐模型相结合。&lt;/li&gt;
&lt;li&gt;考虑其他的评测指标，如覆盖率和多样性；同时，模型中还可以结合物品的内容属性和用户的位置属性。&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Telstra Network Disruptions</title>
      <link>http://cherishzhang.github.io/post/kaggle/Telstra/</link>
      <pubDate>Wed, 02 Mar 2016 21:03:56 +0800</pubDate>
      
      <guid>http://cherishzhang.github.io/post/kaggle/Telstra/</guid>
      <description>

&lt;h2 id=&#34;1-overview:43e8ca42a888262ca3ed3923385c58f2&#34;&gt;&lt;strong&gt;1. Overview&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The goal of the &lt;a href=&#34;https://www.kaggle.com/c/telstra-recruiting-network/&#34;&gt;problem&lt;/a&gt; is to predict Telstr network&amp;rsquo;s fault severity at a time at a particular location based on the &lt;em&gt;log data&lt;/em&gt; available.&lt;/p&gt;

&lt;p&gt;The target has 3 categories:0,1,2. It&amp;rsquo;s a multiclass classification problems.Different types of features are extracted from log files and other sources: event_type.csv, log_feature.csv, resource_type.csv,severity_type.csv.&lt;/p&gt;

&lt;p&gt;My final score is &lt;em&gt;0.44917(72 of 974)&lt;/em&gt; in private leaderboard. Here are my &lt;a href=&#34;https://github.com/Cherishzhang/kaggle/tree/master/Telstra&#34;&gt;code&lt;/a&gt;, and I will record my solution.&lt;/p&gt;

&lt;h2 id=&#34;2-feature-engineer:43e8ca42a888262ca3ed3923385c58f2&#34;&gt;&lt;strong&gt;2. Feature Engineer&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Selecting and designing good features is an important area in machine learning, which is called &lt;em&gt;feature engineering&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;At First, I just merge all files on Id. Here are five type of features. they are location, severity_type, resource_type, event_type, log_feature.
location and severity_type are just one-to-one variables, and others are many_to_one variables.
There are about 1200 locations, some are only in the test set. The correlation between location variable and target is 0.27, it gave me a hint that neighbouring locations may be similar in the network&amp;rsquo;s fault severity problem. I used one_hot encode to solve the many_to_one features. So there are about 400+ features in the initial stage.&lt;/p&gt;

&lt;p&gt;I read a paper about how to preprocess high-cardinality Categorical attributes in classification and regression problems, but it seemed to bring little help. I will tried it again after the competition.&lt;/p&gt;

&lt;p&gt;On the forum, there are a heated discussion about the magic feature. I spent much of my time to find it.
It is the order of the same location in the severity file which follows the order of fault occurrence.It&amp;rsquo;s called &lt;em&gt;Intra-location order&lt;/em&gt;.&lt;br /&gt;
It really did a big help to the final score which improved almost 0.06. To each record, I compute the target(&lt;em&gt;fault_severity&lt;/em&gt;) probabilities from the previous same location records, and used the &lt;em&gt;previous-target_probabilities&lt;/em&gt; as a feature to build the model.&lt;/p&gt;

&lt;h2 id=&#34;3-build-models-and-ensembling:43e8ca42a888262ca3ed3923385c58f2&#34;&gt;&lt;strong&gt;3. Build models and Ensembling&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;I tried many models, decision tree, random forests, svm and xgboost. The xgboost model is performed well.
On ensembling, I just average the random forest and xgboost result as the final result.&lt;/p&gt;

&lt;h2 id=&#34;4-what-i-learned-from-other-kagglers:43e8ca42a888262ca3ed3923385c58f2&#34;&gt;&lt;strong&gt;4. What I learned from other kagglers&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;On the platform, kagglers are willing to share their ideas. Here are some valuable ideas on the &lt;a href=&#34;https://www.kaggle.com/c/telstra-recruiting-network/forums/t/19239/it-s-been-fun-post-your-code-github-links-here-after-the-competition&#34;&gt;Competition Forum&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;some-useful-tips-on-feature-engineering:43e8ca42a888262ca3ed3923385c58f2&#34;&gt;Some useful tips on feature engineering&lt;/h3&gt;

&lt;p&gt;It&amp;rsquo;s the most important step in machine learning whatever models you used.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;About location&lt;br /&gt;
&lt;em&gt;Similar location numbers have similar fault severity.(Treat the location as numeric.)&lt;/em&gt;
&lt;em&gt;Don&amp;rsquo;t one-hot-encode the location, Tree-based classifiers are not good at handling huge sparse feature matrix.&lt;/em&gt;
&lt;em&gt;Frequency of LogType_203 = 0 &amp;amp; LogType_203 &amp;gt;0 per location&lt;/em&gt;
&lt;em&gt;The records in log data are arranged in the order of time.(the magic feature)&lt;/em&gt; Here are two ways to encode
the information, One is for each location, use the row number, which starts from 1 to the total number of rows for that locaiton. The other
is to normalize it between 0 and 1.
&lt;em&gt;percentile transformation of location counts&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;About log feature&lt;br /&gt;
&lt;em&gt;Pattern of log feature&lt;/em&gt;
&amp;ldquo;one hot&amp;rdquo; encoding for all log features with volume &amp;gt; 0, for all rows.
Each &amp;ldquo;one hot&amp;rdquo; encoded pattern treated as a string.
Assigned integer ID to each to each string, used as feature.
The log transform for the count of &amp;ldquo;pattern of log feature&amp;rdquo;, the log transform for counts as &amp;ldquo;pattern of event&amp;rdquo;
and &amp;ldquo;pattern of resource&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Common categorical variables&lt;br /&gt;
&lt;em&gt;For high-cardinality categorical variables, frequency works well.(Add the frequency of each location in both train and test set.)&lt;/em&gt;
&lt;em&gt;Summary statistics to reduce one-to-many relationship to one-to-one, and two-way or more-way interaction among multiple variables.&lt;/em&gt;
&lt;em&gt;Meta features(using Logistic regression to fit sparse matrix as predictors, then ensemble the model).&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;A useful solution&lt;br /&gt;
&amp;gt;a.The order of id on log_feature was frozen.&lt;br /&gt;
&amp;gt;b.Converting location, log_feature into numbers and generating count, mean, sum, etc. features (feature set A)&lt;br /&gt;
&amp;gt;c.Feature B was generated by shifting A forward by 1 row&lt;br /&gt;
&amp;gt;d.Feature C was generated by shifting B backward by 1 row&lt;br /&gt;
&amp;gt;Combining A, B, and C and training xgb, RF, GBM models. My final model is an ensemble model of these models.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>http://cherishzhang.github.io/about/</link>
      <pubDate>Tue, 18 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/about/</guid>
      <description>

&lt;p&gt;&lt;em&gt;Here is my blog site, a record and learning place.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&#34;education:6083a88ee3411b0d17ce02d738f69d47&#34;&gt;Education&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;2014 - Master of Computer Science, Institute of Computing Technology, Chinese Academy of Sciences&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;2010 - Bachelor of Network Engineering , South China University of Technology&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;publications:6083a88ee3411b0d17ce02d738f69d47&#34;&gt;Publications&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;An ensemble method of job recommender systems&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>