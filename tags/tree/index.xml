<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tree on Learn and record</title>
    <link>http://cherishzhang.github.io/tags/tree/</link>
    <description>Recent content in Tree on Learn and record</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Aug 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://cherishzhang.github.io/tags/tree/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Random Forest</title>
      <link>http://cherishzhang.github.io/post/ml/random_forest/</link>
      <pubDate>Wed, 03 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/ml/random_forest/</guid>
      <description>

&lt;h2 id=&#34;1-intro:00635f635026fbac82e8a20eddd81309&#34;&gt;1. Intro&lt;/h2&gt;

&lt;p&gt;基本原理：一个由多个决策树构成的森林，算法分类结果由这些决策树投票得到，决策树在生成的过程当中分别在行方向和列方向上添加&lt;r&gt;随机过程&lt;/r&gt;，行方向上构建决策树时采用&lt;r&gt;放回抽样&lt;/r&gt;（bootstraping）得到训练数据，列方向上采用&lt;r&gt;无放回随机抽样&lt;/r&gt;得到特征子集，并据此得到其最优切分点。&lt;strong&gt;随机森林通过多个决策树投票结果进行分类，算法不容易出现过度拟合问题。&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-construct:00635f635026fbac82e8a20eddd81309&#34;&gt;2. Construct&lt;/h2&gt;

&lt;p&gt;分为两方面:&lt;strong&gt;&lt;r&gt;数据&lt;/r&gt;&lt;/strong&gt;的随机性选取，以及&lt;strong&gt;&lt;r&gt;待选特征&lt;/r&gt;&lt;/strong&gt;的随机选取。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;数据&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;从原始数据集中&lt;strong&gt;有放回&lt;/strong&gt;的抽样，构造子数据集，子数据集的数据量和原始数据集相同。&lt;/li&gt;
&lt;li&gt;利用子数据集构建子决策树&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;待选特征&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在子树的每一个分裂过程中并未用到所有的待选特征。&lt;/li&gt;
&lt;li&gt;从原始特征无放回随机抽取一定比例的特征，用所抽取的特征集作特征选择。&lt;/li&gt;
&lt;li&gt;假设总的特征数量是M，则比例可为\(\sqrt{M}, \frac{1}{2}\sqrt{M}, 2\sqrt{M}\)。&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;训练过程：&lt;br/&gt;
input: 给定训练集S，测试集T，特征维数F。确定参数集(CART的数量t，每棵树的深度d，每个节点用到的特征数量f，终止条件：节点上最少的样本数，节点上最少的信息增益m)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于i = 1,2,&amp;hellip;,t;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;(1) 从S中有放回的抽样，生成一个大小和S相同的训练集S(i)，作为根节点的样本集，从根节点开始训练。&lt;br/&gt;
(2) a. 如果当前节点满足终止条件，则设置当前节点为叶子节点。&lt;/p&gt;

&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;分类问题，该叶子节点的预测输出为当前样本集和中数量最多的那一类C(j),概率p为c(j)占当前样本集的比例；&lt;/li&gt;
&lt;li&gt;回归问题，预测输出为当前样本集所有样本的平均值。&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;(2) b. 如果不满足终止条件，则从F维特征中无放回的随机选取f维特征。利用这f维特征，寻找分类效果最好的那一维特征k及阈值th。当前节点上样本第k维特征小于th的样本被划分到左节点，其余的被划分到右节点。&lt;/p&gt;

&lt;p&gt;(3) 重复(1)(2)直到所有节点都被训练过或标记为叶子节点。 &lt;br/&gt;
(4) 重复(1)(2)(3)直到所有的决策树被训练过。 &lt;br/&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;预测过程：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于i = 1,2,&amp;hellip;,t;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;(1) 从当前树的根节点开始，根据节点对应阈值th，判断是进入左节点还是右节点，直到到达某个叶子节点，并输出预测值。&lt;br/&gt;
(2) 重复执行(1)，直到得到所有树的预测值。&lt;strong&gt;分类问题&lt;/strong&gt;，则输出所有树中预测概率总和最大的那一类，即对每个c(j)的p进行累计；&lt;strong&gt;回归问题&lt;/strong&gt;，则输出为所有树的平均预测值。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;3-evaluation:00635f635026fbac82e8a20eddd81309&#34;&gt;3. Evaluation&lt;/h2&gt;

&lt;p&gt;rf无需交叉验证来评价分类的准确性，本身自带OOB（out-of-bag）错误估计。&lt;/p&gt;

&lt;h2 id=&#34;4-spark-mllib:00635f635026fbac82e8a20eddd81309&#34;&gt;4. Spark MLlib&lt;/h2&gt;

&lt;p&gt;(1)切分点抽样统计&lt;/p&gt;

&lt;p&gt;在单机环境下的决策树对连续变量进行切分点选择时，一般是通过对特征点进行排序，然后取相邻两个数之间的点作为切分点，这在单机环境下是可行的，但如果在分布式环境下如此操作的话，会带来大量的网络传输操作，特别是当数据量达到 PB 级时，算法效率将极为低下。为避免该问题，Spark 中的随机森林在构建决策树时，会对各分区采用一定的子特征策略进行抽样，然后生成各个分区的统计数据，并最终得到切分点。&lt;/p&gt;

&lt;p&gt;(2)特征装箱（Binning）&lt;/p&gt;

&lt;p&gt;决策树的构建过程就是对特征的取值不断进行划分的过程，对于离散的特征，如果有 M 个值，最多\(2^M-1\)个划分，如果值是有序的，那么就最多 M-1 个划分。比如年龄特征，有老，中，少 3 个值，如果无序有\(2^M-1\)个，即 3 种划分：老|中，少；老，中|少；老，少|中；如果是有序的，即按老，中，少的序，那么只有 m-1 个，即 2 种划分，老|中，少；老，中|少。对于连续的特征，其实就是进行范围划分，而划分的点就是 split（切分点），划分出的区间就是 bin。对于连续特征，理论上 split 是无数的，在分布环境下不可能取出所有的值，因此它采用的是（1）中的切点抽样统计方法。&lt;/p&gt;

&lt;p&gt;(3)逐层训练（level-wise training）&lt;/p&gt;

&lt;p&gt;单机版本的决策数生成过程是通过递归调用（本质上是深度优先）的方式构造树，在构造树的同时，需要移动数据，将同一个子节点的数据移动到一起。此方法在分布式数据结构上无法有效的执行，而且也无法执行，因为数据太大，无法放在一起，所以在分布式环境下采用的策略是逐层构建树节点（本质上是广度优先），这样遍历所有数据的次数等于所有树中的最大层数。每次遍历时，只需要计算每个节点所有切分点统计参数，遍历完后，根据节点的特征划分，决定是否切分，以及如何切分。&lt;/p&gt;

&lt;h2 id=&#34;5-codes:00635f635026fbac82e8a20eddd81309&#34;&gt;5. Codes&lt;/h2&gt;

&lt;p&gt;参考sklearn.ensemble.RandomForestRegressor，sklearn.ensemble.RandomForestClassifier&lt;/p&gt;

&lt;p&gt;输入参数&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;名称&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;n_estimators&lt;/td&gt;
&lt;td&gt;一个森林中决策树的个数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;criterion&lt;/td&gt;
&lt;td&gt;分类准则，如&amp;rsquo;rmse&amp;rsquo;,&amp;lsquo;gini&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;max_depth&lt;/td&gt;
&lt;td&gt;树的最大深度，tree-specific&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;min_samples_split&lt;/td&gt;
&lt;td&gt;中间节点分裂前的最少样本数，tree-specific&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;min_samples_leaf&lt;/td&gt;
&lt;td&gt;新生成叶节点上的最少样本数，tree-specific&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;min_weight_fraction_leaf&lt;/td&gt;
&lt;td&gt;叶节点上样本数占全部样本的最小比例值，tree-specific&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;max_features&lt;/td&gt;
&lt;td&gt;每个节点用到的特征数量，tree-specific&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;max_leaf_nodes&lt;/td&gt;
&lt;td&gt;每棵树中的叶节点能达到的最大值，tree-specific&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;bootstrap&lt;/td&gt;
&lt;td&gt;是否构建决策树过程中，对样本进行bootstrap&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;oob_score&lt;/td&gt;
&lt;td&gt;是否使用out-of-bag样本估计泛化误差&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;n_jobs&lt;/td&gt;
&lt;td&gt;并行运行的进程数，默认等于CPU的核数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;random_state&lt;/td&gt;
&lt;td&gt;随机生成器的种子&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;verbose&lt;/td&gt;
&lt;td&gt;是否输出构建过程中的信息&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;warm_start&lt;/td&gt;
&lt;td&gt;设置为是，则使用上次拟合后的solution，再附加决策树，否则，拟合一个新的森林&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;class_weight&lt;/td&gt;
&lt;td&gt;类别权重列表&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;模型属性&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;名称&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;estimators_&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;classes_&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;n&lt;em&gt;classes&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;n&lt;em&gt;features&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;n_outputs&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;feature&lt;em&gt;importances&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;oob_score&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;参考链接:00635f635026fbac82e8a20eddd81309&#34;&gt;参考链接&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.cnblogs.com/hrlnw/p/3850459.html&#34;&gt;RandomForest随机森林总结&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Boosting</title>
      <link>http://cherishzhang.github.io/post/ml/boosting/</link>
      <pubDate>Fri, 20 May 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/ml/boosting/</guid>
      <description>

&lt;h2 id=&#34;intro:d21755d9d058d9a9b44db3100ac0dea8&#34;&gt;Intro&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_boosting&#34;&gt;Boosting&lt;/a&gt; can be interpreted as &lt;strong&gt;an optimization algorithm&lt;/strong&gt; on a suitable cost function. The algorithms that optimize a cost &lt;strong&gt;function over function&lt;/strong&gt; space by &lt;r&gt;iteratively&lt;/r&gt; choosing a function(week hypothesis) that points in the &lt;strong&gt;negative gradient direction&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;在分类问题中，提升(Boosting)思想，通过改变训练数据的概率分布（权重分布），针对不同的训练数据分布，学习多个弱分类器，将这些分类器线性组合，提高分类的性能。&lt;/p&gt;

&lt;p&gt;围绕提升方法，有&lt;strong&gt;&lt;r&gt;两个问题&lt;/r&gt;&lt;/strong&gt;需要考虑：1.每一轮如何改变训练数据的概率或权值分布；2.如何将弱分类器组合成一个强分类器。&lt;/p&gt;

&lt;h2 id=&#34;adaboost-adaptive-boosting:d21755d9d058d9a9b44db3100ac0dea8&#34;&gt;AdaBoost(Adaptive Boosting)&lt;/h2&gt;

&lt;p&gt;AdaBoost提高那些被前几轮弱分类器线性组成的分类器&lt;r&gt;错误分类&lt;/r&gt;的样本权值，在下一轮的弱分类器中更加关注没有得到正确分类的数据；采用&lt;r&gt;加权多数表决&lt;/r&gt;的方法，误差率小的弱分类器权值大，误差率大的弱
分类器权值小。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;算法流程&lt;/strong&gt;：（二分类问题）&lt;br/&gt;
&lt;r&gt;输入&lt;/r&gt;：训练数据集\(T=\{(x_1,y_1),(x_2,y_2),&amp;hellip;,(x_N,y_N)\}\)和弱分类器算法 &lt;br/&gt;
&lt;r&gt;输出&lt;/r&gt;：最终分类器\(G(x)\)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;初始化训练样本的&lt;r&gt;权值分布&lt;/r&gt;(均匀分布)&lt;br/&gt;$$D_1=(w_{11},&amp;hellip;,w_{1i},&amp;hellip;,w_{1N}), w_{1i}=\frac{1}{N}, i=1,2,&amp;hellip;,N$$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对于\(m=1,2,&amp;hellip;,M\)(迭代)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用具有权值分布\(D_m\)的训练数据集学习，得到基本分类器\(G_m(x)\)&lt;/li&gt;
&lt;li&gt;计算\(G_m(x)\)在权值分布\(D_m\)的训练数据集上的&lt;strong&gt;分类误差率&lt;/strong&gt;
$$e_m = P(G_m(x_i) \neq y_i) = \sum_{i=1}^N w_{mi}I(G_m(x_i) \neq y_i)$$&lt;/li&gt;
&lt;li&gt;计算\(G_m(x)\)的系数，即&lt;r&gt;表决权&lt;/r&gt;
$$\alpha_{m} = \frac{1}{2} \log \frac{1-e_m}{e_m}$$&lt;/li&gt;
&lt;li&gt;更新训练数据集的权值分布
$$D_{m+1} = (w_{m+1,1},&amp;hellip;,w_{m+1,i},&amp;hellip;,w_{m+1,N})$$
$$w_{m+1,i} = \frac{w_{mi}}{Z_m} \exp (-\alpha_m y_i G_m(x_i))$$
其中\(Z_m\)是规范化因子,$$Z_m = \sum_{i=1}^N w_{mi} exp(-\alpha_m y_i G_m(x_i))$$&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;构建基本分类器的线性组合，得到&lt;r&gt;最终的分类器&lt;/r&gt;
$$f(x) = \sum_{m=1}^M \alpha_m G_m(x)$$
$$G(x) = sign(f(x))$$&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每轮迭代过程中，Adaboost会重点关注错误分类的样本，使得&lt;strong&gt;算法受噪声和离群点的影响较大&lt;/strong&gt;。\(\alpha_m\)随着 \(e_m\) 的减小而增大，\(w_{mi}\) 也随着\(\alpha_m\) 而改变，不断增大误分类样本的权值，减小正确分类样本的权值。&lt;/p&gt;

&lt;p&gt;AdaBoost的一个解释是，模型为&lt;strong&gt;加法模型，损失函数为指数函数，学习算法为前向分步算法的二分类学习方法&lt;/strong&gt;。前向分步算法(forward stagewise algorithm)求解优化问题的思路是：从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式，就可以简化优化的复杂度。&lt;/p&gt;

&lt;h2 id=&#34;boosting-tree-提升树:d21755d9d058d9a9b44db3100ac0dea8&#34;&gt;Boosting Tree(提升树)&lt;/h2&gt;

&lt;p&gt;采用&lt;strong&gt;加法模型(基函数的线性组合)与前向分步算法，以决策树为基函数的提升方法&lt;/strong&gt;称为提升树，分类问题采用二叉分类树，回归问题采用二叉回归树。&lt;/p&gt;

&lt;p&gt;提升树表示为决策树的加法模型:
$$f_{M(x)} = \sum_{m=1}^M T(x; \Theta_m)$$
其中，\(T(x;\Theta_m)\)表示决策树，\(\Theta_m\)为决策树的参数，M为树的个数&lt;/p&gt;

&lt;p&gt;不同问题的提升树学习算法，&lt;r&gt;主要区别在于使用的损失函数不同&lt;/r&gt;，包括用平方误差损失函数的回归问题，用指数损失函数的分类问题，以及用一般损失函数的一般决策问题。将Adaboost算法中的基本分类器限制为分类决策树，即为针对二分类问题的提升树算法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;提升树算法&lt;/strong&gt;(回归问题):&lt;br/&gt;
&lt;r&gt;输入&lt;/r&gt;:训练数据集\(T=\{(x_1,y_1),(x_2,y_2),&amp;hellip;,(x_N,y_N)\}\)
&lt;br/&gt;
&lt;r&gt;输出&lt;/r&gt;:提升树\(f_M(x)\)&lt;br/&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;确定初始提升树\(f_0(x) = 0\)&lt;/li&gt;
&lt;li&gt;对m = 1,2,&amp;hellip;,M

&lt;ul&gt;
&lt;li&gt;计算残差： \(r_{mi} = y_{i} - f_{m-1}(x_i), i=1,2,&amp;hellip;,N\)&lt;/li&gt;
&lt;li&gt;拟合残差，学习一个回归树，得到\(T(x;\Theta_m)\)&lt;/li&gt;
&lt;li&gt;更新\(f_m(x)=f_{m-1}(x)+T(x;\Theta_m)\)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;得到最终提升树：\(f_M(x)=\sum_{m=1}^M T(x;\Theta_m)\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;gradient-boosting-梯度提升:d21755d9d058d9a9b44db3100ac0dea8&#34;&gt;Gradient Boosting(梯度提升)&lt;/h2&gt;

&lt;p&gt;前向分布算法针对特殊的损失函数(平方和指数)，对于一般的损失函数，Freidman提出了梯度提升算法。它利用最速下降法的近似方法，其&lt;r&gt;&lt;strong&gt;关键&lt;/strong&gt;&lt;/r&gt;是利用损失函数的负梯度在当前模型的值最为回归问题提升树算法中的残差的近似值，拟合一个回归树。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;梯度提升算法&lt;/strong&gt;：&lt;br/&gt;
&lt;r&gt;输入&lt;/r&gt;:训练数据集\(T={(x_1,y_1),(x_2,y_2),&amp;hellip;,(x_N,y_N)}\)，损失函数\(L(y,f(x))\)&lt;br/&gt;
&lt;r&gt;输出&lt;/r&gt;:回归树\(\hat{f} (x)\)&lt;br\&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;初始化，估计是损失函数极小化的常数值，即只有一个根节点的树
\(f_0(x) = arg\mathop{min}\limits_{c} \sum_{i=1}^{N} L(y_i,c)\)&lt;/li&gt;
&lt;li&gt;对m=1,2,&amp;hellip;,M

&lt;ul&gt;
&lt;li&gt;对i=1,2,&amp;hellip;,N,计算
\(r_{mi} = - \{\left [\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)} \right]\}_{f(x)=f_{m-1}(x)}\)&lt;/li&gt;
&lt;li&gt;对于\(r_{mi}\)拟合一个回归树，得到第m棵树的叶节点区域\(R_{mj}$, j=1,2,&amp;hellip;,J\)（对于平方损失函数，就是通常所说的残差，对于一般损失函数，即为残差的近似值）&lt;/li&gt;
&lt;li&gt;对于j=1,2,&amp;hellip;,J,计算 $$c_{mj} = arg\mathop{min}\limits_{c} \sum_{x_i \in R_{mj}} L(y_i,f_{m-1}(x_i)+c)$$&lt;/li&gt;
&lt;li&gt;更新\(f_m(x) = f_{m-1}(x)+\sum_{j=1}^J c_{mj}I(x\in R_mj)\)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;得到回归树 $$\hat{f}(x) = f_M(x) = \sum_{m=1}^M \sum_{j=1}^J c_{mj} I(x \in R_{mj})$$&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>