<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Learn and record</title>
    <link>http://cherishzhang.github.io/post/</link>
    <description>Recent content in Posts on Learn and record</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 20 Aug 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://cherishzhang.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Spark开发学习</title>
      <link>http://cherishzhang.github.io/post/spark/</link>
      <pubDate>Sat, 20 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/spark/</guid>
      <description>

&lt;h3 id=&#34;rdd:912d68735e1dc303baa4d59608c12d96&#34;&gt;RDD&lt;/h3&gt;

&lt;p&gt;RDD，全称为Resilient Distributed Datasets（弹性分布式数据集）。它是一个&lt;r&gt;容错&lt;/r&gt;且&lt;r&gt;并行&lt;/r&gt;的数据结构，可以让用户&lt;r&gt;显式&lt;/r&gt;地将数据存储到磁盘和内存中，并能控制数据的&lt;r&gt;分区&lt;/r&gt;。它只能基于在稳定物理存储中的数据集和其他已有的RDD上执行确定性操作(transformation)来创建。&lt;/p&gt;

&lt;p&gt;举个例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//用普通数组[1,2,...,9]创建RDD，数据分别在3个分区中
val rdd1 = sc.parallelize(1 to 9, 3)
//读取文件创建RDD，每一行就是RDD的一个元素
val rdd2 = sc.textFile(&amp;quot;README.md&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由此可见，RDD&lt;r&gt;本质&lt;/r&gt;是一个&lt;r&gt;只读的分区记录集合&lt;/r&gt;。一个RDD包含多个分区，每个分区是一个dataset片段。&lt;/p&gt;

&lt;p&gt;相关操作函数如下表：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val rdd1 = sc.parallelize(1 to 9, 3)
val rdd2 = sc.parallelize(list(&amp;quot;dog&amp;quot;,&amp;quot;tiger&amp;quot;,&amp;quot;lion&amp;quot;,&amp;quot;cat&amp;quot;,&amp;quot;parther&amp;quot;,&amp;quot;eagle&amp;quot;),2)
val rdd3 = sc.parallelize(List(1,2),(3,4),(3,6)))
&lt;/code&gt;&lt;/pre&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;函数名&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;输入与输出&lt;/th&gt;
&lt;th&gt;Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;map&lt;/td&gt;
&lt;td&gt;对RDD中的每个元素执行一个指定的函数来产生一个新的RDD&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;一对一&lt;/td&gt;
&lt;td&gt;val a = rdd1.map(x =&amp;gt; x*2)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;mapValues&lt;/td&gt;
&lt;td&gt;对象是元素为kv对的RDD，key保持不变，对value进行映射&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;一对一&lt;/td&gt;
&lt;td&gt;val b = rdd2.map(x =&amp;gt; (x.length, x)) &lt;br\&gt; b.mapValues(&amp;ldquo;x&amp;rdquo; + _ + &amp;ldquo;x&amp;rdquo;).collect&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;flatMap&lt;/td&gt;
&lt;td&gt;扁平化map后的结果&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;一对一&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;reduce&lt;/td&gt;
&lt;td&gt;将RDD中元素两两传递给输入函数，产生一个新值，新产生的值与RDD中的下一个元素再被传递给输入函数，&lt;strong&gt;直到最后只有一个值为止&lt;/strong&gt;。&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;多对一&lt;/td&gt;
&lt;td&gt;rdd1.reduce((x,y) =&amp;gt; x+y) //结果为45&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;reduceByKey&lt;/td&gt;
&lt;td&gt;将key相同的元素的value进行reduce，组成一个新的KV对&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;多对一&lt;/td&gt;
&lt;td&gt;rdd3.reduceByKey((x,y) =&amp;gt; x+y).collect //结果为Array((1,2),(3,10))&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;filter&lt;/td&gt;
&lt;td&gt;对每个元素应用f函数，只保留返回值为true的元素&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;子集型&lt;/td&gt;
&lt;td&gt;val d = rdd3.filter{ case(x,y) =&amp;gt; equal(x,y) }.count() //结果为0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;sample&lt;/td&gt;
&lt;td&gt;对元素采样，获取子集&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;子集型&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;cache&lt;/td&gt;
&lt;td&gt;将元素从磁盘缓存到内存&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;cache型&lt;/td&gt;
&lt;td&gt;rdd1.cache()&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;unpersist&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;通常来讲，针对数据处理有几种常见模型，包括：&lt;r&gt;Iterative Algorithms&lt;/r&gt;,&lt;r&gt;Relational Queries&lt;/r&gt;,&lt;r&gt;MapReduce&lt;/r&gt;,&lt;r&gt;Stream Processing&lt;/r&gt;。例如Hadoop MapReduce采用了MapReduces模型，Storm则采用了Stream Processing模型。RDD混合了这四种模型，使得Spark可以应用于各种大数据处理场景。&lt;/p&gt;

&lt;p&gt;Spark将依赖分为narrow与wide。如果RDD的每个分区最多只能被一个Child RDD的一个分区使用，则称之为narrow dependency；若多个Child RDD分区都可以依赖，则称之为wide dependency。不同的操作依据其特性，可能会产生不同的依赖。例如map操作会产生narrow dependency，而join操作则产生wide dependency。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/rdd.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图说明了narrow和wide之间的区别。 narrow dependencies可以支持在同一个cluster node上以管道形式执行多条命令，而且它的失败恢复更有效，只需要重新计算丢失的parent partition即可；wide dependencies需要所有的父分区都是可用的，牵涉到RDD各级的多个Parent Partitions。&lt;/p&gt;

&lt;h3 id=&#34;共享变量:912d68735e1dc303baa4d59608c12d96&#34;&gt;共享变量&lt;/h3&gt;

&lt;p&gt;&lt;r&gt;broadcast&lt;/r&gt;变量：只读的共享变量 每个节点上都有一个拷贝。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val broadcastVar = sc.broadcast(&amp;quot;string test&amp;quot;) //broadcast variable is readonly
val v = broadcastVar.value
println(v)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;r&gt;accumulator&lt;/r&gt;变量：做累加器用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val accum = sc.accumulator(0, &amp;quot;My Accumulator&amp;quot;) //value and name
sc.parallelize(1 to 1000000).foreach(x =&amp;gt; accum+= 1)
println(accum.name + &amp;quot;:&amp;quot; + accum.value)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>决策树</title>
      <link>http://cherishzhang.github.io/post/decision_tree/</link>
      <pubDate>Mon, 08 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/decision_tree/</guid>
      <description>

&lt;h2 id=&#34;1-intro:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;1.Intro&lt;/h2&gt;

&lt;p&gt;决策树常用的算法有&lt;r&gt;ID3&lt;/r&gt;,&lt;r&gt;C4.5&lt;/r&gt;和&lt;r&gt;CART&lt;/r&gt;，学习过程分为三部分：&lt;r&gt;特征选择&lt;/r&gt;，&lt;r&gt;决策树生成&lt;/r&gt;和&lt;r&gt;决策树剪枝&lt;/r&gt;。&lt;/p&gt;

&lt;p&gt;决策树的&lt;r&gt;生成&lt;/r&gt;只考虑局部最优（&lt;r&gt;贪心算法&lt;/r&gt;），而决策树的&lt;r&gt;剪枝&lt;/r&gt;则考虑&lt;r&gt;全局最优&lt;/r&gt;。简而言之，这是&lt;strong&gt;一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对每个子数据集有一个最好的分类过程&lt;/strong&gt;。&lt;/p&gt;

&lt;h2 id=&#34;2-准则函数:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;2.准则函数&lt;/h2&gt;

&lt;p&gt;主要用来衡量节点数据集合的有序性，有&lt;r&gt;熵&lt;/r&gt;，&lt;r&gt;基尼指数(Gini)&lt;/r&gt;，&lt;r&gt;方差&lt;/r&gt;，其中前两种针对分类问题，而方差针对回归问题。寻找最好的分割点是通过量化&lt;strong&gt;分割后&lt;/strong&gt;类的纯度来确定的，计算方式总结如下表：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Impurity&lt;/th&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;formula&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;熵Entropy&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;随机变量X不确定性的度量，X的概率分布为\(P(X=x_i) = p_i\)&lt;/td&gt;
&lt;td&gt;\(H(X) =-\sum_{i=1}^{n} p_ilogp_i\)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;条件熵&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;已知随机变量X的条件下随机变量Y的不确定性&lt;/td&gt;
&lt;td&gt;\(H(Y|X)=\sum_{i=1}^n p_i H(Y|x=x_i)\)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;信息增益(互信息)&lt;/td&gt;
&lt;td&gt;分类ID3&lt;/td&gt;
&lt;td&gt;特征A对训练集D的信息增益，&lt;strong&gt;值越大，特征分类能力越强&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;\(g(D,A)=H(D)-H(D|A)\)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;信息增益比&lt;/td&gt;
&lt;td&gt;分类C4.5&lt;/td&gt;
&lt;td&gt;校正信息增益存在&lt;strong&gt;偏向选择取值较多的特征&lt;/strong&gt;的缺点，n是特征A的取值个数&lt;/td&gt;
&lt;td&gt;\(g_R(D,A) = \frac{g(D,A)}{H_A(D)}\), \(H_A(D)= \frac{|D_i|}{|D|}*log_2(\frac{|D_i|}{|D|})\)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;基尼&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;假设有K类，样本点属于第i类的概率为\(p_i\)，表示集合不确定性&lt;/td&gt;
&lt;td&gt;\(Gini(D) = \sum_{i=1}^{K} p_i*(1-p_i)\)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;基尼&lt;/td&gt;
&lt;td&gt;分类&lt;/td&gt;
&lt;td&gt;经过A=a分割后集合D的不确定性&lt;/td&gt;
&lt;td&gt;\(Gini(D,A)=\frac{|D1|}{|D|} Gini(D1) + \frac{|D2|}{|D|} Gini(D2)\)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;方差&lt;/td&gt;
&lt;td&gt;回归&lt;/td&gt;
&lt;td&gt;平方误差最小，求解最优输出值&lt;/td&gt;
&lt;td&gt;\(\frac{1}{N} \sum_{i=1}^N (y_i-\mu)^2\)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;信息增益:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;信息增益&lt;/h3&gt;

&lt;p&gt;定义： 得知特征X的信息而使类Y的信息&lt;r&gt;不确定减少&lt;/r&gt;的程度。信息增益大的特征有更强的分类能力。&lt;br/&gt;
计算：\(g(D,A) = H(D)-H(D|A)\)。&lt;br/&gt;
方法： 对训练数据集（或子集）D，计算其每个特征的信息增益，比较大小，选择信息增益大的特征。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;算法描述：计算特征A的信息增益
输入：训练数据集D和特征A
输出：特征A对训练数据集D的信息增益g(D,A)
1.计算数据集D的经验熵H(D）
2.计算特征A对数据集D的经验条件熵H(D|A)
3.计算信息增益g(D,A)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-生成算法:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;3 生成算法&lt;/h2&gt;

&lt;p&gt;核心是&lt;strong&gt;在应用准则函数的基础上&lt;/strong&gt;选择特征，递归地构建决策树。&lt;r&gt;具体方法&lt;/r&gt;：从根节点开始，计算所有可能的特征的准则值，选择最优的特征作为节点的特征，由该特征的不同取值建立子节点；再对子节点递归调用以上方法，构建决策树。&lt;strong&gt;结束条件&lt;/strong&gt;：直到所有特征的信息增益均很小或没有特征可选择为止。&lt;/p&gt;

&lt;p&gt;ID3算法应用&lt;r&gt;信息增益&lt;/r&gt;选择特征；C4.5对此进行改进，应用&lt;r&gt;信息增益比&lt;/r&gt;选择特征。&lt;/p&gt;

&lt;p&gt;CART(classification and regression tree)，又称分类与回归树。决策树的生成是一个递归地构建&lt;r&gt;二叉树&lt;/r&gt;的过程，对回归树应用平方误差最小化准则，对分类树用Gini指数最小化准则，进行特征选择，生成二叉树。预测时，先根据样本特征判断该样本点位于哪块区域，然后用该区域内的训练样本点集的&lt;r&gt;众数&lt;/r&gt; (分类) 或&lt;r&gt;均值&lt;/r&gt; (回归) 作为该样本点的预测值。&lt;/p&gt;

&lt;p&gt;算法描述：CART树生成算法&lt;br/&gt;
输入: 训练数据集D，停止条件&lt;br/&gt;
输出: 决策树f(x)&lt;br/&gt;
(1)在平方误差最小或Gini最小的准则下选择&lt;r&gt;最优切分变量j与切分点s&lt;/r&gt;，切分变量从可用特征集中选择。
回归树中，单元\(R_m\)下的最优值是\(R_m\)上所有输入实例对应的输出\(y_i\)的均值，即切分点。分类中，切分点是该变量所有可能的取值。&lt;br/&gt;
(2)用选定的对(j,s)划分区域并决定相应的输出值&lt;br/&gt;
(3)继续对子区域调用(1),(2),直至满足停止条件&lt;br/&gt;
(4)将输入空间划分为M个区域\(R_1\),\(R_2\),&amp;hellip;,\(R_M\).生成如下决策树：&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;$$f(x) = \sum_{m=1}^{M} c_m*I(x \in R_m)$$&lt;/p&gt;

&lt;p&gt;算法的&lt;r&gt;停止条件&lt;/r&gt;是节点中的样本个数小于预定阈值，或样本集的基尼指数小于预订阈值（样本基本属于同一类），或没有更多特征。&lt;/p&gt;

&lt;h2 id=&#34;4-剪枝过程-pruning:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;4 剪枝过程(pruning)&lt;/h2&gt;

&lt;p&gt;将已生成的树进行简化的过程，通过极小化决策树整体的损失函数（loss function）来实现。 分为前向剪枝和后向剪枝。&lt;/p&gt;

&lt;h2 id=&#34;5-单机实现-参考scikit-learn:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;5 单机实现(参考scikit-learn)&lt;/h2&gt;

&lt;p&gt;实现接口分为DecisionTreeClassifier和DecisionTreeRegressor&lt;/p&gt;

&lt;p&gt;关于准则函数：Criterion, ClassificationCriterion(Entropy, Gini), RegressionCriterion(MSE(FriedmanMSE), MAE)&lt;/p&gt;

&lt;p&gt;关于树的生成：&lt;/p&gt;

&lt;h3 id=&#34;使用技巧:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;使用技巧&lt;/h3&gt;

&lt;h2 id=&#34;6-mllib实现:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;6.MLlib实现&lt;/h2&gt;

&lt;h3 id=&#34;6-1-bin和split:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;6.1 bin和split&lt;/h3&gt;

&lt;p&gt;对于离散的feature,如果有m个值，最多有\(2^{m-1}-1\)个划分，如果是有序的，则有m-1个划分；对于连续的feature，进行范围划分。其中划分的点是split，而划分的区间是bin。训练时，两个重要的参数maxDepth（树的最大高度），maxBins（每个特征分裂时，最大划分桶数量）。&lt;/p&gt;

&lt;p&gt;bin中预先计算样本的统计信息，节省计算开销。&lt;strong&gt;分类问题&lt;/strong&gt;，bin中保存训练样本个数 (count) 以及各 label 数目 (count for label)；&lt;strong&gt;回归问题&lt;/strong&gt;，bin中保存训练样本个数 (count)、 label 之和 (sum) 以及 label 的平方和 (squared sum)。&lt;/p&gt;

&lt;p&gt;特征选择时，统计同一分区下，选择不同切分变量时，对应bin中的信息（ImpurityAggregator），合并所有分区，确定最佳切分变量和切分点，从而实现并行化。&lt;/p&gt;

&lt;p&gt;为了防止过拟合，采用前向剪枝。当任一以下情况发生，节点就终止划分，形成叶子节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(1)树高度达到maxDepth
(2)minInfoGain，当前节点的所有属性分割带来的信息增益都比这个值要小
(3)minInstancesPerNode，需要保证节点分割出的左右子节点的最少的样本数量达到这个值
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;6-2-关键优化点:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;6.2 关键优化点：&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;查找最优split，是树的level级别的并行，处于同一层次的节点，查找是并行的。查找次数的时间复杂度为O(L）,其中L是树的层数。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;连续特征，是用排序后的唯一特征值集合作为计算最佳split的候选，对于大规模数据，为了减小开销，采用分位数（等频分箱）作为split的候选。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;统计信息在bin中计算，节省开销。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;6-3-源码分析:998e3d52f8819c2e5f687d19cc26909b&#34;&gt;6.3 源码分析&lt;/h3&gt;

&lt;p&gt;类：CARTPoint(样本点),TreeNode(树结点)&lt;/p&gt;

&lt;p&gt;从根节点开始，对所有特征，从0开始编号，n_fs是全部特征数。统计所有特征所有分箱结果，对应每个样本集，计算该样本，每个特征对应的特征值会被分配到的箱子id。&lt;/p&gt;

&lt;p&gt;统计属于该节点的训练样本集（总样本数，各类别下的样本数，各类别对应的索引，类别总数）；根据统计信息计算当前节点的信息不确定性(Gini或Varience),并给出该节点的预测值（众数或平均值），根据以上信息更新当前树节点的信息，包括节点ID，预测类别ID，所在深度，样本数，特征总数，列采样率，该节点的Gini或Variance,里面包括一系列函数，判断是否是叶节点，分割方式，左右孩子，采样后的特征等。&lt;/p&gt;

&lt;p&gt;决策树生成的过程，用队列Queue实现，将根节点插入队列，每次取队列中的全部节点（层级并行化），并行处理，作为当前节点，计算当前节点的最好分割，将其左右孩子插入队列中。&lt;/p&gt;

&lt;p&gt;并行是通过mapPartitions(foreach, update), reduceByKey(),然后对aggs后的信息进行findBestSplits&lt;/p&gt;

&lt;p&gt;每个待分裂节点，都根据其父节点而初始化，全部特征，每个特征对应的分箱下的n个状态，初始化为0.0&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>word2vec模型和源码解析</title>
      <link>http://cherishzhang.github.io/post/word2vec/</link>
      <pubDate>Fri, 05 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/word2vec/</guid>
      <description>

&lt;h2 id=&#34;词向量:d45865320239d7818255e82b856775aa&#34;&gt;词向量&lt;/h2&gt;

&lt;p&gt;一种简单直观的词表示方法是&lt;r&gt;One-hot&lt;/r&gt;编码，用N位对N个词编码，每个词对应的N维向量中，只有一维为0。这种方式的缺陷是&lt;r&gt;词汇鸿沟&lt;/r&gt;，即词与词之间相互孤立，忽略了它们之间的联系。而word2vec中的词向量是一种distributed representation，它的特点是利用距离刻画词之间的相似性。&lt;/p&gt;

&lt;h2 id=&#34;统计语言模型:d45865320239d7818255e82b856775aa&#34;&gt;统计语言模型&lt;/h2&gt;

&lt;p&gt;统计语言模型是用来计算一个句子的概率的&lt;r&gt;概率模型&lt;/r&gt;，经典的有HAL,LSA,COALS等。通常它基于一个语料库来构建。&lt;/p&gt;

&lt;h3 id=&#34;句子的概率:d45865320239d7818255e82b856775aa&#34;&gt;句子的概率&lt;/h3&gt;

&lt;p&gt;给定一个由T个词\(w_1,w_2,&amp;hellip;,w_T\)按顺序构成的句子，根据Bayes，计算该句子的概率p(W)&lt;/p&gt;

&lt;p&gt;$$p(W) = p(w_1^T) = p(w_1,w_2,&amp;hellip;,w_T) = p(w_1)*p(w_2|w_1)*p(w_3|(w_1^2))&amp;hellip;p(w_T|(w_1^{T-1}))$$&lt;/p&gt;

&lt;p&gt;其中等式中的各个因子（条件概率）就是语言模型的&lt;strong&gt;参数&lt;/strong&gt;，常用的语言模型都是在近似求p(W)。如果给定一个词典大小为N的语料库，考虑长度为T的任意句子，理论上有\(N^T\)种可能组合，而每个句子需要计算T个参数，总共需要计算\(T*N^T\)个参数。无论是计算或保存，都需要很大开销。&lt;/p&gt;

&lt;h3 id=&#34;n-gram模型:d45865320239d7818255e82b856775aa&#34;&gt;n-gram模型&lt;/h3&gt;

&lt;p&gt;做了一个简化的假定，即一个词出现的概率&lt;strong&gt;只与&lt;/strong&gt;它前面的n-1个词相关。实际中最多采用n=3的三元模型。&lt;/p&gt;

&lt;p&gt;$$(p(w_k|w^{k-1}_1) = p(w_k|w^{k-1}_{k-n+1}) = \frac{count(w_{k-n+1}^k)}{count(w_{k-n+1}^{k-1})}$$&lt;/p&gt;

&lt;p&gt;如果\(count(w_{k-n+1}^k)\) = \(count(w_{k-n+1}^{k-1})\) 或者 \(count(w_{k-n+1}^{k-1}) = 0\)，则需要做平滑处理。&lt;/p&gt;

&lt;h3 id=&#34;最大似然:d45865320239d7818255e82b856775aa&#34;&gt;最大似然&lt;/h3&gt;

&lt;p&gt;对句子模型的另一种转化是利用最大似然，构造一个目标函数，优化目标求得最优参数，利用最优参数预测句子概率。目标函数为: $$\prod_{w \in C} p(w|Context(w))$$&lt;/p&gt;

&lt;p&gt;其中C表示语料集，Context(w)表示词w的上下文，即w周边词的集合。实际中采用最大对数似然，即\(\sum_{w \in C} log p(w|Context(w))\)最大化。增加log是为了将乘法运算转换为加法运算(便于求导的常用方法)。问题关键转化为F函数的构造。&lt;/p&gt;

&lt;p&gt;$$p(w|Context(w)) = F(w,Context(w),\theta)$$&lt;/p&gt;

&lt;h2 id=&#34;神经概率语言模型:d45865320239d7818255e82b856775aa&#34;&gt;神经概率语言模型&lt;/h2&gt;

&lt;p&gt;Bengio(03) - 《A Neural Probabilistic Language Model》&lt;/p&gt;

&lt;p&gt;神经网络包含四层：&lt;r&gt;输入层&lt;/r&gt;，&lt;r&gt;投影层&lt;/r&gt;，&lt;r&gt;隐藏层&lt;/r&gt;和&lt;r&gt;输出层&lt;/r&gt;。W,U为权值矩阵，p,q为偏置向量。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/word2vec-neural.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;取二元对(Context(w),w)作为一个训练样本，其中Context(w)取前面n-1个词，每个词\(v_i\)用一个m维向量表示。投影层是一个(n-1)*m长度的向量，输出层的规模为词库中词表的长度D。&lt;/p&gt;

&lt;p&gt;选用双曲正切函数作为隐藏层的激活函数，计算过程为：&lt;/p&gt;

&lt;p&gt;$$z_w = tanh(Wx_w + p), y_w = Uz_w + q$$&lt;/p&gt;

&lt;p&gt;输出为一个长度为D的向量\(y_w = (y_{w,1}, y_{w,2},&amp;hellip;,y_{w,D})^T\)，通过softmax归一化，每个分量表示上下文为Context(w)时下一词为词典第i个词的概率。&lt;/p&gt;

&lt;p&gt;待确定的参数\(\theta\)包括：词向量和填充向量，神经网络参数W,U,p,q。其中大部分&lt;strong&gt;计算集中&lt;/strong&gt;在隐藏层和输出层之间的矩阵向量运算和softmax归一化上。&lt;/p&gt;

&lt;h2 id=&#34;word2vec核心:d45865320239d7818255e82b856775aa&#34;&gt;Word2vec核心&lt;/h2&gt;

&lt;p&gt;word2vec中用到了两个重要模型: &lt;r&gt;CBOW&lt;/r&gt;和&lt;r&gt;Skip-gram&lt;/r&gt;，对于这两个模型，分别都给出了基于&lt;r&gt;Hierarchical Softmax&lt;/r&gt;和&lt;r&gt;Negative Sampling&lt;/r&gt;两套框架的实现。&lt;/p&gt;

&lt;p&gt;CBOW和Skip-gram包括三层：&lt;r&gt;输入层&lt;/r&gt;，&lt;r&gt;投影层&lt;/r&gt;，&lt;r&gt;输出层&lt;/r&gt;。前者是已知词的上下文预测当前词，后者是通过当前词预测上下文。&lt;/p&gt;

&lt;h3 id=&#34;1-cbow:d45865320239d7818255e82b856775aa&#34;&gt;1. CBOW&lt;/h3&gt;

&lt;p&gt;CBOW = Continuous Bag-of-Words, 目标函数为：&lt;/p&gt;

&lt;p&gt;$$\sum_{w \in c} log p(w|Context(w)) = \sum_{t=1}^T log p(w_t|\tau(w_{t-k},w_{t-k-1},&amp;hellip;,w_{t+k-1},w_{t+k}))$$&lt;/p&gt;

&lt;p&gt;其中T表示整个词典的大小，模型的目标是&lt;r&gt;最大化&lt;/r&gt;这个目标函数值。\(w_t\)表示词典中的一个词，即通过和\(w_t\)相邻的窗口大小为k的词来预测\(w_t\)出现的概率。其中\(\tau(w_1,w_2,&amp;hellip;,w_k)\)函数表示以\(w_i(i&amp;lt;=i&amp;lt;=k)\)为参数进行某种运算。在Word2vec里，这种运算是向量加和运算。即&lt;r&gt;把窗口相邻的所有词的向量加和&lt;/r&gt;。&lt;/p&gt;

&lt;h4 id=&#34;hierarchical-softmax:d45865320239d7818255e82b856775aa&#34;&gt;Hierarchical Softmax&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/word2vec-hs-cbow.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;相比于之前的神经概率语言模型，主要有&lt;r&gt;三处改动&lt;/r&gt;：&lt;strong&gt;去掉隐藏层；首尾拼接改为累加求和；输出为树结构。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;================================================================&lt;/p&gt;

&lt;p&gt;关于模型的&lt;r&gt;推导&lt;/r&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;结点的带权路径长度&lt;/strong&gt;: 从根结点到该结点的路径长度乘以该结点的权值&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;树的带权路径长度&lt;/strong&gt;: 所有叶结点的带权路径长度之和&lt;/p&gt;

&lt;p&gt;Huffman树是在给定n个叶子结点和其对应权值的前提下，构造一棵二叉树，使该树的带权路径长度最小。已知语料库中各词的词频，可以构建一棵二叉树，每个词作为树的叶子节点，并保证词频越大的词离根节点越近。每个叶子对应的&lt;r&gt;huffman编码&lt;/r&gt;就是该词的编码。约定左子结点编码为0，右子结点编码为1。&lt;/p&gt;

&lt;p&gt;此外为每个非叶子结点设置一个&lt;r&gt;辅助向量&lt;/r&gt;\(\theta\)。&lt;/p&gt;

&lt;p&gt;对于词w，从根结点到该叶结点的路径中，假设有\(l^w\)层，经历了\(l^w-1\)次二分类，则条件概率\( p(w|Context(w)) = \prod\limits_{j=2}^{l^w} p(d_j^w|X_w,\theta_{j-1}^w) \), \(d_j^w\)表示第j个结点对应的编码，\(\theta_j^w\)表示第j个结点对应的辅助向量。&lt;/p&gt;

&lt;p&gt;第一步，&lt;strong&gt;目标函数对数化&lt;/strong&gt;:
$$F = \sum_{w \in C} log p(w|Context(w)) = \sum_{w \in C} log \prod\limits_{j=2}^{l^w} \sigma(\theta_{j-1}^w X_w)^{1-{d_j^w}} (1-\sigma(\theta_{j-1}^w X_w))^{d_j^w} $$&lt;/p&gt;

&lt;p&gt;第二步，&lt;strong&gt;取内部项F(w,u)&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;$$ F(w,u) =  (1-{d_j^w}) log(\sigma(\theta_{j-1}^w)) + {d_j^w} log(1-\sigma(\theta_{j-1}^w X_w))$$&lt;/p&gt;

&lt;p&gt;第三步，&lt;strong&gt;随机梯度上升&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;\(\theta_{j-1}^w\)的更新公式为：&lt;/p&gt;

&lt;p&gt;$$ \frac{\partial F(w,u)}{\partial \theta_{j-1}^w} = (1 - d_j^w - \sigma(X_w^T*\theta_{j-1}^w)) X_w$$&lt;/p&gt;

&lt;p&gt;$$ \theta_{j-1}^w := \theta_{j-1}^w + \eta (1 - d_j^w - \sigma(X_w^T*\theta_{j-1}^w)) X_w $$&lt;/p&gt;

&lt;p&gt;\(X_w\)和\(\theta_{j-1}^w\)具有对称性，更新公式类似，这里注意更新的\(v(\tilde{w})\)而非加和后的\(X_w\)：&lt;/p&gt;

&lt;p&gt;$$v(\tilde{w}) := v(\tilde{w}) +  \eta \sum\limits_{j=2}^{l^w}(1 - d_j^w - \sigma(X_w^T*\theta_{j-1}^w)) \theta_{j-1}^w, \tilde{w} \in Context(w)$$&lt;/p&gt;

&lt;p&gt;================================================================&lt;/p&gt;

&lt;h4 id=&#34;negative-sampling:d45865320239d7818255e82b856775aa&#34;&gt;Negative Sampling&lt;/h4&gt;

&lt;p&gt;huffman树中从根节点到叶子节点路径上的各个中间结点，分为正负两类，构造好huffman树后，每个词w对应的采样样本（中间结点）也就确定。训练的目标是最大化似然函数值。由此衍生出，用简单的&lt;r&gt;随机负采样&lt;/r&gt;代替复杂的huffman树。已知词w的上下文，预测词w，可将w看作正样本，负样本的抽样策略为&lt;r&gt;带权采样&lt;/r&gt;，即被选中的概率与词频成正比。模型训练的&lt;r&gt;目标&lt;/r&gt;是&lt;strong&gt;最大化正样本的概率同时最小化负样本的概率&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;设NEG(w)是关于词w的一个负样本集，对已一个给定的&lt;r&gt;二元对&lt;/r&gt;(Context(w),w),&lt;r&gt;优化目标&lt;/r&gt;是&lt;/p&gt;

&lt;p&gt;$$ g(w) = \prod\limits_{u \in w \cup NEG(w)} p(u|Context(w))$$&lt;/p&gt;

&lt;p&gt;=================================================================&lt;/p&gt;

&lt;p&gt;关于模型的&lt;r&gt;推导&lt;/r&gt;:&lt;/p&gt;

&lt;p&gt;为每个词设置一个&lt;r&gt;辅助向量\(\theta^u\)&lt;/r&gt;，\(X_w^T\)表示为Context(w)中的k个词进行加和后的向量，\(L^w(u)\)表示词u的样本标签，正样本取1，反之取0。&lt;/p&gt;

&lt;p&gt;第一步，&lt;strong&gt;转换目标函数&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;$$g(w) = \prod\limits_{u \in w \cup NEG(w)} [\sigma(\theta^u X_w^T)]^{L^w(u)} [1 - \sigma(\theta^u X_w^T)] ^ {(1-L^w(u))}$$&lt;/p&gt;

&lt;p&gt;因为正样本只有一个，故可简化为\(g(w) =\sigma(\theta^u X_w^T) \prod\limits_{u \cup NEG(w)} [1 - \sigma(\theta^u X_w^T)] ^ {1-L^w(u)}\)&lt;/p&gt;

&lt;p&gt;第二步，&lt;strong&gt;考虑整个语料库&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;$$F = log G = log \prod\limits_{w \in C} g(w)$$
$$ = \sum\limits_{w \in C} \sum\limits_{u \in w \cup NEG(w)} {L^w(u)}log[\sigma(\theta^u X_w^T)] + (1-L^w(u))log[1 - \sigma(\theta^u X_w^T)]$$
$$ = \sum\limits_{w \in C} \bigg (log(\sigma(\theta^u X_w^T)) + \sum\limits_{u \in NEG(w)} log(\sigma(-\theta^u X_w^T))\bigg)$$&lt;/p&gt;

&lt;p&gt;第三步，&lt;strong&gt;随机梯度上升&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;$$ F(w,u) = L^w(u) log[\sigma(\theta^u X_w^T)] + (1-L^w(u)) log[1 - \sigma(\theta^u X_w^T)] $$&lt;/p&gt;

&lt;p&gt;\(\theta^u\)的&lt;r&gt;更新公式&lt;/r&gt;：&lt;/p&gt;

&lt;p&gt;$$\frac{\partial F(w,u)}{\partial \theta^u}  = L^w(u) (1 -\sigma(\theta^u X_w^T)) X_w - (1-L^w(u)) \sigma(\theta^u X_w^T) X_w $$
$$  =  L^w(u) X_w - \sigma(\theta^u X_w^T) X_w $$&lt;/p&gt;

&lt;p&gt;$$\theta^u := \theta^u + \eta (L^w(u) - \sigma(\theta^u X_w^T)) X_w$$&lt;/p&gt;

&lt;p&gt;根据\(\theta^u\)和\(X_w\)的&lt;r&gt;对称性&lt;/r&gt;，w上下文中的每个词向量的&lt;r&gt;更新公式&lt;/r&gt;为：&lt;/p&gt;

&lt;h1 id=&#34;v-tilde-w-v-tilde-w-sum-limits-u-in-w-cup-neg-w-eta-l-w-u-sigma-theta-u-x-w-t-theta-u-tilde-w-in-context-w:d45865320239d7818255e82b856775aa&#34;&gt;$$v(\tilde{w}) := v(\tilde{w}) + \sum\limits_{u \in w \cup NEG(w)} \eta (L^w(u) - \sigma(\theta^u X_w^T))\theta^u, \tilde{w} \in Context(w)$$&lt;/h1&gt;

&lt;h3 id=&#34;2-skip-gram:d45865320239d7818255e82b856775aa&#34;&gt;2. SKIP-GRAM&lt;/h3&gt;

&lt;h4 id=&#34;hierarchical-softmax-1:d45865320239d7818255e82b856775aa&#34;&gt;Hierarchical Softmax&lt;/h4&gt;

&lt;p&gt;与CBOW不同是，输入层只有一个词向量w。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/word2vec-hs-sg.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;关键在于\(p(Context(w)|w)\)的定义：&lt;/p&gt;

&lt;p&gt;$$ p(Context(w)|w) = \prod\limits_{u \in Context(w)} p(u|w) $$，其他推导类似CBOW。&lt;/p&gt;

&lt;h4 id=&#34;negative-sampling-1:d45865320239d7818255e82b856775aa&#34;&gt;Negative Sampling&lt;/h4&gt;

&lt;p&gt;已知词w，预测词w的上下文，则针对上下文中的每个词，将该词看作正样本，选取一定数量的负样本。目标函数为：&lt;/p&gt;

&lt;p&gt;$$F = log G = log \prod\limits_{w \in C} g(w)$$
$$ = \sum\limits_{w \in C} \sum\limits_{u \in Context(w)} \sum\limits_{z \in u \cup NEG(u)} {L^u(z)}log[\sigma(\theta^z X_w^T)] + (1-L^u(z))log[1 - \sigma(\theta^z X_w^T)]$$&lt;/p&gt;

&lt;p&gt;公式中对于一个样本(w, Context(w)), 对Context(w)中的每个词，都做了一次负采样。而word2vec源码对此作了改动，只对词w进行了|Context(w)|次负采样。&lt;/p&gt;

&lt;h2 id=&#34;c-源码阅读:d45865320239d7818255e82b856775aa&#34;&gt;c++源码阅读&lt;/h2&gt;

&lt;h3 id=&#34;命令行参数:d45865320239d7818255e82b856775aa&#34;&gt;命令行参数&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;参数&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;size&lt;/td&gt;
&lt;td&gt;Projection Layer的长度，即词向量的长度&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;train&lt;/td&gt;
&lt;td&gt;语料库地址，即训练集&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;save-vocab&lt;/td&gt;
&lt;td&gt;词典保存地址&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;read-vocab&lt;/td&gt;
&lt;td&gt;词典读取地址，如果没指定，从语料库中学习词典&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;debug&lt;/td&gt;
&lt;td&gt;debug_mode，debug信息打印控制&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;td&gt;是否保存为二进制&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;cbow&lt;/td&gt;
&lt;td&gt;确定是cbow，还是skip-gram模型&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;alpha&lt;/td&gt;
&lt;td&gt;学习率&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;模型保存地址&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;window&lt;/td&gt;
&lt;td&gt;窗口大小，默认值5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;sample&lt;/td&gt;
&lt;td&gt;采样百分比&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;hs&lt;/td&gt;
&lt;td&gt;是否进行Hierarchical Softmax&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;negative&lt;/td&gt;
&lt;td&gt;是否进行负采样&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;threads&lt;/td&gt;
&lt;td&gt;线程数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;min-count&lt;/td&gt;
&lt;td&gt;有效词频的最小值，默认值5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;classes&lt;/td&gt;
&lt;td&gt;聚类个数&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;数据结构:d45865320239d7818255e82b856775aa&#34;&gt;数据结构&lt;/h3&gt;

&lt;p&gt;每个词用结构体&lt;r&gt;vocab_word&lt;/r&gt;保存信息:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct vocab_word{
    char *word; //词本身
    long long cn; //语料集中的词频
    int *point; //huffman树中，从根节点到该词所在叶节点的中间结点列表
    char *code, codelen; //该词对应的huffman编码和编码长度
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;整个词表用一个数组vocab表示，为了&lt;strong&gt;快速查找&lt;/strong&gt;给定词的信息，建立了一个&lt;r&gt;hash表&lt;/r&gt;，存放的元素是该词在vocab中的下标。&lt;/p&gt;

&lt;p&gt;哈希的策略是&lt;strong&gt;线性探测的开放定址法&lt;/strong&gt;，计算词w的哈希值hv(w)作为下标索引，若该位置被占用，则顺序往下找，直到找到一个未被占用的位置。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vocab = (struct vocab_word *)calloc(vocab_max_size, sizeof(struct vocab_word));
vocab_hash = (int *)calloc(vocab_hash_size, sizeof(int));
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;模型实现:d45865320239d7818255e82b856775aa&#34;&gt;模型实现&lt;/h3&gt;

&lt;h4 id=&#34;1-关于net:d45865320239d7818255e82b856775aa&#34;&gt;1. 关于Net&lt;/h4&gt;

&lt;p&gt;定义了&lt;r&gt;syn0&lt;/r&gt;,&lt;r&gt;syn1&lt;/r&gt;,&lt;r&gt;syn1neg&lt;/r&gt;三个浮点数组，大小都为vocab_size*layer1_size。其中vocab_size为词表的长度，layer1_size为词向量的长度。&lt;/p&gt;

&lt;p&gt;syn0表示输入层需要的每个词的词向量\(v\), syn1表示huffman树中每个中间结点的辅助向量\(\theta\), syn1neg表示negative sampling中为每个词设置的辅助向量\(theta\)。代码中Hierarchical Softmax和Negative Sampling可以同时训练。neu1代表\(X_w\),即输入词向量加和后的向量；neu1e代表\(e\)，即huffman树每一条路径（从根到叶子节点）梯度更新的求和，用来更新相应的词向量。&lt;/p&gt;

&lt;h4 id=&#34;2-sigmoid函数值计算:d45865320239d7818255e82b856775aa&#34;&gt;2. sigmoid函数值计算&lt;/h4&gt;

&lt;p&gt;建模过程中用到&lt;r&gt;逻辑回归&lt;/r&gt;做二分类问题，程序中采用表格的方式，存储\(x \in [-6,6]\区间（等分成1000份）的sigmoid函数值，小于-6，默认为0；大于6，默认为1。每次计算时，查表即可。&lt;/p&gt;

&lt;h4 id=&#34;3-创建huffman树:d45865320239d7818255e82b856775aa&#34;&gt;3. 创建huffman树&lt;/h4&gt;

&lt;p&gt;简单来说，用一个vocab_size*2+1大小的数组存储树的&lt;r&gt;全部节点&lt;/r&gt;，其中前vocab_size个元素为全部的&lt;r&gt;叶子节点&lt;/r&gt;且按词频降序排列；后面的元素表示&lt;r&gt;中间结点&lt;/r&gt;，且初始化词频为1e15。用两个变量&lt;r&gt;pos1&lt;/r&gt;,&lt;r&gt;pos2&lt;/r&gt;记录构造树的情况，其中pos1指向vocab_size-1，pos2指向vocab_size。每次从pos1,pos2选取词频较小的那个结点，并相应的向前或向后移动，更新pos。选择两次，合并两个选中的结点为一棵新树。在构建树的过程中，还需要记录各个节点的编码。&lt;/p&gt;

&lt;p&gt;详见&lt;r&gt;CreateBinaryTree&lt;/r&gt;函数。&lt;/p&gt;

&lt;h4 id=&#34;4-负采样:d45865320239d7818255e82b856775aa&#34;&gt;4. 负采样&lt;/h4&gt;

&lt;p&gt;假设词典中的每个词w对应一个线段l(w),其长度为：&lt;/p&gt;

&lt;p&gt;$$len(w) = \frac{[counter(w)]^{0.75}}{\sum\limits_{u \in D} [counter(u)]^{0.75}} $$&lt;/p&gt;

&lt;p&gt;将这些线段首尾相接拼接在一起，形成一个长度为1的线段。随机往这个线段上打点，则高频词被打中的概率大。将这个单位线段等距离分割成M份(M&amp;gt;&amp;gt;N)，每次生成一个[0,M-1]的随机数r，r对应的词即为采样。&lt;/p&gt;

&lt;p&gt;详见&lt;r&gt;InitUnigramTable&lt;/r&gt;函数。&lt;/p&gt;

&lt;h4 id=&#34;5-学习率:d45865320239d7818255e82b856775aa&#34;&gt;5. 学习率&lt;/h4&gt;

&lt;p&gt;自适应&lt;/p&gt;

&lt;h4 id=&#34;6-多线程:d45865320239d7818255e82b856775aa&#34;&gt;6. 多线程&lt;/h4&gt;

&lt;p&gt;整个工作流程是：首先读取语料库中词表和词频信息，做一些模型初始化工作，然后多线程训练word2vec，判断classes，如果为0，则保存词向量；反之，对得到的词向量进行K-means聚类，聚类个数为classes，并保存聚类结果。&lt;/p&gt;

&lt;p&gt;其中多线程的部分是模型训练的核心，它根据&lt;r&gt;线程数划分文件&lt;/r&gt;，每个线程负责语料库的一部分。首先根据文件指针读取一个句子，用一个数组保存下句子中的每个词；然后根据命令行参数，选择训练CBOW或Skip-gram，其中每个模型都可以采用hs或negative的框架训练。&lt;/p&gt;

&lt;h2 id=&#34;spark-mllib-1-6-实现:d45865320239d7818255e82b856775aa&#34;&gt;Spark MLlib 1.6 实现&lt;/h2&gt;

&lt;h3 id=&#34;可调参数:d45865320239d7818255e82b856775aa&#34;&gt;可调参数&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;参数&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;vectorSize&lt;/td&gt;
&lt;td&gt;词向量的长度&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;learningRate&lt;/td&gt;
&lt;td&gt;学习率&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;numPartitions&lt;/td&gt;
&lt;td&gt;分区数，默认为1，越小准确率越高&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;numIterations&lt;/td&gt;
&lt;td&gt;迭代次数，默认为1，不大于numPartitions&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;seed&lt;/td&gt;
&lt;td&gt;随机数种子&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;minCount&lt;/td&gt;
&lt;td&gt;有效词频的最小值，默认值5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;window&lt;/td&gt;
&lt;td&gt;上下文的窗口大小，默认值5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;主要实现了基于hierarchical softmax框架下的skip-gram模型。&lt;/p&gt;

&lt;h3 id=&#34;广播:d45865320239d7818255e82b856775aa&#34;&gt;广播&lt;/h3&gt;

&lt;p&gt;对于sigmod函数值表格，词信息数组，词的hash表三个变量，通过广播机制，变成全局变量。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val sc = dataset.context //dataset: RDD[Iterable[String]]
val expTable = sc.broadcast(createExpTable)
val bcVocab = sc.broadcast(vocab)
val bcVocabHash = sc.broadcast(vocabHash)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;分布式:d45865320239d7818255e82b856775aa&#34;&gt;分布式&lt;/h3&gt;

&lt;p&gt;根据语料集文件，读取文件中的全部句子，将句子按分区数分发。&lt;/p&gt;

&lt;h2 id=&#34;参考链接:d45865320239d7818255e82b856775aa&#34;&gt;参考链接&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.cnblogs.com/peghoty/p/3857839.html&#34;&gt;http://www.cnblogs.com/peghoty/p/3857839.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Efficient Estimation of Word Representations in Vector Space&lt;/p&gt;

&lt;p&gt;Distributed Representations of Words and Phrases and their Compositionality&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Random Forest</title>
      <link>http://cherishzhang.github.io/post/ml/random_forest/</link>
      <pubDate>Wed, 03 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/ml/random_forest/</guid>
      <description>

&lt;h2 id=&#34;1-intro:00635f635026fbac82e8a20eddd81309&#34;&gt;1. Intro&lt;/h2&gt;

&lt;p&gt;基本原理：一个由多个决策树构成的森林，算法分类结果由这些决策树投票得到，决策树在生成的过程当中分别在行方向和列方向上添加&lt;r&gt;随机过程&lt;/r&gt;，行方向上构建决策树时采用&lt;r&gt;放回抽样&lt;/r&gt;（bootstraping）得到训练数据，列方向上采用&lt;r&gt;无放回随机抽样&lt;/r&gt;得到特征子集，并据此得到其最优切分点。&lt;strong&gt;随机森林通过多个决策树投票结果进行分类，算法不容易出现过度拟合问题。&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-construct:00635f635026fbac82e8a20eddd81309&#34;&gt;2. Construct&lt;/h2&gt;

&lt;p&gt;分为两方面:&lt;strong&gt;&lt;r&gt;数据&lt;/r&gt;&lt;/strong&gt;的随机性选取，以及&lt;strong&gt;&lt;r&gt;待选特征&lt;/r&gt;&lt;/strong&gt;的随机选取。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;数据&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;从原始数据集中&lt;strong&gt;有放回&lt;/strong&gt;的抽样，构造子数据集，子数据集的数据量和原始数据集相同。&lt;/li&gt;
&lt;li&gt;利用子数据集构建子决策树&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;待选特征&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;在子树的每一个分裂过程中并未用到所有的待选特征。&lt;/li&gt;
&lt;li&gt;从原始特征无放回随机抽取一定比例的特征，用所抽取的特征集作特征选择。&lt;/li&gt;
&lt;li&gt;假设总的特征数量是M，则比例可为\(\sqrt{M}, \frac{1}{2}\sqrt{M}, 2\sqrt{M}\)。&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;训练过程：&lt;br/&gt;
input: 给定训练集S，测试集T，特征维数F。确定参数集(CART的数量t，每棵树的深度d，每个节点用到的特征数量f，终止条件：节点上最少的样本数，节点上最少的信息增益m)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于i = 1,2,&amp;hellip;,t;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;(1) 从S中有放回的抽样，生成一个大小和S相同的训练集S(i)，作为根节点的样本集，从根节点开始训练。&lt;br/&gt;
(2) a. 如果当前节点满足终止条件，则设置当前节点为叶子节点。&lt;/p&gt;

&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;分类问题，该叶子节点的预测输出为当前样本集和中数量最多的那一类C(j),概率p为c(j)占当前样本集的比例；&lt;/li&gt;
&lt;li&gt;回归问题，预测输出为当前样本集所有样本的平均值。&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;(2) b. 如果不满足终止条件，则从F维特征中无放回的随机选取f维特征。利用这f维特征，寻找分类效果最好的那一维特征k及阈值th。当前节点上样本第k维特征小于th的样本被划分到左节点，其余的被划分到右节点。&lt;/p&gt;

&lt;p&gt;(3) 重复(1)(2)直到所有节点都被训练过或标记为叶子节点。 &lt;br/&gt;
(4) 重复(1)(2)(3)直到所有的决策树被训练过。 &lt;br/&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;预测过程：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于i = 1,2,&amp;hellip;,t;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;(1) 从当前树的根节点开始，根据节点对应阈值th，判断是进入左节点还是右节点，直到到达某个叶子节点，并输出预测值。&lt;br/&gt;
(2) 重复执行(1)，直到得到所有树的预测值。&lt;strong&gt;分类问题&lt;/strong&gt;，则输出所有树中预测概率总和最大的那一类，即对每个c(j)的p进行累计；&lt;strong&gt;回归问题&lt;/strong&gt;，则输出为所有树的平均预测值。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;3-evaluation:00635f635026fbac82e8a20eddd81309&#34;&gt;3. Evaluation&lt;/h2&gt;

&lt;p&gt;rf无需交叉验证来评价分类的准确性，本身自带OOB（out-of-bag）错误估计。&lt;/p&gt;

&lt;h2 id=&#34;4-spark-mllib:00635f635026fbac82e8a20eddd81309&#34;&gt;4. Spark MLlib&lt;/h2&gt;

&lt;p&gt;(1)切分点抽样统计&lt;/p&gt;

&lt;p&gt;在单机环境下的决策树对连续变量进行切分点选择时，一般是通过对特征点进行排序，然后取相邻两个数之间的点作为切分点，这在单机环境下是可行的，但如果在分布式环境下如此操作的话，会带来大量的网络传输操作，特别是当数据量达到 PB 级时，算法效率将极为低下。为避免该问题，Spark 中的随机森林在构建决策树时，会对各分区采用一定的子特征策略进行抽样，然后生成各个分区的统计数据，并最终得到切分点。&lt;/p&gt;

&lt;p&gt;(2)特征装箱（Binning）&lt;/p&gt;

&lt;p&gt;决策树的构建过程就是对特征的取值不断进行划分的过程，对于离散的特征，如果有 M 个值，最多\(2^M-1\)个划分，如果值是有序的，那么就最多 M-1 个划分。比如年龄特征，有老，中，少 3 个值，如果无序有\(2^M-1\)个，即 3 种划分：老|中，少；老，中|少；老，少|中；如果是有序的，即按老，中，少的序，那么只有 m-1 个，即 2 种划分，老|中，少；老，中|少。对于连续的特征，其实就是进行范围划分，而划分的点就是 split（切分点），划分出的区间就是 bin。对于连续特征，理论上 split 是无数的，在分布环境下不可能取出所有的值，因此它采用的是（1）中的切点抽样统计方法。&lt;/p&gt;

&lt;p&gt;(3)逐层训练（level-wise training）&lt;/p&gt;

&lt;p&gt;单机版本的决策数生成过程是通过递归调用（本质上是深度优先）的方式构造树，在构造树的同时，需要移动数据，将同一个子节点的数据移动到一起。此方法在分布式数据结构上无法有效的执行，而且也无法执行，因为数据太大，无法放在一起，所以在分布式环境下采用的策略是逐层构建树节点（本质上是广度优先），这样遍历所有数据的次数等于所有树中的最大层数。每次遍历时，只需要计算每个节点所有切分点统计参数，遍历完后，根据节点的特征划分，决定是否切分，以及如何切分。&lt;/p&gt;

&lt;h2 id=&#34;5-codes:00635f635026fbac82e8a20eddd81309&#34;&gt;5. Codes&lt;/h2&gt;

&lt;p&gt;参考sklearn.ensemble.RandomForestRegressor，sklearn.ensemble.RandomForestClassifier&lt;/p&gt;

&lt;p&gt;输入参数&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;名称&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;n_estimators&lt;/td&gt;
&lt;td&gt;一个森林中决策树的个数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;criterion&lt;/td&gt;
&lt;td&gt;分类准则，如&amp;rsquo;rmse&amp;rsquo;,&amp;lsquo;gini&amp;rsquo;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;max_depth&lt;/td&gt;
&lt;td&gt;树的最大深度，tree-specific&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;min_samples_split&lt;/td&gt;
&lt;td&gt;中间节点分裂前的最少样本数，tree-specific&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;min_samples_leaf&lt;/td&gt;
&lt;td&gt;新生成叶节点上的最少样本数，tree-specific&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;min_weight_fraction_leaf&lt;/td&gt;
&lt;td&gt;叶节点上样本数占全部样本的最小比例值，tree-specific&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;max_features&lt;/td&gt;
&lt;td&gt;每个节点用到的特征数量，tree-specific&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;max_leaf_nodes&lt;/td&gt;
&lt;td&gt;每棵树中的叶节点能达到的最大值，tree-specific&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;bootstrap&lt;/td&gt;
&lt;td&gt;是否构建决策树过程中，对样本进行bootstrap&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;oob_score&lt;/td&gt;
&lt;td&gt;是否使用out-of-bag样本估计泛化误差&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;n_jobs&lt;/td&gt;
&lt;td&gt;并行运行的进程数，默认等于CPU的核数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;random_state&lt;/td&gt;
&lt;td&gt;随机生成器的种子&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;verbose&lt;/td&gt;
&lt;td&gt;是否输出构建过程中的信息&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;warm_start&lt;/td&gt;
&lt;td&gt;设置为是，则使用上次拟合后的solution，再附加决策树，否则，拟合一个新的森林&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;class_weight&lt;/td&gt;
&lt;td&gt;类别权重列表&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;模型属性&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;名称&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;estimators_&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;classes_&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;n&lt;em&gt;classes&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;n&lt;em&gt;features&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;n_outputs&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;feature&lt;em&gt;importances&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;oob_score&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;参考链接:00635f635026fbac82e8a20eddd81309&#34;&gt;参考链接&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.cnblogs.com/hrlnw/p/3850459.html&#34;&gt;RandomForest随机森林总结&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Boosting</title>
      <link>http://cherishzhang.github.io/post/ml/boosting/</link>
      <pubDate>Fri, 20 May 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/ml/boosting/</guid>
      <description>

&lt;h2 id=&#34;intro:d21755d9d058d9a9b44db3100ac0dea8&#34;&gt;Intro&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_boosting&#34;&gt;Boosting&lt;/a&gt; can be interpreted as &lt;strong&gt;an optimization algorithm&lt;/strong&gt; on a suitable cost function. The algorithms that optimize a cost &lt;strong&gt;function over function&lt;/strong&gt; space by &lt;r&gt;iteratively&lt;/r&gt; choosing a function(week hypothesis) that points in the &lt;strong&gt;negative gradient direction&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;在分类问题中，提升(Boosting)思想，通过改变训练数据的概率分布（权重分布），针对不同的训练数据分布，学习多个弱分类器，将这些分类器线性组合，提高分类的性能。&lt;/p&gt;

&lt;p&gt;围绕提升方法，有&lt;strong&gt;&lt;r&gt;两个问题&lt;/r&gt;&lt;/strong&gt;需要考虑：1.每一轮如何改变训练数据的概率或权值分布；2.如何将弱分类器组合成一个强分类器。&lt;/p&gt;

&lt;h2 id=&#34;adaboost-adaptive-boosting:d21755d9d058d9a9b44db3100ac0dea8&#34;&gt;AdaBoost(Adaptive Boosting)&lt;/h2&gt;

&lt;p&gt;AdaBoost提高那些被前几轮弱分类器线性组成的分类器&lt;r&gt;错误分类&lt;/r&gt;的样本权值，在下一轮的弱分类器中更加关注没有得到正确分类的数据；采用&lt;r&gt;加权多数表决&lt;/r&gt;的方法，误差率小的弱分类器权值大，误差率大的弱
分类器权值小。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;算法流程&lt;/strong&gt;：（二分类问题）&lt;br/&gt;
&lt;r&gt;输入&lt;/r&gt;：训练数据集\(T=\{(x_1,y_1),(x_2,y_2),&amp;hellip;,(x_N,y_N)\}\)和弱分类器算法 &lt;br/&gt;
&lt;r&gt;输出&lt;/r&gt;：最终分类器\(G(x)\)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;初始化训练样本的&lt;r&gt;权值分布&lt;/r&gt;(均匀分布)&lt;br/&gt;$$D_1=(w_{11},&amp;hellip;,w_{1i},&amp;hellip;,w_{1N}), w_{1i}=\frac{1}{N}, i=1,2,&amp;hellip;,N$$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对于\(m=1,2,&amp;hellip;,M\)(迭代)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用具有权值分布\(D_m\)的训练数据集学习，得到基本分类器\(G_m(x)\)&lt;/li&gt;
&lt;li&gt;计算\(G_m(x)\)在权值分布\(D_m\)的训练数据集上的&lt;strong&gt;分类误差率&lt;/strong&gt;
$$e_m = P(G_m(x_i) \neq y_i) = \sum_{i=1}^N w_{mi}I(G_m(x_i) \neq y_i)$$&lt;/li&gt;
&lt;li&gt;计算\(G_m(x)\)的系数，即&lt;r&gt;表决权&lt;/r&gt;
$$\alpha_{m} = \frac{1}{2} \log \frac{1-e_m}{e_m}$$&lt;/li&gt;
&lt;li&gt;更新训练数据集的权值分布
$$D_{m+1} = (w_{m+1,1},&amp;hellip;,w_{m+1,i},&amp;hellip;,w_{m+1,N})$$
$$w_{m+1,i} = \frac{w_{mi}}{Z_m} \exp (-\alpha_m y_i G_m(x_i))$$
其中\(Z_m\)是规范化因子,$$Z_m = \sum_{i=1}^N w_{mi} exp(-\alpha_m y_i G_m(x_i))$$&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;构建基本分类器的线性组合，得到&lt;r&gt;最终的分类器&lt;/r&gt;
$$f(x) = \sum_{m=1}^M \alpha_m G_m(x)$$
$$G(x) = sign(f(x))$$&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每轮迭代过程中，Adaboost会重点关注错误分类的样本，使得&lt;strong&gt;算法受噪声和离群点的影响较大&lt;/strong&gt;。\(\alpha_m\)随着 \(e_m\) 的减小而增大，\(w_{mi}\) 也随着\(\alpha_m\) 而改变，不断增大误分类样本的权值，减小正确分类样本的权值。&lt;/p&gt;

&lt;p&gt;AdaBoost的一个解释是，模型为&lt;strong&gt;加法模型，损失函数为指数函数，学习算法为前向分步算法的二分类学习方法&lt;/strong&gt;。前向分步算法(forward stagewise algorithm)求解优化问题的思路是：从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式，就可以简化优化的复杂度。&lt;/p&gt;

&lt;h2 id=&#34;boosting-tree-提升树:d21755d9d058d9a9b44db3100ac0dea8&#34;&gt;Boosting Tree(提升树)&lt;/h2&gt;

&lt;p&gt;采用&lt;strong&gt;加法模型(基函数的线性组合)与前向分步算法，以决策树为基函数的提升方法&lt;/strong&gt;称为提升树，分类问题采用二叉分类树，回归问题采用二叉回归树。&lt;/p&gt;

&lt;p&gt;提升树表示为决策树的加法模型:
$$f_{M(x)} = \sum_{m=1}^M T(x; \Theta_m)$$
其中，\(T(x;\Theta_m)\)表示决策树，\(\Theta_m\)为决策树的参数，M为树的个数&lt;/p&gt;

&lt;p&gt;不同问题的提升树学习算法，&lt;r&gt;主要区别在于使用的损失函数不同&lt;/r&gt;，包括用平方误差损失函数的回归问题，用指数损失函数的分类问题，以及用一般损失函数的一般决策问题。将Adaboost算法中的基本分类器限制为分类决策树，即为针对二分类问题的提升树算法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;提升树算法&lt;/strong&gt;(回归问题):&lt;br/&gt;
&lt;r&gt;输入&lt;/r&gt;:训练数据集\(T=\{(x_1,y_1),(x_2,y_2),&amp;hellip;,(x_N,y_N)\}\)
&lt;br/&gt;
&lt;r&gt;输出&lt;/r&gt;:提升树\(f_M(x)\)&lt;br/&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;确定初始提升树\(f_0(x) = 0\)&lt;/li&gt;
&lt;li&gt;对m = 1,2,&amp;hellip;,M

&lt;ul&gt;
&lt;li&gt;计算残差： \(r_{mi} = y_{i} - f_{m-1}(x_i), i=1,2,&amp;hellip;,N\)&lt;/li&gt;
&lt;li&gt;拟合残差，学习一个回归树，得到\(T(x;\Theta_m)\)&lt;/li&gt;
&lt;li&gt;更新\(f_m(x)=f_{m-1}(x)+T(x;\Theta_m)\)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;得到最终提升树：\(f_M(x)=\sum_{m=1}^M T(x;\Theta_m)\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;gradient-boosting-梯度提升:d21755d9d058d9a9b44db3100ac0dea8&#34;&gt;Gradient Boosting(梯度提升)&lt;/h2&gt;

&lt;p&gt;前向分布算法针对特殊的损失函数(平方和指数)，对于一般的损失函数，Freidman提出了梯度提升算法。它利用最速下降法的近似方法，其&lt;r&gt;&lt;strong&gt;关键&lt;/strong&gt;&lt;/r&gt;是利用损失函数的负梯度在当前模型的值最为回归问题提升树算法中的残差的近似值，拟合一个回归树。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;梯度提升算法&lt;/strong&gt;：&lt;br/&gt;
&lt;r&gt;输入&lt;/r&gt;:训练数据集\(T={(x_1,y_1),(x_2,y_2),&amp;hellip;,(x_N,y_N)}\)，损失函数\(L(y,f(x))\)&lt;br/&gt;
&lt;r&gt;输出&lt;/r&gt;:回归树\(\hat{f} (x)\)&lt;br\&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;初始化，估计是损失函数极小化的常数值，即只有一个根节点的树
\(f_0(x) = arg\mathop{min}\limits_{c} \sum_{i=1}^{N} L(y_i,c)\)&lt;/li&gt;
&lt;li&gt;对m=1,2,&amp;hellip;,M

&lt;ul&gt;
&lt;li&gt;对i=1,2,&amp;hellip;,N,计算
\(r_{mi} = - \{\left [\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)} \right]\}_{f(x)=f_{m-1}(x)}\)&lt;/li&gt;
&lt;li&gt;对于\(r_{mi}\)拟合一个回归树，得到第m棵树的叶节点区域\(R_{mj}$, j=1,2,&amp;hellip;,J\)（对于平方损失函数，就是通常所说的残差，对于一般损失函数，即为残差的近似值）&lt;/li&gt;
&lt;li&gt;对于j=1,2,&amp;hellip;,J,计算 $$c_{mj} = arg\mathop{min}\limits_{c} \sum_{x_i \in R_{mj}} L(y_i,f_{m-1}(x_i)+c)$$&lt;/li&gt;
&lt;li&gt;更新\(f_m(x) = f_{m-1}(x)+\sum_{j=1}^J c_{mj}I(x\in R_mj)\)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;得到回归树 $$\hat{f}(x) = f_M(x) = \sum_{m=1}^M \sum_{j=1}^J c_{mj} I(x \in R_{mj})$$&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Data Quality in Recommendation</title>
      <link>http://cherishzhang.github.io/post/paper/Data%20Quality%20in%20Recommendation/</link>
      <pubDate>Wed, 16 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/paper/Data%20Quality%20in%20Recommendation/</guid>
      <description>

&lt;p&gt;论文名称：&lt;a href=&#34;http://dl.acm.org/citation.cfm?id=2799670&amp;amp;preflayout=tabs&#34;&gt;Data Quality Matters in Recommender Systems, RecSys &amp;lsquo;15&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;摘要:c7e7f190dc4ad9c3cea8a960f18951d1&#34;&gt;摘要&lt;/h2&gt;

&lt;p&gt;&lt;r&gt;数据质量好坏&lt;/r&gt;在信息系统中是一个很重要的因素，但在推荐系统中考虑得却不是很多。比较常用的方法是ad-hoc式的清洗，如，去除数据集中的噪声和不可靠的记录。显露出的缺点是,没有结合数据集本身的特性。&lt;/p&gt;

&lt;p&gt;论文中的主要贡献是考虑推荐系统中两个核心的数据质量问题：&lt;strong&gt;&lt;r&gt;sparsity&lt;/r&gt;&lt;/strong&gt; 和 &lt;strong&gt;&lt;r&gt;redundancy&lt;/r&gt;&lt;/strong&gt;，并设计了数据集相关的&lt;r&gt;阈值模型&lt;/r&gt;和&lt;r&gt;采样等级模型&lt;/r&gt;，然后在一系列的公开数据集上做了验证试验。&lt;/p&gt;

&lt;h2 id=&#34;具体方法:c7e7f190dc4ad9c3cea8a960f18951d1&#34;&gt;具体方法&lt;/h2&gt;

&lt;h3 id=&#34;关于数据的稀疏性:c7e7f190dc4ad9c3cea8a960f18951d1&#34;&gt;关于数据的稀疏性&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;阈值模型(Threshold model)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;很多基于分数的推荐系统数据集中包含一部分&lt;strong&gt;冷启动&lt;/strong&gt;的用户和物品。一般的数据清洗操会去除这些用户和物品的打分信息。现实中的问题是，怎样得到最优的清洗阈值。最简单的暴力方法(brute-force)是评估所有可能的组合，但时间空间复杂度高，在实际中不可行。&lt;/p&gt;

&lt;p&gt;首先，明确目标:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The aim is to develop a heuristic method that predicts the optimal thresholds for a given user-item rating matrix, without building the model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然后，采用了一些合理的假设，如假设要预测的物品阈值是与&lt;strong&gt;物品向量&lt;/strong&gt;的平均长度&lt;code&gt;$\overline{r}_i$&lt;/code&gt;相关，在打分矩阵中，即为平均每个物品被打分的次数。由于推荐数据集中，只有一小部分的流行商品会被打分多次，大部分商品被打分的次数相对很少，这引入了模型的另外一个特征，即物品向量长度的&lt;strong&gt;power-law&lt;/strong&gt;分布,令H为物品向量长度的分布函数，拟合函数\(H={Ax}^{(-m)}\),其中x是单个物品向量的长度，m是个正数。最终可以得到数据集对应的m参数值。m越大，长尾分布中的尾部越向下，反映到数据集上，就是更加少的物品被多次打分过，也就是说，m越大，数据集本身越稀疏。&lt;/p&gt;

&lt;p&gt;得到以上两个特征因素，接下来就是构建阈值模型，得到公式: \({IT}_d = \gamma*\frac{log(\overline{r}_i)}{m^2}\)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;评价方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;总共24个数据集，包括10个开放数据集，如Movielens, Million Songs, Flixster, Moviepilot, Filmtipset, Yelp, Yahoo! Music(broken down into albums, artists, and tracks), and BookCrossing。以及14个从公司或站点获取的专业数据集(proprietary)。&lt;/p&gt;

&lt;p&gt;每个数据集9-1划分成训练集和测试集，使用*Precision@K*作为衡量测试集的标准(K为测试集的总记录数)，10折交叉验证，平均准确率作为最后的得分。&lt;/p&gt;

&lt;p&gt;实验过程：
- 首先，寻找每个数据集d的最优IT值。具体对于某个IT值，过滤掉低于IT的item，对剩余数据进行矩阵分解(Matrix Factorization),建立推荐模型，并通过测试集评测模型的准确率。不断迭代，增长IT，寻找使得测试集准确率最高的IT作为最优阈值\({IT}^{opt}_d\)，对应的准确率为\({P}^{opt}_d\)。
- 接下来，对24个数据集采用leave-one-out交叉验证。在23个数据集上训练阈值模型，然后预测剩余数据集d的阈值\({IT}^{pred}_d\),并在过滤后的数据上训练推荐模型，计算准确率\(P^{pred}_d\)。&lt;/p&gt;

&lt;p&gt;两个评测指标：&lt;/p&gt;

&lt;p&gt;$$ NTE_d = |{IT}^{opt}_d - {IT}^{pred}_d|/{IT}^{opt}_d $$&lt;/p&gt;

&lt;p&gt;$$ AR_d = P^{pred}_d/P^{opt}_d $$&lt;/p&gt;

&lt;p&gt;注：&lt;strong&gt;IT&lt;/strong&gt;(item threshold), &lt;strong&gt;AR&lt;/strong&gt;(accuracy ratio), &lt;strong&gt;NTE&lt;/strong&gt;(normalized threshold error)&lt;/p&gt;

&lt;p&gt;通过计算，\({NTE}_d\)和\({AR}_d\)之间的相关度为-0.54。这表明IT的误差越小，推荐模型的准确率越高。&lt;/p&gt;

&lt;h4 id=&#34;关于数据的冗余性:c7e7f190dc4ad9c3cea8a960f18951d1&#34;&gt;&lt;strong&gt;关于数据的冗余性&lt;/strong&gt;&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;模型&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;数据集可以通过随机采样，建立模型，使得尽可能得与在全部数据上建立模型的结果相似。这对于实际中的大规模数据是有效的。&lt;/p&gt;

&lt;p&gt;模型目标：
   - The aim is to pick the &lt;strong&gt;loweset sampling rate&lt;/strong&gt; that will still result in the recommendation model as close as possible to the model that would have been built using the complete data.&lt;/p&gt;

&lt;p&gt;与阈值模型不同的是，在采样率的取值上没有最优。因为在全体数据上构建推荐模型总是最优的。&lt;/p&gt;

&lt;p&gt;定义\(SR\)为采样数据的推荐结果与全体数据的推荐结果相似度不低于\(\Delta\)时的最低采样率。&lt;/p&gt;

&lt;p&gt;定义\(U_d, I_d, R_d\)为数据集d中的用户数量，商品数量，和打分项个数，给定采样率\(SR\),从所有用户中随机选取\(SR*U_d\)个用户，在采样的数据集上建立推荐模型，对一个固定的测试集产生预测结果。比较与全部数据预测的结果差异。&lt;/p&gt;

&lt;p&gt;以对用户采样举例，假设数据的冗余性与三个因素相关a.用户数 b.打分矩阵的稀疏性 c.V-structure，得到公式：&lt;/p&gt;

&lt;p&gt;$${SR}_d = tanh(\frac{1}{{V-structure}_d * \sqrt{U_d} * \frac{R_d}{I_d * U_d}})$$&lt;/p&gt;

&lt;p&gt;其中V-structure的定义为平均相似度之比，分子中的每对用户至少有一个打分项目相同，分母中的每对用户为全体用户的所有组合可能。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;评价&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;采用19个专业数据集，9-1比例分训练集和测试集，10折交叉验证。实验主要验证了模型的合理性。&lt;/p&gt;

&lt;h3 id=&#34;我的思考:c7e7f190dc4ad9c3cea8a960f18951d1&#34;&gt;&lt;strong&gt;我的思考&lt;/strong&gt;&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;这篇论文提出了两个模型公式，并完成了实验验证。优点是模型公式与数据集本身的特性相关，不干涉推荐系统的建模过程。&lt;/li&gt;
&lt;li&gt;遗留下的问题：数据的稀疏性和冗余性只针对MF推荐模型，还可以扩展到与其他推荐模型相结合。&lt;/li&gt;
&lt;li&gt;考虑其他的评测指标，如覆盖率和多样性；同时，模型中还可以结合物品的内容属性和用户的位置属性。&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Telstra Network Disruptions</title>
      <link>http://cherishzhang.github.io/post/kaggle/Telstra/</link>
      <pubDate>Wed, 02 Mar 2016 21:03:56 +0800</pubDate>
      
      <guid>http://cherishzhang.github.io/post/kaggle/Telstra/</guid>
      <description>

&lt;h2 id=&#34;1-overview:43e8ca42a888262ca3ed3923385c58f2&#34;&gt;&lt;strong&gt;1. Overview&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The goal of the &lt;a href=&#34;https://www.kaggle.com/c/telstra-recruiting-network/&#34;&gt;problem&lt;/a&gt; is to predict Telstr network&amp;rsquo;s fault severity at a time at a particular location based on the &lt;em&gt;log data&lt;/em&gt; available.&lt;/p&gt;

&lt;p&gt;The target has 3 categories:0,1,2. It&amp;rsquo;s a multiclass classification problems.Different types of features are extracted from log files and other sources: event_type.csv, log_feature.csv, resource_type.csv,severity_type.csv.&lt;/p&gt;

&lt;p&gt;My final score is &lt;em&gt;0.44917(72 of 974)&lt;/em&gt; in private leaderboard. Here are my &lt;a href=&#34;https://github.com/Cherishzhang/kaggle/tree/master/Telstra&#34;&gt;code&lt;/a&gt;, and I will record my solution.&lt;/p&gt;

&lt;h2 id=&#34;2-feature-engineer:43e8ca42a888262ca3ed3923385c58f2&#34;&gt;&lt;strong&gt;2. Feature Engineer&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Selecting and designing good features is an important area in machine learning, which is called &lt;em&gt;feature engineering&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;At First, I just merge all files on Id. Here are five type of features. they are location, severity_type, resource_type, event_type, log_feature.
location and severity_type are just one-to-one variables, and others are many_to_one variables.
There are about 1200 locations, some are only in the test set. The correlation between location variable and target is 0.27, it gave me a hint that neighbouring locations may be similar in the network&amp;rsquo;s fault severity problem. I used one_hot encode to solve the many_to_one features. So there are about 400+ features in the initial stage.&lt;/p&gt;

&lt;p&gt;I read a paper about how to preprocess high-cardinality Categorical attributes in classification and regression problems, but it seemed to bring little help. I will tried it again after the competition.&lt;/p&gt;

&lt;p&gt;On the forum, there are a heated discussion about the magic feature. I spent much of my time to find it.
It is the order of the same location in the severity file which follows the order of fault occurrence.It&amp;rsquo;s called &lt;em&gt;Intra-location order&lt;/em&gt;.&lt;br /&gt;
It really did a big help to the final score which improved almost 0.06. To each record, I compute the target(&lt;em&gt;fault_severity&lt;/em&gt;) probabilities from the previous same location records, and used the &lt;em&gt;previous-target_probabilities&lt;/em&gt; as a feature to build the model.&lt;/p&gt;

&lt;h2 id=&#34;3-build-models-and-ensembling:43e8ca42a888262ca3ed3923385c58f2&#34;&gt;&lt;strong&gt;3. Build models and Ensembling&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;I tried many models, decision tree, random forests, svm and xgboost. The xgboost model is performed well.
On ensembling, I just average the random forest and xgboost result as the final result.&lt;/p&gt;

&lt;h2 id=&#34;4-what-i-learned-from-other-kagglers:43e8ca42a888262ca3ed3923385c58f2&#34;&gt;&lt;strong&gt;4. What I learned from other kagglers&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;On the platform, kagglers are willing to share their ideas. Here are some valuable ideas on the &lt;a href=&#34;https://www.kaggle.com/c/telstra-recruiting-network/forums/t/19239/it-s-been-fun-post-your-code-github-links-here-after-the-competition&#34;&gt;Competition Forum&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;some-useful-tips-on-feature-engineering:43e8ca42a888262ca3ed3923385c58f2&#34;&gt;Some useful tips on feature engineering&lt;/h3&gt;

&lt;p&gt;It&amp;rsquo;s the most important step in machine learning whatever models you used.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;About location&lt;br /&gt;
&lt;em&gt;Similar location numbers have similar fault severity.(Treat the location as numeric.)&lt;/em&gt;
&lt;em&gt;Don&amp;rsquo;t one-hot-encode the location, Tree-based classifiers are not good at handling huge sparse feature matrix.&lt;/em&gt;
&lt;em&gt;Frequency of LogType_203 = 0 &amp;amp; LogType_203 &amp;gt;0 per location&lt;/em&gt;
&lt;em&gt;The records in log data are arranged in the order of time.(the magic feature)&lt;/em&gt; Here are two ways to encode
the information, One is for each location, use the row number, which starts from 1 to the total number of rows for that locaiton. The other
is to normalize it between 0 and 1.
&lt;em&gt;percentile transformation of location counts&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;About log feature&lt;br /&gt;
&lt;em&gt;Pattern of log feature&lt;/em&gt;
&amp;ldquo;one hot&amp;rdquo; encoding for all log features with volume &amp;gt; 0, for all rows.
Each &amp;ldquo;one hot&amp;rdquo; encoded pattern treated as a string.
Assigned integer ID to each to each string, used as feature.
The log transform for the count of &amp;ldquo;pattern of log feature&amp;rdquo;, the log transform for counts as &amp;ldquo;pattern of event&amp;rdquo;
and &amp;ldquo;pattern of resource&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Common categorical variables&lt;br /&gt;
&lt;em&gt;For high-cardinality categorical variables, frequency works well.(Add the frequency of each location in both train and test set.)&lt;/em&gt;
&lt;em&gt;Summary statistics to reduce one-to-many relationship to one-to-one, and two-way or more-way interaction among multiple variables.&lt;/em&gt;
&lt;em&gt;Meta features(using Logistic regression to fit sparse matrix as predictors, then ensemble the model).&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;A useful solution&lt;br /&gt;
&amp;gt;a.The order of id on log_feature was frozen.&lt;br /&gt;
&amp;gt;b.Converting location, log_feature into numbers and generating count, mean, sum, etc. features (feature set A)&lt;br /&gt;
&amp;gt;c.Feature B was generated by shifting A forward by 1 row&lt;br /&gt;
&amp;gt;d.Feature C was generated by shifting B backward by 1 row&lt;br /&gt;
&amp;gt;Combining A, B, and C and training xgb, RF, GBM models. My final model is an ensemble model of these models.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>