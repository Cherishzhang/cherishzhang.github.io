<!DOCTYPE html>
<html lang="en-us">
<head>
<meta charset="utf-8">
<meta name="generator" content="Hugo 0.18.1" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link href="http://fonts.googleapis.com/css?family=Roboto:400,700" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/github.min.css">
<link rel="stylesheet" href="/css/normalize.css">
<link rel="stylesheet" href="/css/skeleton.css">
<link rel="stylesheet" href="/css/custom.css">
<link rel="alternate" href="/index.xml" type="application/rss+xml" title="Learn and record">
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<link rel="stylesheet" href="/css/solarized-dark.css">
<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<title>Spark之RDD - Learn and record</title>
</head>
<body>

<div class="container">

	<header role="banner">
		<div class="header-logo">
			<a href="/"><img src="/images/logo.jpg" width="60" height="60"></a>
		</div>
		
	</header>


	<main role="main">
		<article itemscope itemtype="http://schema.org/BlogPosting">
			<h1 class="entry-title" itemprop="headline">Spark之RDD</h1>
			<span class="entry-meta"><time itemprop="datePublished" datetime="2016-08-20">August 20, 2016</time></span>
			<section itemprop="entry-text">
				

<h3 id="rdd">RDD</h3>

<p>RDD，全称为Resilient Distributed Datasets（弹性分布式数据集）。简单接触了spark后，萌发了想要深入且透彻的了解RDD到底是什么。</p>

<p>从英文解释来看，它应该是Dataset的集合，且是一个提供了许多操作接口的数据集合。和普通数据集的区别是：<r>实际数据分布存储在一批机器的内存或硬盘中</r>。当然RDD还有容错和并行的功能。</p>

<p>下面摘录了一段对RDD的解释：</p>

<p><sg>它是一个<strong>容错</strong>且<strong>并行</strong>的数据结构，可以让用户<strong>显式</strong>地将数据存储到磁盘和内存中，并能控制数据的<strong>分区</strong>。它只能基于在稳定物理存储中的数据集和其他已有的RDD上执行确定性操作(transformation)来创建。</sg></p>

<p>所以RDD的两个关键点为：带操作接口的数据集，分布式。</p>

<p>举个例子：</p>

<pre><code>//读取文件创建RDD
val rdd = sc.textFile(&quot;README.md&quot;).cache()
val numAs = rdd.filter(line =&gt; line.contains(&quot;a&quot;)).count()//包含a的行数
val numBs = rdd.filter(line =&gt; line.contains(&quot;b&quot;)).count()//包含b的行数
println(&quot;Lines with a: %s, Lines with b: %s&quot;.format(numAs, numBs))
</code></pre>

<p>由此可见，RDD<r>本质</r>是一个<r>只读的分区记录集合</r>。一个RDD包含多个分区，每个分区是一个dataset片段。</p>

<p>前面提到了带操作接口的数据集，操作接口主要是RDD提供了对数据进行映射，归一等操作的API接口。这无疑方便了对分布式环境中的数据整合操作。操作分为两类：转换(transformations)和行动(actions)。前者根据原有的RDD创建一个新的RDD，后者则操作结果返回给driver。</p>

<p>以前总是不理解操作分成两类的原因，直接每次操作后都返回结果不可以么？解释是这样的，首先考虑到有些操作(如filter)过后，返回结果的数据量较大，而有些操作(如count)过后，返回的结果却很简洁。spark设计时，将转换操作设置为lazy的，即不会立即计算结果，只有遇到action，才会处理并返回结果。这样会使spark更高效。</p>

<p>转换类的操作有：map, filter, flatMap, mapPartitions, mapPartitionsWithIndex, sample, union, intersection, distinct, groupByKey, reduceByKey, aggregateByKey, sortByKey, join, cogroup, cartesian, pipe, coalesce, repartition。</p>

<p>行动类的操作有：reduce, collect, count, first, take, takeSample, takeOrdered, saveAsTextFile, saveAsSequenceFile, saveAsObjectFile</p>

<p>相关操作函数如下表：</p>

<pre><code>val rdd1 = sc.parallelize(1 to 9, 3)
val rdd2 = sc.parallelize(list(&quot;dog&quot;,&quot;tiger&quot;,&quot;lion&quot;,&quot;cat&quot;,&quot;parther&quot;,&quot;eagle&quot;),2)
val rdd3 = sc.parallelize(List(1,2),(3,4),(3,6)))
</code></pre>

<table>
<thead>
<tr>
<th>函数名</th>
<th>描述</th>
<th align="center">输入与输出</th>
<th>Code</th>
</tr>
</thead>

<tbody>
<tr>
<td>map</td>
<td>对RDD中的每个元素执行一个指定的函数来产生一个新的RDD，映射函数的参数为RDD中的每一个元素</td>
<td align="center">一对一</td>
<td>val a = rdd1.map(x =&gt; x*2)</td>
</tr>

<tr>
<td>mapValues</td>
<td>对象是元素为kv对的RDD，key保持不变，对value进行映射</td>
<td align="center">一对一</td>
<td>val b = rdd2.map(x =&gt; (x.length, x)) <br\> b.mapValues(&ldquo;x&rdquo; + _ + &ldquo;x&rdquo;).collect</td>
</tr>

<tr>
<td>flatMap</td>
<td>扁平化map后的结果，多个集合合并为一个集合</td>
<td align="center">一对一</td>
<td></td>
</tr>

<tr>
<td>mapPartitions</td>
<td>映射函数的参数为RDD中<strong>每一个分区</strong>的迭代器</td>
<td align="center">一对一</td>
<td>可用于为RDD中数据按分区创建连接</td>
</tr>

<tr>
<td>reduce</td>
<td>将RDD中元素两两传递给输入函数，产生一个新值，新产生的值与RDD中的下一个元素再被传递给输入函数，<strong>直到最后只有一个值为止</strong>。</td>
<td align="center">多对一</td>
<td>rdd1.reduce((x,y) =&gt; x+y) //结果为45</td>
</tr>

<tr>
<td>reduceByKey</td>
<td>将key相同的元素的value进行reduce，组成一个新的KV对</td>
<td align="center">多对一</td>
<td>rdd3.reduceByKey((x,y) =&gt; x+y).collect //结果为Array((1,2),(3,10))</td>
</tr>

<tr>
<td>filter</td>
<td>对每个元素应用f函数，只保留返回值为true的元素</td>
<td align="center">子集型</td>
<td>val d = rdd3.filter{ case(x,y) =&gt; equal(x,y) }.count() //结果为0</td>
</tr>

<tr>
<td>sample</td>
<td>对元素采样，获取子集</td>
<td align="center">子集型</td>
<td></td>
</tr>

<tr>
<td>cache</td>
<td>将元素从磁盘缓存到内存</td>
<td align="center">cache型</td>
<td>rdd1.cache()</td>
</tr>

<tr>
<td>persist</td>
<td>对RDD缓存，位置可指定</td>
<td align="center">cache型</td>
<td></td>
</tr>
</tbody>
</table>

<p>([1-9]\d+|[2-9])
通常来讲，针对数据处理有几种常见模型，包括：<r>Iterative Algorithms</r>,<r>Relational Queries</r>,<r>MapReduce</r>,<r>Stream Processing</r>。例如Hadoop MapReduce采用了MapReduces模型，Storm则采用了Stream Processing模型。RDD混合了这四种模型，使得Spark可以应用于各种大数据处理场景。</p>

<p>Spark将依赖分为narrow与wide。如果RDD的每个分区最多只能被一个Child RDD的一个分区使用，则称之为narrow dependency；若多个Child RDD分区都可以依赖，则称之为wide dependency。不同的操作依据其特性，可能会产生不同的依赖。例如map操作会产生narrow dependency，而join操作则产生wide dependency。</p>

<p><img src="http://ocwpxekrq.bkt.clouddn.com/rdd.png" alt="" /></p>

<p>上图说明了narrow和wide之间的区别。 narrow dependencies可以支持在同一个cluster node上以管道形式执行多条命令，而且它的失败恢复更有效，只需要重新计算丢失的parent partition即可；wide dependencies需要所有的父分区都是可用的，牵涉到RDD各级的多个Parent Partitions。</p>

<h3 id="共享变量">共享变量</h3>

<p><r>broadcast</r>变量：只读的共享变量 每个节点上都有一个拷贝。</p>

<pre><code>val broadcastVar = sc.broadcast(&quot;string test&quot;) //broadcast variable is readonly
val v = broadcastVar.value
println(v)
</code></pre>

<p><r>accumulator</r>变量：做累加器用</p>

<pre><code>val accum = sc.accumulator(0, &quot;My Accumulator&quot;) //value and name
sc.parallelize(1 to 1000000).foreach(x =&gt; accum+= 1)
println(accum.name + &quot;:&quot; + accum.value)
</code></pre>

<h3 id="sparkcontext">SparkContext</h3>

			</section>
		</article>
	</main>


	<footer role="contentinfo">
		<div class="hr"></div>
		<div class="footer-link">
			<a href="mailto:mailto:cherishzhang920508@gmail.com" target="_blank">Email</a></span>
			
			
			<a href="https://github.com/Cherishzhang" target="_blank">GitHub</a></span>
		</div>
		<div class="copyright">Copyright &copy; </div>
	</footer>

</div>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

</body>
</html>