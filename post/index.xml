<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Learn and record</title>
    <link>http://cherishzhang.github.io/post/index.xml</link>
    <description>Recent content in Posts on Learn and record</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 Mar 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://cherishzhang.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>[ml] GloVe模型理解</title>
      <link>http://cherishzhang.github.io/post/ml/glove/</link>
      <pubDate>Fri, 03 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/ml/glove/</guid>
      <description>

&lt;p&gt;一篇初学者的论文阅读笔记。尽量通俗易懂，尽量不加公式。&lt;/p&gt;

&lt;h2 id=&#34;写在前面的例子&#34;&gt;写在前面的例子&lt;/h2&gt;

&lt;p&gt;首先，是一个分析词之间共现概率的例子：&lt;/p&gt;

&lt;p&gt;考虑词i和词j是某个特定领域（如：热力学）的词，比如i=&lt;r&gt;ice&lt;/r&gt;, j=&lt;r&gt;steam&lt;/r&gt;。这里还会用到一个词k，分析\(ratio = \frac{P_{ik}}{P_{jk}} \)的情况：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;词k和ice有关，和steam无关，如k=&lt;r&gt;solid&lt;/r&gt;。那么\( ratio \)的值会很大。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;词k和steam有关，和ice无关，如k=&lt;r&gt;gas&lt;/r&gt;。那么\( ratio \)的值会很小。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;词k和ice, steam均相关/均不相关，如k=&lt;r&gt;water/fashion&lt;/r&gt;。那么\( ratio \)的值会趋于1。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这个例子想说明什么呢？ 下面是论文中的原话:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Compared to the raw probabilities, the ratio is better able to distinguish relevant words (solid and gas) from irrelevant words (water and fashion) and it is also better able to discriminate between the two relevant words.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;翻译一下，大概是，和原始概率值相比，&lt;strong&gt;ratio可以更好的区分相关词(solid, gas)和不相关词(water和fashion)&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;比起单纯分析两个词共现概率的值大小，共现概率的比更能说明词与词之间的相关程度。打个不恰当的比方，没有对比就没有伤害。&lt;/p&gt;

&lt;h2 id=&#34;关于模型&#34;&gt;关于模型&lt;/h2&gt;

&lt;p&gt;这也就因此引出了本文的正题。&lt;/p&gt;

&lt;p&gt;作为一个标杆性质的模型，word2vec可谓是集万千宠爱于一身。embedding的思想也被用于机器学习能涉及的各个领域。&lt;/p&gt;

&lt;p&gt;Word2Vec呢，通过滑动窗口，利用词之间的共现，训练词的向量表示。而glove捕捉的是全局信息，所谓全局，就是例子中ratio的引入。&lt;/p&gt;

&lt;p&gt;模型的核心， 即&lt;r&gt;损失函数&lt;/r&gt; J：&lt;/p&gt;

&lt;p&gt;$$ J = \sum_{i,j=1}^V f(X_{ij}) (w_i^T \tilde{w_j} + b_i + \tilde{b_j} - logX_{ij})^2 $$&lt;/p&gt;

&lt;p&gt;看上去，与矩阵分解的损失函数有些类似。关于推导过程，这里就不做过多赘述（可阅读论文相关段落）。以笔者的理解，简单的概况如下：&lt;/p&gt;

&lt;p&gt;从一个未知的F函数（即ratio）入手，&lt;/p&gt;

&lt;p&gt;$$ F(w_i,w_j, \tilde{w_k}) = \frac{P_{ik}}{P_{jk}} $$&lt;/p&gt;

&lt;p&gt;发现函数F在一些限制条件（向量线性关系的保持，对称性）下，可以被定义成一种唯一的形式。&lt;/p&gt;

&lt;p&gt;$$ F(w_i^T\tilde{w_k}) = \frac{X_{ik}}{X_{i}} $$&lt;/p&gt;

&lt;p&gt;其中F = exp， 可以推导出：\( w_i^T\tilde{w_k} + b_i + \tilde{b_k} = log(X_{ik}) \)&lt;/p&gt;

&lt;p&gt;加入权重因子的考虑，这也就如愿得到了上面的损失函数。&lt;/p&gt;

&lt;p&gt;&lt;r&gt;权重因子&lt;/r&gt;：使低频的词共现，高频的词共现都能不被overweighted。文中给出了如下的定义：&lt;/p&gt;

&lt;p&gt;$$
f(x)=\begin {cases}
(x/x_{max})^\alpha, &amp;amp; x&amp;lt;x_{max} \\
1, &amp;amp; x\geq{x_{max}}
\end {cases}
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[ml] Stacking</title>
      <link>http://cherishzhang.github.io/post/ml/stacking/</link>
      <pubDate>Fri, 24 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/ml/stacking/</guid>
      <description>

&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;

&lt;p&gt;机器学习比赛中常常会用到的一个技巧：stacking，它通过组合不同的预测模型来提高最终结果的准确率。下面内容主要是阅读&lt;a href=&#34;https://burakhimmetoglu.com/2016/12/01/stacking-models-for-improved-predictions/&#34;&gt;文章&lt;/a&gt;后，总结学习到的一些东西。&lt;/p&gt;

&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;

&lt;p&gt;以kaggle上&lt;a href=&#34;https://www.kaggle.com/c/house-prices-advanced-regression-techniques&#34;&gt;房屋价格预测&lt;/a&gt;为例，该挑战给出了有关房屋的多维特征，目标是通过构建模型，预测房屋的价格。stacking的可行做法如下：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;将训练集随机分成10 folds，留最后一份作为验证集(holdout data for validation)。前9份在level 1中使用；&lt;/p&gt;

&lt;p&gt;level 1: 选取XGBoost, neural network, support vector regression这三个模型，对于每个模型M，交叉验证的方式产生一个预测结果（用8份训练模型，1份预测结果）。&lt;/p&gt;

&lt;p&gt;level 2: 每个模型产生了一个9 folds大小的预测结果，将它们当成训练的feature，训练一个线性模型，并在最后一份数据上面做验证。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;some-points&#34;&gt;Some points&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;out-of-example&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;第一层level中，通过交叉验证的方式构造第二层level中的输入训练集。第二层level中，用初始化预留的最后一折数据做模型验证。&lt;/p&gt;

&lt;p&gt;假设不按照如上方法，而是在level 1时，使用全部的训练数据对模型进行参数拟合，将预测结果用于level 2时，就会出现过拟合的问题，导致偏差增大（预测结果会偏向M个模型中效果最好的一个）。&lt;/p&gt;

&lt;p&gt;stacking方法通过使用out-of-example预测，有可能使level 2的预测结果好于level 1中的各个单模型。&lt;/p&gt;

&lt;p&gt;具体流程，如下图：
&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/stacking.png&#34; alt=&#34; &#34; title=&#34;stacking&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;work for small/medium dataset&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;stacking适合中小规模数据集&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;detail&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;重复10次，将训练集划分10-folds。这会产生10个level 2的预测结果，可以对该结果集取平均。&lt;/p&gt;

&lt;p&gt;一旦确定stacking可以提高预测结果的精度，将整个流程再次运行一次。此时不再保留第 N folds作为holdout data。用N-1 folds训练模型，得到剩余1折的预测结果。集齐训练集的全部预测结果，作为level 2的输入，训练模型，输出最终测试集的结果。&lt;/p&gt;

&lt;h2 id=&#34;ref&#34;&gt;Ref&lt;/h2&gt;

&lt;p&gt;1.&lt;a href=&#34;https://burakhimmetoglu.com/2016/12/01/stacking-models-for-improved-predictions/&#34;&gt;Stacking models for improved predictions&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2.&lt;a href=&#34;http://mlwave.com/kaggle-ensembling-guide/&#34;&gt;KAGGLE ENSEMBLING GUIDE&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;3.&lt;a href=&#34;https://cran.r-project.org/web/packages/caretEnsemble/vignettes/caretEnsemble-intro.html&#34;&gt;A Brief Introduction to caretEnsemble&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>概率分布函数</title>
      <link>http://cherishzhang.github.io/post/probability_distribution/</link>
      <pubDate>Thu, 22 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/probability_distribution/</guid>
      <description>

&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;

&lt;p&gt;&amp;ldquo;Jingle bells, jingle bells, jingle all the way&amp;hellip;.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;传说北极圈住着一个老人，会在圣诞节前夜为好孩子专门送上礼物。&lt;/p&gt;

&lt;p&gt;但今年他貌似遇到了些问题。what? &lt;r&gt; %&amp;gt;_&amp;lt;% &lt;/r&gt;&lt;/p&gt;

&lt;p&gt;这是kaggle上&lt;a href=&#34;https://www.kaggle.com/c/santas-uncertain-bags&#34;&gt;一个赛题&lt;/a&gt;的背景，由于精灵误操作数据库，导致所有礼物对应的重量信息丢失了。在不能重新称重所有礼物的前提下，如何估计礼物重量，从而将礼物更合适地装入包裹中。&lt;/p&gt;

&lt;h2 id=&#34;概率分布&#34;&gt;概率分布&lt;/h2&gt;

&lt;p&gt;已知礼物(gift)分为&lt;r&gt;9类&lt;/r&gt;，共&lt;r&gt;1000个包裹&lt;/r&gt;(bag)，每个包裹要满足&lt;r&gt;三个条件&lt;/r&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;每个包裹的总重量不超过50磅，超过的话，该包裹不计入总数&lt;/li&gt;
&lt;li&gt;每个礼物只能使用一次&lt;/li&gt;
&lt;li&gt;每个包裹必须有3个或3个以上的礼物&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;虽然丢失了礼物的重量信息，但幸运的是，已知每类礼物重量的概率分布。可以根据已知的概率分布来推算每个礼物的重量。&lt;/p&gt;

&lt;p&gt;具体如下表：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;type&lt;/th&gt;
&lt;th&gt;num&lt;/th&gt;
&lt;th&gt;distribution&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;horse&lt;/td&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td&gt;\( N(5, 2) \)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;ball&lt;/td&gt;
&lt;td&gt;1100&lt;/td&gt;
&lt;td&gt;\( 1 + N(1,0.3) \)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;bike&lt;/td&gt;
&lt;td&gt;500&lt;/td&gt;
&lt;td&gt;\( N(20,10) \)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;train&lt;/td&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td&gt;\( N(10,5) \)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;coal&lt;/td&gt;
&lt;td&gt;166&lt;/td&gt;
&lt;td&gt;\(47 * beta(0.5,0.5) \)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;book&lt;/td&gt;
&lt;td&gt;1200&lt;/td&gt;
&lt;td&gt;\(chisquare(2) \)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;doll&lt;/td&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td&gt;\(gamma(5,1) \)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;block&lt;/td&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td&gt;\(triangular(5,10,20)\)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;gloves&lt;/td&gt;
&lt;td&gt;200&lt;/td&gt;
&lt;td&gt;3.0 + rand(0,1) if rand(0,1) &amp;lt; 0.3 else rand(0,1)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;下面介绍一下几个概率分布函数。&lt;/p&gt;

&lt;h4 id=&#34;1-正态分布-normal-distribution&#34;&gt;1. 正态分布(Normal distribution)&lt;/h4&gt;

&lt;p&gt;又名高斯分布。正态曲线呈钟型，两头低，中间高，左右对称，又称之为钟型曲线。&lt;/p&gt;

&lt;p&gt;若随机变量X服从一个数学期望为\( \mu \)(位置)、方差为\( \sigma^2 \)(幅度)的正态分布，记为\( N(\mu, \sigma^2) \)。该图形有3个特征，集中性(均数位于中央)；对称性；均匀变动性。&lt;/p&gt;

&lt;h4 id=&#34;2-二项分布-binomial-distribution&#34;&gt;2. 二项分布(Binomial distribution)&lt;/h4&gt;

&lt;p&gt;重复n次&lt;r&gt;独立&lt;/r&gt;的伯努利实验，每次实验只有两种可能的结果，其发生与否互相对立且独立。当实验次数为1时，二项分布服从0-1分布。&lt;/p&gt;

&lt;p&gt;举个例子，抛硬币正面向上的概率为0.5，抛N次，正面朝上发生K次的概率是服从二项分布的。&lt;/p&gt;

&lt;h4 id=&#34;3-贝塔分布-beta-distribution&#34;&gt;3. 贝塔分布(Beta distribution)&lt;/h4&gt;

&lt;p&gt;可以看作一个概率的概率分布。&lt;/p&gt;

&lt;p&gt;对于抛硬币，可以很准确知道正面朝上(系统成功)的概率为0.5，但通常情况下，系统成功的概率p是未知的。&lt;/p&gt;

&lt;p&gt;为了&lt;r&gt;测试系统成功的概率p&lt;/r&gt;，进行n次实验，统计成功次数为k，于是很直观计算出p=k/n。但由于系统成功的概率是未知的，计算出的p只是真实概率的一个最佳估计。这时p也是一个随机变量，它符合beta分布。&lt;/p&gt;

&lt;h4 id=&#34;4-卡方分布-chi-square-distribution&#34;&gt;4. 卡方分布(chi-square distribution)&lt;/h4&gt;

&lt;p&gt;若n个相互独立的随机变量，均服从&lt;r&gt;标准正态分布&lt;/r&gt;，则这n个服从标准正态分布的随机变量的平方和构成一新的随机变量，其分布规律称为卡方分布。&lt;/p&gt;

&lt;p&gt;卡方分布是由正态分布构造而成的一个新的分布。当n很大时，近似为正态分布。&lt;/p&gt;

&lt;h4 id=&#34;5-gamma分布&#34;&gt;5. Gamma分布&lt;/h4&gt;

&lt;p&gt;\(\alpha\)表示事件发生的次数，\(\beta\)表示该事件发生一次的概率，那么gamma分布代表这一事件发生\(\alpha\)次所需要时间的分布。&lt;/p&gt;

&lt;h4 id=&#34;6-三角分布&#34;&gt;6. 三角分布&lt;/h4&gt;

&lt;p&gt;如其名，其概率密度函数是一个三角形状的。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;numpy.random.triangular(left, mode, right)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;r&gt;left&lt;/r&gt;，&lt;r&gt;right&lt;/r&gt;分别为左右边界，&lt;r&gt;mode&lt;/r&gt;是峰顶。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alias Table(别名表)</title>
      <link>http://cherishzhang.github.io/post/alias/</link>
      <pubDate>Mon, 19 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/alias/</guid>
      <description>

&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;天之道，损有余而补不足；人之道，损不足以奉有余。&lt;/strong&gt; &amp;mdash;-老子《道德经》&lt;/p&gt;

&lt;p&gt;大概意思是自然的规律是多的地方补不足的地方，比如水往低处流。而人道与此相反。&lt;/p&gt;

&lt;p&gt;下面就来说说这个神奇的算法。&lt;/p&gt;

&lt;h2 id=&#34;采样&#34;&gt;采样&lt;/h2&gt;

&lt;p&gt;假设给定一个盒子，盒子中放有各种颜色的若干个相同大小的小球，对小球进行随机采样，即每次从盒子中摸出一个，记录颜色；放回，然后继续。。。&lt;/p&gt;

&lt;p&gt;那如何利用计算机模拟这一随机采样的过程呢？&lt;/p&gt;

&lt;p&gt;抽象成一个如下问题，给定如下一个集合
[a:1,b:2,c:3,d:4]，对该集合中的字母按其权重采样，一个直观的方法是对权重值加和，统计每个字母所占比例，计算对应的CDF(cumulative distribution function)，如下表：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;val&lt;/td&gt;
&lt;td&gt;a:1&lt;/td&gt;
&lt;td&gt;b:2&lt;/td&gt;
&lt;td&gt;c:3&lt;/td&gt;
&lt;td&gt;d:4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;prob&lt;/td&gt;
&lt;td&gt;0.1&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;0.4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;cum&lt;/td&gt;
&lt;td&gt;0.1&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;0.6&lt;/td&gt;
&lt;td&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;令p=rand()产生一个0～1的随机值，然后看p落在CDF的哪个范围内，就取该值返回。如p=0.4，属于0.3~0.6，那么就取c作为采样值返回。通过二分查找，该取样的时间复杂度为O(logN)。&lt;/p&gt;

&lt;h2 id=&#34;alias&#34;&gt;Alias&lt;/h2&gt;

&lt;p&gt;别名表的方法可以让采样在&lt;r&gt;O(1)&lt;/r&gt;的时间复杂度内完成。适用于离散值较多的情景。&lt;/p&gt;

&lt;p&gt;采样值通过两次rand()来决定。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;第一轮 生成一个随机数，确定一个采样值&lt;/li&gt;
&lt;li&gt;第二轮 生成另一个随机数，根据alias表中的概率，判断选择该值还是其别名。&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;val&lt;/td&gt;
&lt;td&gt;a:1&lt;/td&gt;
&lt;td&gt;b:2&lt;/td&gt;
&lt;td&gt;c:3&lt;/td&gt;
&lt;td&gt;d:4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;alias&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;prob&lt;/td&gt;
&lt;td&gt;0.4&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;0.6&lt;/td&gt;
&lt;td&gt;1.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;图解：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/alias.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;代码实现&#34;&gt;代码实现&lt;/h2&gt;

&lt;p&gt;下面是借鉴的line中的部分代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;double *prob;
long long *alias;
int num_edges;
void InitAliasTable()
{
    prob = (double *)malloc(num_edges*sizeof(double));
    alias = (long long *)malloc(num_edges*sizeof(long long));
    if (alias == NULL || prob == NULL)
    {
        printf(&amp;quot;Error: memory allocation failed!\n&amp;quot;);
        exit(1);
    }

    double *norm_prob = (double*)malloc(num_edges*sizeof(double));
    long long *large_block = (long long*)malloc(num_edges*sizeof(long long));
    long long *small_block = (long long*)malloc(num_edges*sizeof(long long));
    if (norm_prob == NULL || large_block == NULL || small_block == NULL)
    {
        printf(&amp;quot;Error: memory allocation failed!\n&amp;quot;);
        exit(1);
    }

    double sum = 0;
    long long cur_small_block, cur_large_block;
    long long num_small_block = 0, num_large_block = 0;

    for (long long k = 0; k != num_edges; k++)
        sum += edge_weight[k];
    for (long long k = 0; k != num_edges; k++)
        norm_prob[k] = edge_weight[k] * num_edges / sum;

    for (long long k = num_edges - 1; k &amp;gt;= 0; k--)
    {
        if (norm_prob[k]&amp;lt;1)
            small_block[num_small_block++] = k; //比随机采样要小的概率选中
        else
            large_block[num_large_block++] = k; //比随机采样要大的概率选中
    }

    while (num_small_block &amp;amp;&amp;amp; num_large_block) //损有余而补不足
    {
        cur_small_block = small_block[--num_small_block]; //不足
        cur_large_block = large_block[--num_large_block]; //有余
        prob[cur_small_block] = norm_prob[cur_small_block];
        alias[cur_small_block] = cur_large_block;
        norm_prob[cur_large_block] = norm_prob[cur_large_block] + norm_prob[cur_small_block] - 1;
        if (norm_prob[cur_large_block] &amp;lt; 1)
            small_block[num_small_block++] = cur_large_block;
        else
            large_block[num_large_block++] = cur_large_block;
    }

    while (num_large_block) prob[large_block[--num_large_block]] = 1;
    while (num_small_block) prob[small_block[--num_small_block]] = 1;

    free(norm_prob);
    free(small_block);
    free(large_block);
}

long long SampleAnEdge(double rand_value1, double rand_value2)
{
    long long k = (long long)num_edges * rand_value1;
    return rand_value2 &amp;lt; prob[k] ? k : alias[k];
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>哈希总结及简单实现</title>
      <link>http://cherishzhang.github.io/post/hash/</link>
      <pubDate>Sun, 18 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/hash/</guid>
      <description>

&lt;h2 id=&#34;哈希表&#34;&gt;哈希表&lt;/h2&gt;

&lt;p&gt;之前看word2vec和line的实现代码，总会出现哈希表。这里总结一下哈希表的用处。&lt;/p&gt;

&lt;p&gt;所谓哈希，其实就是一个映射函数y=f(key)，给定key时，可以得出y的值，希望独特的key对应独特的y。&lt;/p&gt;

&lt;p&gt;一般使用的场景如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/hash.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;（怕自己总是搞不清楚，所以干脆画了张图）&lt;/p&gt;

&lt;p&gt;但不同关键字可能得到同一散列地址,这种现象称&lt;strong&gt;碰撞&lt;/strong&gt;。处理冲突的方法如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;开放寻址法：线性探测再散列，二次探测再散列，伪随机数再散列&lt;/li&gt;
&lt;li&gt;再散列法&lt;/li&gt;
&lt;li&gt;链地址法&lt;/li&gt;
&lt;li&gt;建立一个公共溢出区&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;

&lt;p&gt;下面是代码实现部分，借鉴了line开源代码的具体实现。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include&amp;lt;stdlib.h&amp;gt;
#include&amp;lt;string.h&amp;gt;
struct Node {
    double val;
    char *key;
}

const int hash_table_size = 30000000;
int *hash_table;
struct Node *nodes;
unsigned int Hash(char *key) {
    unsigned int seed = 131;
    unsigned int hash = 0;
    while(*key) {
        hash = hash * seed + (*key++);
    }
    return hash % hash_table_size;
}
void InitHashTable() {
    hash_table = (int *)malloc(hash_table_size*sizeof(int));
    for (int k = 0; k != hash_table_size; k++) hash_table[k] = -1;
}
void InsertHashTable(char *key, int value) {
    int addr = Hash(key);
    while(hash_table[addr] != -1) addr = (addr + 1) % hash_table_size;
    hash_table[addr] = value;
}
int SearchHashTable(char *key) {
    int addr = Hash(key);
    while(1){
        if(hash_table[addr] == -1) return -1;
        if(!strcmp(key, node[hash_table[addr]].key)) return hash_table[addr];
        addr = (addr + 1) % hash_table_size;
    }
    return -1;
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>[tips]日常总结1</title>
      <link>http://cherishzhang.github.io/post/daily_tips_1/</link>
      <pubDate>Sat, 17 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/daily_tips_1/</guid>
      <description>

&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;

&lt;p&gt;下面笔者会记录一下日常中学习到的小知识点，以备查看。当一些知识储备充足，会系统地再做整理。&lt;/p&gt;

&lt;h2 id=&#34;12-17&#34;&gt;12-17&lt;/h2&gt;

&lt;h4 id=&#34;1-merge-join-concat-in-pandas-http-pandas-pydata-org-pandas-docs-stable-merging-html&#34;&gt;1. &lt;a href=&#34;http://pandas.pydata.org/pandas-docs/stable/merging.html&#34;&gt;merge,join,concat in pandas&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;之前接触过pandas，用的比较多的是merge，对于join的用法一直比较模糊。&lt;/p&gt;

&lt;p&gt;在数据处理和特征挖掘中，表与表之间的连接是一个经常使用的操作。&lt;/p&gt;

&lt;p&gt;函数原型如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;merge(left, right, how=&#39;inner&#39;, on=None, left_on=None,right_on=None,
left_index=False, right_index=False, sort=True, suffixes=(&#39;_x&#39;,&#39;_y&#39;),
copy=True, indicator=False)
join(self,other,on=None,how=&#39;left&#39;,lsuffix=&#39;&#39;,rsuffix=&#39;&#39;, sort=False)
concat(objs, axis=0, join=&#39;outer&#39;,join_axes=None, ignore_index=False,
keys=None, levels=None, names=None, verify_integrity=False,copy=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;r&gt;merge - 通过键值拼接列&lt;/r&gt;&lt;/p&gt;

&lt;p&gt;它类似于关系数据库中的join操作，可以根据一个或多个键将DataFrame中的行拼接起来。&lt;/p&gt;

&lt;p&gt;下面对其中的几个参数做些说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;on=None 指定拼接所用列名，如果两个对象的列名不统一，可以分别通过left_on和right_on指定。&lt;/li&gt;
&lt;li&gt;left_index/right_index 用行索引做连接键时，指定为True&lt;/li&gt;
&lt;li&gt;how 当两个表存在不重合的键时，结果的选取方式，&amp;rsquo;inner&amp;rsquo;-交集;&amp;lsquo;outer&amp;rsquo;-并集;&amp;lsquo;left&amp;rsquo;/&amp;lsquo;right&amp;rsquo;分别取一边。&lt;/li&gt;
&lt;li&gt;suffixes=(&amp;rsquo;_x&amp;rsquo;,&amp;rsquo;_y&amp;rsquo;) 当左右表格存在除连接键外的同名列时，通过添加后缀的方式作区分。&lt;/li&gt;
&lt;li&gt;多对多连接，结果为行的笛卡尔积。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;r&gt;join - 索引上的合并，拼接列&lt;/r&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;默认按索引合并，合并相同或相似的索引。&lt;/li&gt;
&lt;li&gt;可以连接多个frame&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;r&gt;concat - 以轴连接多个对象&lt;/r&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;默认axis=0情况下，concat是将objs按照append的方式拼接起来&lt;/li&gt;
&lt;li&gt;axis=1情况下与merge相同。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;12-18&#34;&gt;12-18&lt;/h2&gt;

&lt;h4 id=&#34;1-malloc-realloc-calloc&#34;&gt;1. malloc, realloc, calloc&lt;/h4&gt;

&lt;p&gt;在stdlib.h函数库中，它们的返回值是请求系统分配的地址，如果失败返回NULL。函数原型如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void* malloc(unsigned size);
void* calloc(size_t numElements, size_T sizeOfElement);
void* realloc(void* ptr, unsigned newsize);
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;malloc 在内存的动态存储区中分配一块长度为size字节的连续区域，返回该区域的首地址。&lt;/li&gt;
&lt;li&gt;calloc 在内存中申请numElements*sizeOfElement字节大小的连续地址空间。&lt;/li&gt;
&lt;li&gt;realloc 给一个已经分配了地址的指针重新分配空间，参数ptr为原有的空间地址，newsize是重新申请的地址长度。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;区别：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;malloc不能初始化所分配的内存空间，而calloc可以。&lt;/li&gt;
&lt;li&gt;realloc可以对给定指针所指的空间进行扩大或者缩小，但原有内容保持不变。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;12-25&#34;&gt;12-25&lt;/h2&gt;

&lt;h4 id=&#34;1-histogram&#34;&gt;1. histogram&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;##Compute the histogram of a set of data
numpy.histogram(a, bins=10, range=None, normed=False, weights=None, density=None)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;hitogram对数组a进行直方图统计，将数组a的取值范围分为bins个区间，并统计a中的每个值落入各个区间中的次数。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;##examples
import numpy as np
import matplotlib.pyplot as plt
np.histogram([1,2,3,1,2,4], bins=[0,1,2,3])
plt.hist([1,2,3,1,2,4], bins=3)
plt.title(&amp;quot;Histogram with 3 bins&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>[c&#43;&#43;] atoi函数</title>
      <link>http://cherishzhang.github.io/post/cplus/atoi/</link>
      <pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/cplus/atoi/</guid>
      <description>

&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;

&lt;p&gt;这是遇到过的一道面试题，在很多工程代码中也经常用到。该函数主要实现将字符串转化成整数。&lt;/p&gt;

&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;

&lt;p&gt;一种最简单的方法，遍历字符串，然后依次解析每个字符。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Solution 1
// A simple C++ program for implementation of atoi
int atoi(const char *nptr){
    int res = 0;
    for(int i = 0; nptr[i] != &#39;\0&#39;; ++i)
        res = res*10 + nptr[i] - &#39;0&#39;;
     return res;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;下面考虑一些特殊情况（实际中的特殊情况要全部考虑在内，这很重要）。首先是如何解析负数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Solution 2
int atoi(const char *nptr){
    int res = 0;
    int sign =-1;
    int i = 0;

    if(nptr[0]== &#39;-&#39;){
        sign = -1;
        i++;
     }
    for(; nptr[i] != &#39;\0&#39;; ++i)
        res = res*10 + nptr[i];
    return res*sign;
}
 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在此基础上，考虑空字符和非法字符的情况。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//Solution 3
bool isNumericChar(char x){
    return (x &amp;gt;= &#39;0&#39; &amp;amp;&amp;amp; x &amp;lt;= &#39;9&#39;) ? true: false;
}
int atoi(const char *nptr){
    if(*nptr == NULL)
        return 0;
    int res = 0;
    int sign = 1;
    int i = 0;
    
    if(nptr[0] == &#39;-&#39;){
        sign = -1;
        i++;
    }
    for(; nptr[i] != &#39;\0&#39;; ++i){
        if(isNumericChar(nptr[i]) == false)
            return 0;
         res = res*10 + nptr[i]-&#39;0&#39;;
    }
    return sign*res;
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>笔试/面试总结</title>
      <link>http://cherishzhang.github.io/post/interviews/list/</link>
      <pubDate>Sun, 25 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/interviews/list/</guid>
      <description>&lt;p&gt;关于笔试/面试基本知识点总结：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://cherishzhang.github.io/post/interviews/tcpip&#34;&gt;TCP/IP相关&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://cherishzhang.github.io/post/interviews/os&#34;&gt;linux/操作系统相关&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://cherishzhang.github.io/post/interviews/stl&#34;&gt;C/C++相关&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://cherishzhang.github.io/post/interviews/algorithm&#34;&gt;算法题相关&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>[rec] 推荐系统总结</title>
      <link>http://cherishzhang.github.io/post/rec/recommender/</link>
      <pubDate>Thu, 08 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/rec/recommender/</guid>
      <description>

&lt;p&gt;本文是对推荐系统相关知识的一个梳理和总结。主要是参考阿里的一篇技术分享ppt。&lt;/p&gt;

&lt;p&gt;推荐系统的核心问题是&lt;strong&gt;&lt;r&gt;如何评估一个用户(user)对一个物品(item)的评分(喜欢程度)&lt;/r&gt;&lt;/strong&gt;。&lt;/p&gt;

&lt;h2 id=&#34;场景和指标&#34;&gt;场景和指标&lt;/h2&gt;

&lt;p&gt;推荐系统主要用来解决信息过量负载(&lt;r&gt;information overload&lt;/r&gt;)的问题，可以应用&lt;strong&gt;&lt;r&gt;多种场景&lt;/r&gt;&lt;/strong&gt;中。如:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;音乐，电影&lt;/p&gt;

&lt;p&gt;电子商务中商品推荐&lt;/p&gt;

&lt;p&gt;个性化阅读&lt;/p&gt;

&lt;p&gt;社交好友推荐、朋友圈推荐&lt;/p&gt;

&lt;p&gt;基于位置的服务推荐&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;&lt;r&gt;评价标准&lt;/r&gt;&lt;/strong&gt;：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;名称&lt;/th&gt;
&lt;th&gt;作用&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;用户满意度(&lt;r&gt;user satisfaction&lt;/r&gt;)&lt;/td&gt;
&lt;td&gt;调研或用户反馈；点击率、转化率等&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;准确率(&lt;r&gt;Accuracy&lt;/r&gt;)&lt;/td&gt;
&lt;td&gt;precision/recall/F-score&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;覆盖率(&lt;r&gt;Coverage&lt;/r&gt;)&lt;/td&gt;
&lt;td&gt;长尾分布，尾部物品和用户&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;多样性(&lt;r&gt;Diversity&lt;/r&gt;)&lt;/td&gt;
&lt;td&gt;两两之间不相似&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;新颖性(&lt;r&gt;Novelty&lt;/r&gt;)&lt;/td&gt;
&lt;td&gt;没听过的物品&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;惊喜性(&lt;r&gt;Serendipity&lt;/r&gt;)&lt;/td&gt;
&lt;td&gt;？&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;用户信任度(&lt;r&gt;Trust&lt;/r&gt;)/可解释性(&lt;r&gt;explanation&lt;/r&gt;)&lt;/td&gt;
&lt;td&gt;推荐理由&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;鲁棒性/健壮性(&lt;r&gt;Robustness&lt;/r&gt;)&lt;/td&gt;
&lt;td&gt;哈利波特现象；抗攻击、反作弊&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;实时性(&lt;r&gt;Real-time/online&lt;/r&gt;)&lt;/td&gt;
&lt;td&gt;新加入的物品；新的用户行为(实时意图)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;商业目标(&lt;r&gt;business target&lt;/r&gt;)&lt;/td&gt;
&lt;td&gt;一个用户带来多少盈利&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;&lt;r&gt;影响推荐效果的因素&lt;/r&gt;&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;用户交互界面(User Interface)（用户感知）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数据（收集and处理）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;领域知识（产品定位）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;算法迭代&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;非个性化-推荐-热度排行-popularity&#34;&gt;&lt;strong&gt;非个性化&lt;/strong&gt;推荐：热度排行(Popularity)&lt;/h2&gt;

&lt;p&gt;排行榜算法&lt;r&gt;主要分为&lt;/r&gt;：&lt;/p&gt;

&lt;p&gt;单一维度的投票；多个维度的综合打分；考虑时间因素(引入衰减权重，如半衰期、冷却定律)；考虑反馈信息(Reddit)；考虑置信度(威尔逊区间)；防止马太效应(MBA)。&lt;/p&gt;

&lt;p&gt;这个方法容易实现，可以解决&lt;r&gt;新用户的冷启动&lt;/r&gt;问题。但无法对用户做出个性化推荐。通常情况下，可以先对新用户做热度排行推荐，然后根据用户的互动历史，做个性化推荐。&lt;/p&gt;

&lt;h2 id=&#34;协同过滤-collaborative-filtering&#34;&gt;&lt;strong&gt;协同过滤&lt;/strong&gt;(Collaborative Filtering)&lt;/h2&gt;

&lt;p&gt;它的来源很简单，即朋友之间会互相推荐自己喜爱的产品。基本组成元素有：&lt;r&gt;评分矩阵&lt;/r&gt;和&lt;r&gt;相似度度量&lt;/r&gt;。&lt;/p&gt;

&lt;p&gt;主要分为两大类：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Memory-based(neighborhood-based)方法：Item-based/User-based CF&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Model-based方法：频繁项挖掘/聚类/分类/回归/矩阵分解/RBM/图模型&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;h4 id=&#34;user-based&#34;&gt;User-based&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;计算目标用户的(前k个)相似用户(相似度度量：Pearson,Jaccard,cosine)&lt;/p&gt;

&lt;p&gt;找出相似用户喜欢的物品，并预测目标用户对这些物品的评分(knn, regression)&lt;/p&gt;

&lt;p&gt;过滤掉目标用户已经消费过的物品&lt;/p&gt;

&lt;p&gt;将剩余物品按照预测评分排序，返回top-N物品&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;item-based&#34;&gt;Item-based&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;计算item之间的两两相似性&lt;/p&gt;

&lt;p&gt;假设预测用户对item的评分，首先找出用户评分过的物品，根据它们与item的相似度预测用户的的评分&lt;/p&gt;

&lt;p&gt;选取用户评分最高的前N个商品进行推荐。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;问题1&lt;/strong&gt;： 比较user-based和item-based？&lt;/p&gt;

&lt;p&gt;Item之间的相似性是静态稳定的；而user之间的相似性是动态复杂的。&lt;/br&gt;
user-based: 可以帮助用户&lt;r&gt;发现新商品&lt;/r&gt;，带来惊喜性。但需要复杂的在线计算，而且无法处理新用户问题。&lt;/br&gt;
item-based: &lt;r&gt;准确性&lt;/r&gt;好，便于离线计算；但推荐缺乏多样性。&lt;/br&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;问题2&lt;/strong&gt;：协同过滤的优缺点？&lt;/p&gt;

&lt;p&gt;优点：模型通用性好，实现简单。&lt;/br&gt;
缺点：&lt;r&gt;冷启动&lt;/r&gt;问题，&lt;r&gt;数据稀疏性&lt;/r&gt;；假定“过去的行为决定现在”，没有考虑&lt;r&gt;具体情景&lt;/r&gt;的差异；&lt;r&gt;热门倾向性&lt;/r&gt;(Popularity Bias)，很难推荐出小众偏好。&lt;/p&gt;

&lt;h4 id=&#34;关联规则&#34;&gt;关联规则&lt;/h4&gt;

&lt;p&gt;基于物品之间的&lt;r&gt;共现性&lt;/r&gt;挖掘频繁项，应用场景为：买了又买，看了又看。主要有A-priori和FP-growth两种算法。&lt;/p&gt;

&lt;p&gt;支持度：\(s(X,Y) = \frac{包含X,Y的记录数}{总记录数}\)&lt;/p&gt;

&lt;p&gt;置信度：\(c(X,Y) = \frac{包含X,Y的记录数}{包含X的记录数}\)&lt;/p&gt;

&lt;p&gt;关联规则实现简单，通用性较强，适合&lt;strong&gt;商品搭配场景&lt;/strong&gt;；
在相似商品上的推荐效果往往不如协同过滤好；
注意在&lt;strong&gt;变量间关联&lt;/strong&gt;的解释上，由于一些隐含因素的影响，可能得出完全相反的结论(&lt;r&gt;辛普森悖论&lt;/r&gt;)。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;ps: 辛普森悖论，两组数据分别讨论时满足某种性质，一旦合并考虑，却可能导致相反的结论。&lt;r&gt;主要原因&lt;/r&gt;是数据的不同分组之间基数差异很大（通常一个组相对于其他组来说数量占绝对优势）。一个&lt;r&gt;结论&lt;/r&gt;：分析数据时合理地将数据分组很重要。&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&#34;聚类-clustering&#34;&gt;聚类(Clustering)&lt;/h4&gt;

&lt;p&gt;可以对用户按爱好分群，商品按相似度聚类。常用的算法有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;k-means, 层次聚类&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Louvain, 基于密度的聚类方法&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;聚类可以一定程度上解决&lt;strong&gt;数据稀疏性问题&lt;/strong&gt;，但精准度往往不如协同过滤好。&lt;/p&gt;

&lt;h4 id=&#34;分类-回归&#34;&gt;分类/回归&lt;/h4&gt;

&lt;p&gt;它的基本思想是把&lt;r&gt;评分预测&lt;/r&gt;看作一个多分类(回归)问题。常用的分类器有LR,Navie Bayes，把item的特征向量作为模型的输入。&lt;/p&gt;

&lt;p&gt;比较通用，可以和其他方法组合，提高预测的准确性；需要大量训练数据，防止过拟合现象。&lt;/p&gt;

&lt;h4 id=&#34;svd&#34;&gt;SVD&lt;/h4&gt;

&lt;p&gt;它的原理是将(用户,物品)的评分矩阵R，分解成三个矩阵U,S,V。SVD本身要求矩阵必须是稠密矩阵，而评分矩阵非常稀疏；此外，SVD的计算复杂度很高。&lt;/p&gt;

&lt;p&gt;于是，就出现了关于SVD的一系列变形，如FunkSVD,BiasSVD,SVD++。具体可参见开源算法包&lt;r&gt;svdfeture&lt;/r&gt;。&lt;/p&gt;

&lt;h4 id=&#34;item的向量化&#34;&gt;Item的向量化&lt;/h4&gt;

&lt;p&gt;向量化的目的是将Item进行知识、概念层次上的表达。常用方法有pLSA,LDA,&lt;a href=&#34;http://cherishzhang.github.io/post/word2vec/&#34;&gt;Word2Vec&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;向量化时大多基于行为数据，&lt;r&gt;行为少的item向量化效果不好&lt;/r&gt;。&lt;/p&gt;

&lt;h4 id=&#34;玻尔兹曼机&#34;&gt;玻尔兹曼机&lt;/h4&gt;

&lt;p&gt;Boltzmann机：两层神经网络(无输出层)；对称、全连接；每个神经元有0/1(激活/未激活)两种状态，某个时刻的状态是随机的，由一定概率确定。&lt;/p&gt;

&lt;p&gt;受限Boltzmann机：受限，层内无连接。&lt;/p&gt;

&lt;p&gt;如何对(user,item)进行建模？一个用户一个RBM，共享隐层，权重W和偏置b。&lt;/p&gt;

&lt;h4 id=&#34;图模型&#34;&gt;图模型&lt;/h4&gt;

&lt;p&gt;多用于社交网络中人与人的关系挖掘。方法有SimRank,SimRank++,Markov模型。&lt;/p&gt;

&lt;p&gt;借助图的结构的传导性，可以发现协同过滤发现不了的弱相似性，给推荐带来一定的惊喜性。但该模型仍面临数据稀疏性和冷启动问题。&lt;/p&gt;

&lt;h2 id=&#34;基于-内容-content-based&#34;&gt;基于&lt;strong&gt;内容&lt;/strong&gt;(Content-based)&lt;/h2&gt;

&lt;p&gt;内容包括：&lt;r&gt;文本描述&lt;/r&gt;（通常用NLP技术挖掘关键词）、&lt;r&gt;Item属性&lt;/r&gt;(如电影的主题，职位的行业)、&lt;r&gt;Item特征&lt;/r&gt;(如语音信号表示、图像向量表示)&lt;/p&gt;

&lt;p&gt;基本组成：&lt;r&gt;item特征向量&lt;/r&gt;(如文本的TF-IDF向量)，&lt;r&gt;用户profile向量&lt;/r&gt;(根据用户偏好的items来提取)、&lt;r&gt;匹配分&lt;/r&gt;(cosine,分类/回归模型)&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;方面&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;优点&lt;/td&gt;
&lt;td&gt;能够推荐出用户独有的&lt;r&gt;小众偏好&lt;/r&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;可以一定程度解决&lt;r&gt;数据稀疏&lt;/r&gt;问题和&lt;r&gt;item冷启动&lt;/r&gt;问题&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;具有很好的&lt;r&gt;解释性&lt;/r&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;缺点&lt;/td&gt;
&lt;td&gt;如何提取出有意义的特征&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;很难将不同item的特征组合在一起(邻域思想)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;惊喜性&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;依赖用户profile的准确性&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;混合-方法-hybrid-approaches&#34;&gt;&lt;strong&gt;混合&lt;/strong&gt;方法(Hybrid Approaches)&lt;/h2&gt;

&lt;p&gt;通常比单个算法好，需要在不同算法之间、理论效果和实际可行性之间权衡。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;方法&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;weighted&lt;/td&gt;
&lt;td&gt;加权&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;switching&lt;/td&gt;
&lt;td&gt;切换，确定一个合理的跳进&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Mixed&lt;/td&gt;
&lt;td&gt;混合&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;feature combination&lt;/td&gt;
&lt;td&gt;特征组合，不同特征组合在一起&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;feature augmentation&lt;/td&gt;
&lt;td&gt;特征扩展，一个模型的输出作为另一个的特征&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;cascade&lt;/td&gt;
&lt;td&gt;级联，粗排 -&amp;gt; 精排&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;一些新进展&#34;&gt;一些新进展&lt;/h2&gt;

&lt;h3 id=&#34;learning-to-rank&#34;&gt;Learning to rank&lt;/h3&gt;

&lt;p&gt;将排序看做一个ML问题，评价准则为NDCG(normalized discounted cumulative gain)或MRR(mean reciprocal rank)。算法分为三类：Pointwise, Pairwise, Listwise。&lt;/p&gt;

&lt;h3 id=&#34;页面整体优化-page-optimization&#34;&gt;页面整体优化(Page Optimization)&lt;/h3&gt;

&lt;p&gt;关于用户注意力建模的一篇文章Modeling User Attention and Interaction on the Web。&lt;/p&gt;

&lt;h3 id=&#34;情景推荐-context-aware&#34;&gt;情景推荐(Context-aware)&lt;/h3&gt;

&lt;p&gt;针对不同推荐场景，情景也有所不同。看电影，是周末还是工作日？订餐，餐厅距离？网购，用户心情？母婴：孩子年龄？&lt;/p&gt;

&lt;p&gt;算法主要有&lt;r&gt;张量分解(Tensor Factorization)&lt;/r&gt;，&lt;r&gt;分解机(Factorization Machine)&lt;/r&gt;。其中后者的效果更好。&lt;/p&gt;

&lt;h3 id=&#34;深度学习&#34;&gt;深度学习&lt;/h3&gt;

&lt;p&gt;主要用到的有卷积神经网络(CNN)和循环神经网络(RNN)。&lt;/p&gt;

&lt;h2 id=&#34;参考链接&#34;&gt;参考链接&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://mp.weixin.qq.com/s?__biz=MzIzOTU0NTQ0MA==&amp;amp;mid=2247483811&amp;amp;idx=1&amp;amp;sn=fc3ee4ddfc4a8d6014a4cd90cdb5983c&#34;&gt;常用推荐算法&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://mp.weixin.qq.com/s?__biz=MzA5NzkxMzg1Nw==&amp;amp;mid=2653160714&amp;amp;idx=1&amp;amp;sn=7e53c70b3567f2277206dd5ba5347591&#34;&gt;百分点亿级个性化推荐系统的发展历程和实践架构&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spark之RDD</title>
      <link>http://cherishzhang.github.io/post/spark/sparkRDD/</link>
      <pubDate>Sat, 20 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/spark/sparkRDD/</guid>
      <description>

&lt;h3 id=&#34;rdd&#34;&gt;RDD&lt;/h3&gt;

&lt;p&gt;RDD，全称为Resilient Distributed Datasets（弹性分布式数据集）。简单接触了spark后，萌发了想要深入且透彻的了解RDD到底是什么。&lt;/p&gt;

&lt;p&gt;从英文解释来看，它应该是Dataset的集合，且是一个提供了许多操作接口的数据集合。和普通数据集的区别是：&lt;r&gt;实际数据分布存储在一批机器的内存或硬盘中&lt;/r&gt;。当然RDD还有容错和并行的功能。&lt;/p&gt;

&lt;p&gt;下面摘录了一段对RDD的解释：&lt;/p&gt;

&lt;p&gt;&lt;sg&gt;它是一个&lt;strong&gt;容错&lt;/strong&gt;且&lt;strong&gt;并行&lt;/strong&gt;的数据结构，可以让用户&lt;strong&gt;显式&lt;/strong&gt;地将数据存储到磁盘和内存中，并能控制数据的&lt;strong&gt;分区&lt;/strong&gt;。它只能基于在稳定物理存储中的数据集和其他已有的RDD上执行确定性操作(transformation)来创建。&lt;/sg&gt;&lt;/p&gt;

&lt;p&gt;所以RDD的两个关键点为：带操作接口的数据集，分布式。&lt;/p&gt;

&lt;p&gt;举个例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//读取文件创建RDD
val rdd = sc.textFile(&amp;quot;README.md&amp;quot;).cache()
val numAs = rdd.filter(line =&amp;gt; line.contains(&amp;quot;a&amp;quot;)).count()//包含a的行数
val numBs = rdd.filter(line =&amp;gt; line.contains(&amp;quot;b&amp;quot;)).count()//包含b的行数
println(&amp;quot;Lines with a: %s, Lines with b: %s&amp;quot;.format(numAs, numBs))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;由此可见，RDD&lt;r&gt;本质&lt;/r&gt;是一个&lt;r&gt;只读的分区记录集合&lt;/r&gt;。一个RDD包含多个分区，每个分区是一个dataset片段。&lt;/p&gt;

&lt;p&gt;前面提到了带操作接口的数据集，操作接口主要是RDD提供了对数据进行映射，归一等操作的API接口。这无疑方便了对分布式环境中的数据整合操作。操作分为两类：转换(transformations)和行动(actions)。前者根据原有的RDD创建一个新的RDD，后者则操作结果返回给driver。&lt;/p&gt;

&lt;p&gt;以前总是不理解操作分成两类的原因，直接每次操作后都返回结果不可以么？解释是这样的，首先考虑到有些操作(如filter)过后，返回结果的数据量较大，而有些操作(如count)过后，返回的结果却很简洁。spark设计时，将转换操作设置为lazy的，即不会立即计算结果，只有遇到action，才会处理并返回结果。这样会使spark更高效。&lt;/p&gt;

&lt;p&gt;转换类的操作有：map, filter, flatMap, mapPartitions, mapPartitionsWithIndex, sample, union, intersection, distinct, groupByKey, reduceByKey, aggregateByKey, sortByKey, join, cogroup, cartesian, pipe, coalesce, repartition。&lt;/p&gt;

&lt;p&gt;行动类的操作有：reduce, collect, count, first, take, takeSample, takeOrdered, saveAsTextFile, saveAsSequenceFile, saveAsObjectFile&lt;/p&gt;

&lt;p&gt;相关操作函数如下表：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val rdd1 = sc.parallelize(1 to 9, 3)
val rdd2 = sc.parallelize(list(&amp;quot;dog&amp;quot;,&amp;quot;tiger&amp;quot;,&amp;quot;lion&amp;quot;,&amp;quot;cat&amp;quot;,&amp;quot;parther&amp;quot;,&amp;quot;eagle&amp;quot;),2)
val rdd3 = sc.parallelize(List(1,2),(3,4),(3,6)))
&lt;/code&gt;&lt;/pre&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;函数名&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;输入与输出&lt;/th&gt;
&lt;th&gt;Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;map&lt;/td&gt;
&lt;td&gt;对RDD中的每个元素执行一个指定的函数来产生一个新的RDD，映射函数的参数为RDD中的每一个元素&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;一对一&lt;/td&gt;
&lt;td&gt;val a = rdd1.map(x =&amp;gt; x*2)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;mapValues&lt;/td&gt;
&lt;td&gt;对象是元素为kv对的RDD，key保持不变，对value进行映射&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;一对一&lt;/td&gt;
&lt;td&gt;val b = rdd2.map(x =&amp;gt; (x.length, x)) &lt;br\&gt; b.mapValues(&amp;ldquo;x&amp;rdquo; + _ + &amp;ldquo;x&amp;rdquo;).collect&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;flatMap&lt;/td&gt;
&lt;td&gt;扁平化map后的结果，多个集合合并为一个集合&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;一对一&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;mapPartitions&lt;/td&gt;
&lt;td&gt;映射函数的参数为RDD中&lt;strong&gt;每一个分区&lt;/strong&gt;的迭代器&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;一对一&lt;/td&gt;
&lt;td&gt;可用于为RDD中数据按分区创建连接&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;reduce&lt;/td&gt;
&lt;td&gt;将RDD中元素两两传递给输入函数，产生一个新值，新产生的值与RDD中的下一个元素再被传递给输入函数，&lt;strong&gt;直到最后只有一个值为止&lt;/strong&gt;。&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;多对一&lt;/td&gt;
&lt;td&gt;rdd1.reduce((x,y) =&amp;gt; x+y) //结果为45&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;reduceByKey&lt;/td&gt;
&lt;td&gt;将key相同的元素的value进行reduce，组成一个新的KV对&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;多对一&lt;/td&gt;
&lt;td&gt;rdd3.reduceByKey((x,y) =&amp;gt; x+y).collect //结果为Array((1,2),(3,10))&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;filter&lt;/td&gt;
&lt;td&gt;对每个元素应用f函数，只保留返回值为true的元素&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;子集型&lt;/td&gt;
&lt;td&gt;val d = rdd3.filter{ case(x,y) =&amp;gt; equal(x,y) }.count() //结果为0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;sample&lt;/td&gt;
&lt;td&gt;对元素采样，获取子集&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;子集型&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;cache&lt;/td&gt;
&lt;td&gt;将元素从磁盘缓存到内存&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;cache型&lt;/td&gt;
&lt;td&gt;rdd1.cache()&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;persist&lt;/td&gt;
&lt;td&gt;对RDD缓存，位置可指定&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;cache型&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;([1-9]\d+|[2-9])
通常来讲，针对数据处理有几种常见模型，包括：&lt;r&gt;Iterative Algorithms&lt;/r&gt;,&lt;r&gt;Relational Queries&lt;/r&gt;,&lt;r&gt;MapReduce&lt;/r&gt;,&lt;r&gt;Stream Processing&lt;/r&gt;。例如Hadoop MapReduce采用了MapReduces模型，Storm则采用了Stream Processing模型。RDD混合了这四种模型，使得Spark可以应用于各种大数据处理场景。&lt;/p&gt;

&lt;p&gt;Spark将依赖分为narrow与wide。如果RDD的每个分区最多只能被一个Child RDD的一个分区使用，则称之为narrow dependency；若多个Child RDD分区都可以依赖，则称之为wide dependency。不同的操作依据其特性，可能会产生不同的依赖。例如map操作会产生narrow dependency，而join操作则产生wide dependency。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/rdd.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上图说明了narrow和wide之间的区别。 narrow dependencies可以支持在同一个cluster node上以管道形式执行多条命令，而且它的失败恢复更有效，只需要重新计算丢失的parent partition即可；wide dependencies需要所有的父分区都是可用的，牵涉到RDD各级的多个Parent Partitions。&lt;/p&gt;

&lt;h3 id=&#34;共享变量&#34;&gt;共享变量&lt;/h3&gt;

&lt;p&gt;&lt;r&gt;broadcast&lt;/r&gt;变量：只读的共享变量 每个节点上都有一个拷贝。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val broadcastVar = sc.broadcast(&amp;quot;string test&amp;quot;) //broadcast variable is readonly
val v = broadcastVar.value
println(v)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;r&gt;accumulator&lt;/r&gt;变量：做累加器用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val accum = sc.accumulator(0, &amp;quot;My Accumulator&amp;quot;) //value and name
sc.parallelize(1 to 1000000).foreach(x =&amp;gt; accum+= 1)
println(accum.name + &amp;quot;:&amp;quot; + accum.value)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;sparkcontext&#34;&gt;SparkContext&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>[ml] word2vec模型和源码解析</title>
      <link>http://cherishzhang.github.io/post/ml/word2vec/</link>
      <pubDate>Fri, 05 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/ml/word2vec/</guid>
      <description>

&lt;h2 id=&#34;词向量&#34;&gt;词向量&lt;/h2&gt;

&lt;p&gt;一种简单直观的词表示方法是&lt;r&gt;One-hot&lt;/r&gt;编码，用N位对N个词编码，每个词对应的N维向量中，只有一维为0。这种方式的缺陷是&lt;r&gt;词汇鸿沟&lt;/r&gt;，即词与词之间相互孤立，忽略了它们之间的联系。而word2vec中的词向量是一种distributed representation，它的&lt;r&gt;特点&lt;/r&gt;是&lt;strong&gt;利用距离刻画词之间的相似性&lt;/strong&gt;。&lt;/p&gt;

&lt;h2 id=&#34;统计语言模型&#34;&gt;统计语言模型&lt;/h2&gt;

&lt;p&gt;统计语言模型是用来计算一个句子的概率的&lt;r&gt;概率模型&lt;/r&gt;，经典的有HAL,LSA,COALS等。通常它基于一个语料库来构建。&lt;/p&gt;

&lt;h3 id=&#34;句子的概率&#34;&gt;句子的概率&lt;/h3&gt;

&lt;p&gt;给定一个由T个词\(w_1,w_2,&amp;hellip;,w_T\)按顺序构成的句子，根据Bayes，计算该句子的概率p(W)&lt;/p&gt;

&lt;p&gt;$$p(W) = p(w_1^T) = p(w_1,w_2,&amp;hellip;,w_T) = p(w_1)*p(w_2|w_1)*p(w_3|(w_1^2))&amp;hellip;p(w_T|(w_1^{T-1}))$$&lt;/p&gt;

&lt;p&gt;其中等式中的各个因子（条件概率）就是语言模型的&lt;strong&gt;参数&lt;/strong&gt;，常用的语言模型都是在近似求p(W)。如果给定一个词典大小为N的语料库，考虑长度为T的任意句子，理论上有\(N^T\)种可能组合，而每个句子需要计算T个参数，总共需要计算&lt;r&gt;\(T*N^T\)&lt;/r&gt;个参数。无论是计算或保存，都需要很大开销。&lt;/p&gt;

&lt;h3 id=&#34;n-gram模型&#34;&gt;n-gram模型&lt;/h3&gt;

&lt;p&gt;针对参数繁多，该模型做了一个简化的假定，即一个词出现的概率&lt;strong&gt;只与&lt;/strong&gt;它前面的n-1个词相关。实际中最多采用&lt;r&gt;n=3&lt;/r&gt;的三元模型。&lt;/p&gt;

&lt;p&gt;$$(p(w_k|w^{k-1}_1) = p(w_k|w^{k-1}_{k-n+1}) = \frac{count(w_{k-n+1}^k)}{count(w_{k-n+1}^{k-1})}$$&lt;/p&gt;

&lt;p&gt;如果过程中出现\(count(w_{k-n+1}^k)\) = \(count(w_{k-n+1}^{k-1})\) 或者 \(count(w_{k-n+1}^{k-1}) = 0\)，则需要做&lt;r&gt;平滑处理&lt;/r&gt;。&lt;/p&gt;

&lt;p&gt;n-gram模型的缺陷是无法建模更远的关系，语料的不足使得无法训练更高阶的语言模型；而且无法建模出词之间的相似度。&lt;/p&gt;

&lt;h3 id=&#34;最大似然&#34;&gt;最大似然&lt;/h3&gt;

&lt;p&gt;对句子模型的另一种转化是利用机器学习，构造一个目标函数，并最大似然该函数。优化目标求得最优参数，利用最优参数预测未知句子的概率。&lt;strong&gt;&lt;r&gt;目标函数&lt;/r&gt;&lt;/strong&gt;一般为: $$\prod_{w \in C} p(w|Context(w))$$&lt;/p&gt;

&lt;p&gt;其中C表示&lt;r&gt;语料集&lt;/r&gt;，Context(w)表示词w的&lt;r&gt;上下文&lt;/r&gt;，即w周边词的集合。实际中采用最大对数似然，即\(\sum\limits_{w \in C} log p(w|Context(w))\)最大化。&lt;strong&gt;增加log是为了将乘法运算转换为加法运算(便于求导的常用方法)&lt;/strong&gt;。问题关键在于如何构造F函数。&lt;/p&gt;

&lt;p&gt;$$p(w|Context(w)) = F(w,Context(w),\theta)$$&lt;/p&gt;

&lt;h2 id=&#34;神经概率语言模型&#34;&gt;神经概率语言模型&lt;/h2&gt;

&lt;p&gt;Bengio(03) - 《A Neural Probabilistic Language Model》&lt;/p&gt;

&lt;p&gt;神经网络包含四层：&lt;r&gt;输入层&lt;/r&gt;，&lt;r&gt;投影层&lt;/r&gt;，&lt;r&gt;隐藏层&lt;/r&gt;和&lt;r&gt;输出层&lt;/r&gt;。W,U为权值矩阵，p,q为偏置向量。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/word2vec-neural.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;取&lt;r&gt;二元对(Context(w),w)&lt;/r&gt;作为一个训练样本，其中Context(w)取前面n-1个词，每个词\(v_i\)用一个m维向量表示。投影层是一个(n-1)*m长度的向量，输出层的规模为词库中词表的长度D。&lt;/p&gt;

&lt;p&gt;选用双曲正切函数作为隐藏层的激活函数，计算过程为：&lt;/p&gt;

&lt;p&gt;$$z_w = tanh(Wx_w + p), y_w = Uz_w + q$$&lt;/p&gt;

&lt;p&gt;输出为一个长度为D的向量\(y_w = (y_{w,1}, y_{w,2},&amp;hellip;,y_{w,D})^T\)，通过softmax归一化，每个分量表示上下文为Context(w)时下一词为词典第i个词的概率。&lt;/p&gt;

&lt;p&gt;&lt;r&gt;待确定的参数集\(\Theta\)&lt;/r&gt;包括：&lt;strong&gt;词向量和填充向量，神经网络参数W,U,p,q&lt;/strong&gt;。其中大部分&lt;strong&gt;计算集中&lt;/strong&gt;在&lt;r&gt;隐藏层和输出层之间的矩阵向量运算&lt;/r&gt;和&lt;r&gt;softmax归一化&lt;/r&gt;上。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;关于softmax归一化：
形式为：$$ P(y=i) = \frac{exp(\sum_d w_{id}x_{d})}{\sum_j exp(\sum_d w_{jd} x_{d})} $$
主要是针对多分类问题，归一化输出概率值。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;word2vec核心&#34;&gt;Word2vec核心&lt;/h2&gt;

&lt;p&gt;word2vec中用到了两个重要模型: &lt;r&gt;CBOW&lt;/r&gt;和&lt;r&gt;Skip-gram&lt;/r&gt;，对于这两个模型，分别都给出了基于&lt;r&gt;Hierarchical Softmax&lt;/r&gt;和&lt;r&gt;Negative Sampling&lt;/r&gt;两套框架的实现。而word2vec的高效具体体现在这两个策略上。&lt;/p&gt;

&lt;p&gt;CBOW和Skip-gram包括三层：&lt;r&gt;输入层&lt;/r&gt;，&lt;r&gt;投影层&lt;/r&gt;，&lt;r&gt;输出层&lt;/r&gt;。前者是已知词的上下文预测当前词，后者是通过当前词预测上下文。&lt;/p&gt;

&lt;h3 id=&#34;1-cbow&#34;&gt;1. CBOW&lt;/h3&gt;

&lt;p&gt;CBOW = Continuous Bag-of-Words, 目标函数为：&lt;/p&gt;

&lt;p&gt;$$\sum_{w \in c} log p(w|Context(w)) = \sum_{t=1}^T log p(w_t|\tau(w_{t-k},w_{t-k-1},&amp;hellip;,w_{t+k-1},w_{t+k}))$$&lt;/p&gt;

&lt;p&gt;其中T表示整个词典的大小，模型的目标是&lt;r&gt;最大化&lt;/r&gt;这个目标函数值。\(w_t\)表示词典中的一个词，即通过和\(w_t\)相邻的窗口大小为k的词来预测\(w_t\)出现的概率。其中\(\tau(w_1,w_2,&amp;hellip;,w_k)\)函数表示以\(w_i(i&amp;lt;=i&amp;lt;=k)\)为参数进行某种运算。在Word2vec里，这种运算是向量加和运算。即&lt;r&gt;把窗口相邻的所有词的向量加和&lt;/r&gt;。&lt;/p&gt;

&lt;h4 id=&#34;hierarchical-softmax&#34;&gt;Hierarchical Softmax&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/word2vec-hs-cbow.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;相比于之前的神经概率语言模型，主要有&lt;r&gt;三处改动&lt;/r&gt;：&lt;strong&gt;去掉隐藏层；首尾拼接改为累加求和；输出为树结构。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;问题1&lt;/strong&gt;：huffman树的应用原理，及hierarchical softmax到底指什么？&lt;/p&gt;

&lt;p&gt;hs可以保证叶节点输出的概率值是归一化的。将二叉树中的每个中间结点看作一个隐含的二分类器，一个词的huffman编码就是多个隐含二分类器作用的结果。最后输出层得到的是在当前上下文（输入层）的前提下，各个词的概率，softmax是指这些概率之和为1，即归一化。证明如下：当树的高度为2，即根节点连接两个叶子节点，设左叶子节点的概率为p，则右叶子节点概率为1-p。它们的概率和为1；当树的高度为k时，从根到叶子节点的一个路径可以看成k个独立事件的一种发生可能，所有的叶子节点刚好包括了全部可能情况，概率总和为1。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;问题2&lt;/strong&gt;：简单描述一下模型流程？&lt;/p&gt;

&lt;p&gt;根据上图，首先从训练集中选取一个sentence，对于其中的一个词w，计算其上下文词向量的加和，得到\(X_w\)向量(长度为词向量的维度)，即投影层。对于huffman树中的每个非叶子节点，添加一个辅助向量\(\theta\)，\(X_w\)和\(\theta\)层层作用后，可以得到每个叶子节点的概率。计算输出概率与真实值的残差，然后用随机梯度优化参数，使残差尽可能小。在模型训练过程中，可以得到最优参数下的词向量集合。这些词向量就是word2vec。&lt;/p&gt;

&lt;p&gt;================================================================&lt;/p&gt;

&lt;p&gt;关于模型的&lt;r&gt;推导&lt;/r&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;结点的带权路径长度&lt;/strong&gt;: 从根结点到该结点的路径长度乘以该结点的权值&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;树的带权路径长度&lt;/strong&gt;: 所有叶结点的带权路径长度之和&lt;/p&gt;

&lt;p&gt;Huffman树是在给定n个叶子结点和其对应权值的前提下，构造一棵二叉树，使该树的带权路径长度最小。已知语料库中各词的词频，可以构建一棵二叉树，每个词作为树的叶子节点，并保证词频越大的词离根节点越近。每个叶子对应的&lt;r&gt;huffman编码&lt;/r&gt;就是该词的编码。约定左子结点编码为0，右子结点编码为1。&lt;/p&gt;

&lt;p&gt;此外为每个非叶子结点设置一个&lt;r&gt;辅助向量&lt;/r&gt;\(\theta\)。&lt;/p&gt;

&lt;p&gt;对于词w，从根结点到该叶结点的路径中，假设有\(l^w\)层，经历了\(l^w-1\)次二分类，则条件概率\( p(w|Context(w)) = \prod\limits_{j=2}^{l^w} p(d_j^w|X_w,\theta_{j-1}^w) \), \(d_j^w\)表示第j个结点对应的编码，\(\theta_j^w\)表示第j个结点对应的辅助向量。&lt;/p&gt;

&lt;p&gt;第一步，&lt;strong&gt;目标函数对数化&lt;/strong&gt;:
$$F = \sum_{w \in C} log p(w|Context(w)) = \sum_{w \in C} log \prod\limits_{j=2}^{l^w} \sigma(\theta_{j-1}^w X_w)^{1-{d_j^w}} (1-\sigma(\theta_{j-1}^w X_w))^{d_j^w} $$&lt;/p&gt;

&lt;p&gt;第二步，&lt;strong&gt;取内部项F(w,u)&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;$$ F(w,u) =  (1-{d_j^w}) log(\sigma(\theta_{j-1}^w)) + {d_j^w} log(1-\sigma(\theta_{j-1}^w X_w))$$&lt;/p&gt;

&lt;p&gt;第三步，&lt;strong&gt;随机梯度上升&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;\(\theta_{j-1}^w\)的更新公式为：&lt;/p&gt;

&lt;p&gt;$$ \frac{\partial F(w,u)}{\partial \theta_{j-1}^w} = (1 - d_j^w - \sigma(X_w^T*\theta_{j-1}^w)) X_w$$&lt;/p&gt;

&lt;p&gt;$$ \theta_{j-1}^w := \theta_{j-1}^w + \eta (1 - d_j^w - \sigma(X_w^T*\theta_{j-1}^w)) X_w $$&lt;/p&gt;

&lt;p&gt;\(X_w\)和\(\theta_{j-1}^w\)具有对称性，更新公式类似，这里注意更新的\(v(\tilde{w})\)而非加和后的\(X_w\)：&lt;/p&gt;

&lt;p&gt;$$v(\tilde{w}) := v(\tilde{w}) +  \eta \sum\limits_{j=2}^{l^w}(1 - d_j^w - \sigma(X_w^T*\theta_{j-1}^w)) \theta_{j-1}^w, \tilde{w} \in Context(w)$$&lt;/p&gt;

&lt;p&gt;================================================================&lt;/p&gt;

&lt;h4 id=&#34;negative-sampling&#34;&gt;Negative Sampling&lt;/h4&gt;

&lt;p&gt;huffman树中从根节点到叶子节点路径上的各个中间结点，分为正负两类，构造好huffman树后，每个词w对应的采样样本（中间结点）也就确定。训练的目标是最大化似然函数值。由此衍生出，用简单的&lt;r&gt;随机负采样&lt;/r&gt;代替复杂的huffman树。已知词w的上下文，预测词w，可将w看作正样本，负样本的抽样策略为&lt;r&gt;带权采样&lt;/r&gt;，即被选中的概率与词频成正比。模型训练的&lt;r&gt;目标&lt;/r&gt;是&lt;strong&gt;最大化正样本的概率同时最小化负样本的概率&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;设NEG(w)是关于词w的一个负样本集，对已一个给定的&lt;r&gt;二元对&lt;/r&gt;(Context(w),w),&lt;r&gt;优化目标&lt;/r&gt;是&lt;/p&gt;

&lt;p&gt;$$ g(w) = \prod\limits_{u \in w \cup NEG(w)} p(u|Context(w))$$&lt;/p&gt;

&lt;p&gt;=================================================================&lt;/p&gt;

&lt;p&gt;关于模型的&lt;r&gt;推导&lt;/r&gt;:&lt;/p&gt;

&lt;p&gt;为每个词设置一个&lt;r&gt;辅助向量\(\theta^u\)&lt;/r&gt;，\(X_w^T\)表示为Context(w)中的k个词进行加和后的向量，\(L^w(u)\)表示词u的样本标签，正样本取1，反之取0。&lt;/p&gt;

&lt;p&gt;第一步，&lt;strong&gt;转换目标函数&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;$$g(w) = \prod\limits_{u \in w \cup NEG(w)} [\sigma(\theta^u X_w^T)]^{L^w(u)} [1 - \sigma(\theta^u X_w^T)] ^ {(1-L^w(u))}$$&lt;/p&gt;

&lt;p&gt;因为正样本只有一个，故可简化为\(g(w) =\sigma(\theta^u X_w^T) \prod\limits_{u \cup NEG(w)} [1 - \sigma(\theta^u X_w^T)] ^ {1-L^w(u)}\)&lt;/p&gt;

&lt;p&gt;第二步，&lt;strong&gt;考虑整个语料库&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;$$F = log G = log \prod\limits_{w \in C} g(w)$$
$$ = \sum\limits_{w \in C} \sum\limits_{u \in w \cup NEG(w)} {L^w(u)}log[\sigma(\theta^u X_w^T)] + (1-L^w(u))log[1 - \sigma(\theta^u X_w^T)]$$
$$ = \sum\limits_{w \in C} \bigg (log(\sigma(\theta^u X_w^T)) + \sum\limits_{u \in NEG(w)} log(\sigma(-\theta^u X_w^T))\bigg)$$&lt;/p&gt;

&lt;p&gt;第三步，&lt;strong&gt;随机梯度上升&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;$$ F(w,u) = L^w(u) log[\sigma(\theta^u X_w^T)] + (1-L^w(u)) log[1 - \sigma(\theta^u X_w^T)] $$&lt;/p&gt;

&lt;p&gt;\(\theta^u\)的&lt;r&gt;更新公式&lt;/r&gt;：&lt;/p&gt;

&lt;p&gt;$$\frac{\partial F(w,u)}{\partial \theta^u}  = L^w(u) (1 -\sigma(\theta^u X_w^T)) X_w - (1-L^w(u)) \sigma(\theta^u X_w^T) X_w $$
$$  =  L^w(u) X_w - \sigma(\theta^u X_w^T) X_w $$&lt;/p&gt;

&lt;p&gt;$$\theta^u := \theta^u + \eta (L^w(u) - \sigma(\theta^u X_w^T)) X_w$$&lt;/p&gt;

&lt;p&gt;根据\(\theta^u\)和\(X_w\)的&lt;r&gt;对称性&lt;/r&gt;，w上下文中的每个词向量的&lt;r&gt;更新公式&lt;/r&gt;为：&lt;/p&gt;

&lt;h1 id=&#34;v-tilde-w-v-tilde-w-sum-limits-u-in-w-cup-neg-w-eta-l-w-u-sigma-theta-u-x-w-t-theta-u-tilde-w-in-context-w&#34;&gt;$$v(\tilde{w}) := v(\tilde{w}) + \sum\limits_{u \in w \cup NEG(w)} \eta (L^w(u) - \sigma(\theta^u X_w^T))\theta^u, \tilde{w} \in Context(w)$$&lt;/h1&gt;

&lt;h3 id=&#34;2-skip-gram&#34;&gt;2. SKIP-GRAM&lt;/h3&gt;

&lt;h4 id=&#34;hierarchical-softmax-1&#34;&gt;Hierarchical Softmax&lt;/h4&gt;

&lt;p&gt;与CBOW不同是，输入层只有一个词向量w。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/word2vec-hs-sg.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;关键在于\(p(Context(w)|w)\)的定义：&lt;/p&gt;

&lt;p&gt;$$ p(Context(w)|w) = \prod\limits_{u \in Context(w)} p(u|w) $$，其他推导类似CBOW。&lt;/p&gt;

&lt;h4 id=&#34;negative-sampling-1&#34;&gt;Negative Sampling&lt;/h4&gt;

&lt;p&gt;已知词w，预测词w的上下文，则针对上下文中的每个词，将该词看作正样本，选取一定数量的负样本。目标函数为：&lt;/p&gt;

&lt;p&gt;$$F = log G = log \prod\limits_{w \in C} g(w)$$
$$ = \sum\limits_{w \in C} \sum\limits_{u \in Context(w)} \sum\limits_{z \in u \cup NEG(u)} {L^u(z)}log[\sigma(\theta^z X_w^T)] + (1-L^u(z))log[1 - \sigma(\theta^z X_w^T)]$$&lt;/p&gt;

&lt;p&gt;公式中对于一个样本(w, Context(w)), 对Context(w)中的每个词，都做了一次负采样。而word2vec源码对此作了改动，只对词w进行了|Context(w)|次负采样。&lt;/p&gt;

&lt;h2 id=&#34;c源码阅读&#34;&gt;c源码阅读&lt;/h2&gt;

&lt;h3 id=&#34;命令行参数&#34;&gt;命令行参数&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;参数&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;size&lt;/td&gt;
&lt;td&gt;Projection Layer的长度，即词向量的长度&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;train&lt;/td&gt;
&lt;td&gt;语料库地址，即训练集&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;save-vocab&lt;/td&gt;
&lt;td&gt;词典保存地址&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;read-vocab&lt;/td&gt;
&lt;td&gt;词典读取地址，如果没指定，从语料库中学习词典&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;debug&lt;/td&gt;
&lt;td&gt;debug_mode，debug信息打印控制&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;binary&lt;/td&gt;
&lt;td&gt;是否保存为二进制&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;cbow&lt;/td&gt;
&lt;td&gt;确定是cbow，还是skip-gram模型&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;alpha&lt;/td&gt;
&lt;td&gt;学习率&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;output&lt;/td&gt;
&lt;td&gt;模型保存地址&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;window&lt;/td&gt;
&lt;td&gt;窗口大小，默认值5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;sample&lt;/td&gt;
&lt;td&gt;采样百分比&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;hs&lt;/td&gt;
&lt;td&gt;是否进行Hierarchical Softmax&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;negative&lt;/td&gt;
&lt;td&gt;是否进行负采样&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;threads&lt;/td&gt;
&lt;td&gt;线程数&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;min-count&lt;/td&gt;
&lt;td&gt;有效词频的最小值，默认值5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;classes&lt;/td&gt;
&lt;td&gt;聚类个数&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;数据结构&#34;&gt;数据结构&lt;/h3&gt;

&lt;p&gt;每个词用结构体&lt;r&gt;vocab_word&lt;/r&gt;保存信息:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct vocab_word{
    char *word; //词本身
    long long cn; //语料集中的词频
    int *point; //huffman树中，从根节点到该词所在叶节点的中间结点列表
    char *code, codelen; //该词对应的huffman编码和编码长度
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;整个词表用一个数组vocab表示，为了&lt;strong&gt;快速查找&lt;/strong&gt;给定词的信息，建立了一个&lt;r&gt;hash表&lt;/r&gt;，存放的元素是该词在vocab中的下标。&lt;/p&gt;

&lt;p&gt;哈希的策略是&lt;strong&gt;线性探测的开放定址法&lt;/strong&gt;，计算词w的哈希值hv(w)作为下标索引，若该位置被占用，则顺序往下找，直到找到一个未被占用的位置。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vocab = (struct vocab_word *)calloc(vocab_max_size, sizeof(struct vocab_word));
vocab_hash = (int *)calloc(vocab_hash_size, sizeof(int));
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;模型实现&#34;&gt;模型实现&lt;/h3&gt;

&lt;h4 id=&#34;1-关于net&#34;&gt;1. 关于Net&lt;/h4&gt;

&lt;p&gt;对于hs来说，参数主要有&lt;r&gt;词向量&lt;/r&gt;(syn0)，&lt;r&gt;huffman树中间结点的辅助向量&lt;/r&gt;(syn1)；对于skip-gram来说，参数主要有&lt;r&gt;词向量&lt;/r&gt;(syn0),&lt;r&gt;负采样中每个词的辅助向量&lt;/r&gt;(syn1neg)。&lt;/p&gt;

&lt;p&gt;关于初始化，syn0采用&lt;strong&gt;零初始化&lt;/strong&gt;，syn1,syn1neg采用&lt;strong&gt;随机初始化&lt;/strong&gt;([\(-\frac{-0.5}{size}, \frac{0.5}{size}\)])，其中size为词向量的长度。&lt;/p&gt;

&lt;p&gt;代码中Hierarchical Softmax和Negative Sampling可以同时训练。neu1代表\(X_w\),即输入词向量加和后的向量；neu1e代表\(e\)，即huffman树每一条路径（从根到叶子节点）梯度更新的求和，用来更新相应的词向量。&lt;/p&gt;

&lt;h4 id=&#34;2-sigmoid函数值计算&#34;&gt;2. sigmoid函数值计算&lt;/h4&gt;

&lt;p&gt;建模过程中用到&lt;r&gt;逻辑回归&lt;/r&gt;做二分类问题，需要计算sigmod函数值。程序中采用的&lt;r&gt;近似计算&lt;/r&gt;的方法，将区间&lt;r&gt;(-6,-6)&lt;/r&gt;等分成若干份(1000)，事先将每个结点处的函数值保存起来，对于某个x，判断x的范围，如果小于-6，返回0；如果大于6；返回1；否则对x取整查表，返回对应值。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;sigmoid函数中涉及指数运算，指数运算利用幂级数展开式实现，如果展开项数过多，则计算比较耗时。&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&#34;3-词频和窗口&#34;&gt;3. 词频和窗口&lt;/h4&gt;

&lt;p&gt;删除小于阈值(min_count)的低频词，对高频词采用&lt;r&gt;subsampling&lt;/r&gt;，可以提高训练速度和词向量精度。具体做法：给定一个阈值t，对于词w，以\(prob(w) = 1-(\sqrt{(\frac{t}{f(w)})} + \frac{t}{f(w)})\)的概率舍弃。&lt;/p&gt;

&lt;p&gt;模型训练以行为单位进行，对于一个给定行，包含T个词，每个词为一个训练样本，对于某个词w，定义context(w)为：设置一个窗口阈值参数window(5)，每次构造context(w)时，生成一个随机数\(c \in [1, window]\)，w前后各取c个词构成context。&lt;/p&gt;

&lt;h4 id=&#34;4-创建huffman树&#34;&gt;4. 创建huffman树&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/huffman.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;简单来说，用一个vocab_size*2+1大小的数组存储树的&lt;r&gt;全部节点&lt;/r&gt;，其中前vocab_size个元素为全部的&lt;r&gt;叶子节点&lt;/r&gt;且按词频降序排列；后面的元素表示&lt;r&gt;中间结点&lt;/r&gt;，且初始化词频为1e15。用两个变量&lt;r&gt;pos1&lt;/r&gt;,&lt;r&gt;pos2&lt;/r&gt;记录构造树的情况，其中pos1指向vocab_size-1，pos2指向vocab_size。每次从pos1,pos2选取词频较小的那个结点，并相应的向前或向后移动，更新pos。选择两次，合并两个选中的结点为一棵新树。&lt;/p&gt;

&lt;p&gt;在构建树的过程中，不断更新中间结点的词频，并保存各个节点的编码(0/1)，各个节点的父节点(层次信息)。从叶子节点(词)回溯，得出该词在树中的&lt;r&gt;路径&lt;/r&gt;和&lt;r&gt;huffman编码&lt;/r&gt;。&lt;/p&gt;

&lt;p&gt;详见&lt;r&gt;CreateBinaryTree&lt;/r&gt;函数。&lt;/p&gt;

&lt;h4 id=&#34;5-负采样&#34;&gt;5. 负采样&lt;/h4&gt;

&lt;p&gt;假设词典中的每个词w对应一个线段l(w),其长度为：&lt;/p&gt;

&lt;p&gt;$$len(w) = \frac{[counter(w)]^{0.75}}{\sum\limits_{u \in D} [counter(u)]^{0.75}} $$&lt;/p&gt;

&lt;p&gt;将这些线段首尾相接拼接在一起，形成一个长度为1的线段。随机往这个线段上打点，则高频词被打中的概率大。将这个单位线段等距离分割成M份(M远大于N,\(M=10^8\))，每次生成一个[0,M-1]的随机数r，r对应的词即为采样。有点类似k-means++的做法。&lt;/p&gt;

&lt;p&gt;详见&lt;r&gt;InitUnigramTable&lt;/r&gt;函数。&lt;/p&gt;

&lt;h4 id=&#34;6-学习率-eta&#34;&gt;6. 学习率\(\eta\)&lt;/h4&gt;

&lt;p&gt;学习率采用&lt;r&gt;自适应&lt;/r&gt;的方法，具体为：预先设置一个初始学习率(0.025)，每处理完固定数目(10000)个词，对学习率进行一次调整。&lt;/p&gt;

&lt;p&gt;$$\eta = \eta_{0}(1-\frac{word\_count\_actual}{train\_words+1})$$&lt;/p&gt;

&lt;p&gt;随着训练的进行\(\eta\)会逐渐变小，当小于阈值\(\eta_{min}\)时，\(\eta = \eta_{min}\)&lt;/p&gt;

&lt;h4 id=&#34;7-多线程并行&#34;&gt;7. 多线程并行&lt;/h4&gt;

&lt;p&gt;整个工作流程是：首先读取语料库中词表并统计词频，做一些模型初始化工作，然后多线程训练word2vec，判断classes，如果为0，则保存词向量；反之，对得到的词向量进行K-means聚类，聚类个数为classes，并保存聚类结果。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pthread_t *pt = (pthread_t *)malloc(num_threads * sizeof(pthread_t));
for (a = 0; a &amp;lt; num_threads; a++)
    pthread_create(&amp;amp;pt[a], NULL, TrainModelThread, (void *)a);
for (a = 0; a &amp;lt; num_threads; a++)
    pthread_join(pt[a], NULL);
//平衡划分语料文件
fseek(fi, file_size / (long long)num_threads * (long long)id, SEEK_SET);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并行的&lt;r&gt;关键&lt;/r&gt;在于&lt;strong&gt;如何分割好并行的任务和如何达成任务之间的良好通信&lt;/strong&gt;。word2vec根据&lt;r&gt;线程数将语料划分成若干份&lt;/r&gt;，每个线程负责语料库的一部分，但训练的神经网络、词向量和哈夫曼树是共享的。首先根据文件指针读取一个句子，用一个数组保存下句子中的每个词；然后根据命令行参数，选择训练CBOW或Skip-gram，其中每个模型都可以采用hs或negative的框架训练。&lt;/p&gt;

&lt;h2 id=&#34;spark-mllib-1-6-实现&#34;&gt;Spark MLlib 1.6 实现&lt;/h2&gt;

&lt;h3 id=&#34;可调参数&#34;&gt;可调参数&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;参数&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;vectorSize&lt;/td&gt;
&lt;td&gt;词向量的长度&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;learningRate&lt;/td&gt;
&lt;td&gt;学习率&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;numPartitions&lt;/td&gt;
&lt;td&gt;分区数，默认为1，越小准确率越高&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;numIterations&lt;/td&gt;
&lt;td&gt;迭代次数，默认为1，不大于numPartitions&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;seed&lt;/td&gt;
&lt;td&gt;随机数种子&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;minCount&lt;/td&gt;
&lt;td&gt;有效词频的最小值，默认值5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;window&lt;/td&gt;
&lt;td&gt;上下文的窗口大小，默认值5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;主要实现了基于hierarchical softmax框架下的skip-gram模型。&lt;/p&gt;

&lt;h3 id=&#34;广播&#34;&gt;广播&lt;/h3&gt;

&lt;p&gt;对于sigmod函数值表格，词信息数组，词的hash表三个变量，通过广播机制，变成全局变量。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val sc = dataset.context //dataset: RDD[Iterable[String]]
val expTable = sc.broadcast(createExpTable)
val bcVocab = sc.broadcast(vocab)
val bcVocabHash = sc.broadcast(vocabHash)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;分布式&#34;&gt;分布式&lt;/h3&gt;

&lt;p&gt;根据语料集文件，读取文件中的全部句子，将句子按分区数分发。&lt;/p&gt;

&lt;h2 id=&#34;参考链接&#34;&gt;参考链接&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.cnblogs.com/peghoty/p/3857839.html&#34;&gt;word2vec 中的数学原理详解&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1301.3781.pdf&#34;&gt;论文：Efficient Estimation of Word Representations in Vector Space&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&#34;&gt;论文：Distributed Representations of Words and Phrases and their Compositionality&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[ml] Decision Tree模型</title>
      <link>http://cherishzhang.github.io/post/ml/decision_tree/</link>
      <pubDate>Mon, 01 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/ml/decision_tree/</guid>
      <description>

&lt;h2 id=&#34;1-intro&#34;&gt;1.Intro&lt;/h2&gt;

&lt;p&gt;决策树常用的算法有&lt;r&gt;ID3&lt;/r&gt;,&lt;r&gt;C4.5&lt;/r&gt;和&lt;r&gt;CART&lt;/r&gt;，学习过程分为三部分：&lt;r&gt;特征选择&lt;/r&gt;，&lt;r&gt;决策树生成&lt;/r&gt;和&lt;r&gt;决策树剪枝&lt;/r&gt;。&lt;/p&gt;

&lt;p&gt;决策树的&lt;r&gt;生成&lt;/r&gt;只考虑局部最优（&lt;r&gt;贪心算法&lt;/r&gt;），而决策树的&lt;r&gt;剪枝&lt;/r&gt;则考虑&lt;r&gt;全局最优&lt;/r&gt;。简而言之，这是&lt;strong&gt;一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对每个子数据集有一个最好的分类过程&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;They are easy to &lt;r&gt;interpret&lt;/r&gt;, &lt;r&gt;handle categorical features&lt;/r&gt;, extend to the &lt;r&gt;multiclass classification setting&lt;/r&gt;, do not require &lt;r&gt;feature scaling&lt;/r&gt;, and are able to &lt;r&gt;capture non-linearities and feature interactions&lt;/r&gt;.&lt;/p&gt;

&lt;h2 id=&#34;2-准则函数&#34;&gt;2.准则函数&lt;/h2&gt;

&lt;p&gt;主要用来衡量节点数据集合的有序性，有&lt;r&gt;熵(Entropy)&lt;/r&gt;，&lt;r&gt;基尼指数(Gini)&lt;/r&gt;，&lt;r&gt;方差(Variance)&lt;/r&gt;，其中前两种针对分类问题，而方差针对回归问题。寻找最好的分割点是通过量化&lt;strong&gt;分割后&lt;/strong&gt;类的纯度来确定的，计算方式总结如下表：&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Impurity&lt;/th&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;formula&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;熵&lt;/td&gt;
&lt;td&gt;分类&lt;/td&gt;
&lt;td&gt;假设有K类，样本点属于第i类的概率为\(p_i\)，度量数据的不确定性&lt;/td&gt;
&lt;td&gt;\(E(D) =\sum_{i=1}^{K} -p_ilogp_i\)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;基尼&lt;/td&gt;
&lt;td&gt;分类&lt;/td&gt;
&lt;td&gt;同上&lt;/td&gt;
&lt;td&gt;\(Gini(D) = \sum_{i=1}^{K} p_i*(1-p_i)\)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;方差&lt;/td&gt;
&lt;td&gt;回归&lt;/td&gt;
&lt;td&gt;度量数据的离散程度&lt;/td&gt;
&lt;td&gt;\(Variance(D) = \frac{1}{N} \sum_{i=1}^N (y_i-\mu)^2\),\(\mu = \frac{1}{N} \sum_{i=1}^N y_i\)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;信息增益&lt;/strong&gt;: 得知某特征X的信息而使目标Y的信息&lt;r&gt;不确定减少&lt;/r&gt;的程度。信息增益大的特征有更强的分类能力。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;具体方法&lt;/strong&gt;：对训练集D，计算每个特征的信息增益，比较大小，选择信息增益大的特征列来分割集合。假设分割点将数据集分成左右两部分，则信息增益为 \( IG(D,s) = Impurity(D) - \frac{N_{left}}{N} Impurity(D_{left}) - \frac{N_{right}}{N} Impurity(D_{right})\)&lt;/p&gt;

&lt;p&gt;ID3s算法选择信息增益作分类，为了校正信息增益存在“偏向选择取值较多的特征”的缺点，C4.5采用&lt;r&gt;信息增益比&lt;/r&gt;\(IG_R(D,s) = \frac{IG(D,s)}{E_s(D)}\), \(E_s(D)= \sum_{i=1}^K -\frac{|D_i|}{|D|}*log_2(\frac{|D_i|}{|D|})\), 其中K是特征s的取值个数。&lt;/p&gt;

&lt;h2 id=&#34;3-生成算法&#34;&gt;3 生成算法&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;具体方法&lt;/strong&gt;：从根节点开始，计算所有可能的特征的准则值(应用准则函数)，选择最优的特征作为节点的特征，由该特征的不同取值建立子节点；再对子节点递归调用以上方法，构建决策树。&lt;/p&gt;

&lt;h4 id=&#34;cart&#34;&gt;CART&lt;/h4&gt;

&lt;p&gt;CART(classification and regression tree)，又称分类与回归树。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CART的生成是一个递归地构建&lt;r&gt;二叉树&lt;/r&gt;的过程&lt;/strong&gt;，对回归树应用平方误差最小化准则，对分类树用Gini指数最小化准则，进行特征选择，生成二叉树。预测时，先根据样本特征判断该样本点位于哪块区域，然后用该区域内的训练样本点集的&lt;r&gt;众数&lt;/r&gt; (分类) 或&lt;r&gt;均值&lt;/r&gt; (回归) 作为该样本点的预测值。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;算法描述&lt;/strong&gt;：CART树生成算法&lt;br/&gt;
&lt;strong&gt;输入&lt;/strong&gt;: 训练数据集D，停止条件&lt;br/&gt;
&lt;strong&gt;输出&lt;/strong&gt;: 决策树f(x)&lt;br/&gt;
&lt;strong&gt;(1)&lt;/strong&gt;在平方误差最小或Gini最小的准则下选择&lt;r&gt;最优切分变量j与切分点s&lt;/r&gt;，切分变量从可用特征集中选择。
回归树中，单元\(R_m\)下的最优值是\(R_m\)上所有输入实例对应的输出\(y_i\)的均值，即切分点。分类中，切分点是该变量所有可能的取值。&lt;br/&gt;
&lt;strong&gt;(2)&lt;/strong&gt;用选定的对(j,s)划分区域并决定相应的输出值&lt;br/&gt;
&lt;strong&gt;(3)&lt;/strong&gt;继续对子区域调用(1),(2),直至满足停止条件&lt;br/&gt;
&lt;strong&gt;(4)&lt;/strong&gt;将输入空间划分为M个区域\(R_1\),\(R_2\),&amp;hellip;,\(R_M\).生成如下决策树：&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;$$f(x) = \sum_{m=1}^{M} c_m*I(x \in R_m)$$&lt;/p&gt;

&lt;p&gt;算法的&lt;r&gt;停止条件&lt;/r&gt;是节点中的样本个数小于预定阈值，或样本集的基尼指数小于预订阈值（样本基本属于同一类），或没有更多特征。&lt;/p&gt;

&lt;h2 id=&#34;4-剪枝过程-pruning&#34;&gt;4 剪枝过程(pruning)&lt;/h2&gt;

&lt;p&gt;将已生成的树进行简化的过程，通过极小化决策树整体的损失函数（loss function）来实现。 分为前向剪枝和后向剪枝。前向剪枝是在构造决策树的同时对树进行剪枝；后向剪枝是在决策树构建完成后，对树从根节点向上递归剪枝。&lt;/p&gt;

&lt;h2 id=&#34;5-mllib实现&#34;&gt;5.MLlib实现&lt;/h2&gt;

&lt;h4 id=&#34;连续值特征&#34;&gt;连续值特征&lt;/h4&gt;

&lt;p&gt;对于连续值类型的特征，单台机器上的做法是选取所有出现过的值作为切分点候选集。加速计算的方法是对该集合按值大小排序，用排序好的数组做候选集。&lt;/p&gt;

&lt;p&gt;在分布式中，对于规模大的数据集，排序是比较耗时的。于是先&lt;r&gt;对整体数据按比例采样&lt;/r&gt;，然后&lt;r&gt;采用分位数(等频分箱)作为分割点的近似候选集&lt;/r&gt;(&lt;gr&gt;&lt;strong&gt;优化点&lt;/strong&gt;&lt;/gr&gt;)。&lt;/p&gt;

&lt;p&gt;&lt;r&gt;采样策略&lt;/r&gt;为选取样本集中的&lt;strong&gt;&lt;r&gt;max&lt;/r&gt;(maxBins*maxBins, 10000)&lt;/strong&gt;条记录。&lt;r&gt;等频分箱策略&lt;/r&gt;有三种(&lt;strong&gt;Sort&lt;/strong&gt;,MinMax,ApproxHist)，目前只支持第一种。规定分箱总数不能超过训练样本的实例数(maxBins的默认值为32)。&lt;/p&gt;

&lt;h4 id=&#34;类别值特征&#34;&gt;类别值特征&lt;/h4&gt;

&lt;p&gt;对于离散值类型的特征，有m个值，那么最多产生\(2^{m-1}-1\)种划分可能，\(2*(2^{m-1}-1)\)个分箱。对于二分类或回归问题，可以将划分候选缩减到m-1种可能。&lt;/p&gt;

&lt;p&gt;举个例子，在二分类问题中，一个特征有A,B,C三个值。特征值为A的样本集中label是1的比例为0.2，B和C分别为0.6，0.4。则产生了一个类别值的有序序列ACB，共有两种划分可能：A|C,B，A,C|B。&lt;/p&gt;

&lt;p&gt;多分类问题中，离散特征会产生\(2^{m-1}-1\)种划分可能，当划分数大于maxBins，将m个类别值根据impurity排序，产生共m-1种划分可能。&lt;/p&gt;

&lt;h4 id=&#34;bin和split&#34;&gt;bin和split&lt;/h4&gt;

&lt;p&gt;split是划分点，bin是划分区间。每个特征都分别有一组bin和split。&lt;/p&gt;

&lt;p&gt;其中bin中&lt;r&gt;预先计算样本的统计信息&lt;/r&gt;，节省计算开销(&lt;gr&gt;&lt;strong&gt;优化点&lt;/strong&gt;&lt;/gr&gt;)。&lt;strong&gt;分类问题&lt;/strong&gt;，bin中保存训练样本个数 (count) 以及各 label 数目 (count for label)；&lt;strong&gt;回归问题&lt;/strong&gt;，bin中保存训练样本个数 (count)、 label 之和 (sum) 以及 label 的平方和 (squared sum)。&lt;/p&gt;

&lt;p&gt;特征选择时，对于某个特征，计算不同切分变量下信息增益的大小，确定出该特征下的最佳切分点。选择全部候选特征集中最优的切分点作为当前决策。特征选择是树的level级别的并行，对于同一层次的节点，查找可以并行，查找操作的时间复杂度为O(L),L为树的层数(&lt;gr&gt;&lt;strong&gt;优化点&lt;/strong&gt;&lt;/gr&gt;)。&lt;/p&gt;

&lt;h4 id=&#34;终止条件&#34;&gt;终止条件&lt;/h4&gt;

&lt;p&gt;为了防止过拟合，采用前向剪枝。当任一以下情况发生，节点就终止划分，形成叶子节点：
(1)树高度达到maxDepth &lt;br/&gt;
(2)minInfoGain，当前节点的所有属性分割带来的信息增益都比这个值要小 &lt;br/&gt;
(3)minInstancesPerNode，需要保证节点分割出的左右子节点的最少的样本数量达到这个值 &lt;br/&gt;&lt;/p&gt;

&lt;h4 id=&#34;源码分析&#34;&gt;源码分析&lt;/h4&gt;

&lt;p&gt;下面就一起揭开decision tree的真正面纱吧！&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://ocwpxekrq.bkt.clouddn.com/DecisionTree.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;gr&gt;输入 - RDD[LabelPoint]&lt;/gr&gt;：
LabelPoint类有两个属性label(标签值), features(特征向量)。将训练数据集转换为LabelPoint形式，作为决策树训练的输入之一。&lt;/p&gt;

&lt;p&gt;&lt;gr&gt;输入 - Strategy&lt;/gr&gt;:
这是关于模型训练过程中需要指定的参数集。&lt;/p&gt;

&lt;p&gt;&lt;gr&gt;buildMetadata()&lt;/gr&gt;:
DecisionTreeMetadata是一个决策树&lt;r&gt;元数据信息&lt;/r&gt;类，其中大部分属性和Strategy类重合。该函数的&lt;r&gt;功能&lt;/r&gt;: &lt;strong&gt;对于类别值特征，区分有序/无序；统计类别值特征的划分点数目和分箱数目。&lt;/strong&gt;是训练决策树模型的一个准备工作。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def buildMetadata(input:RDD[LabeledPoint], strategy:Strategy,
numTrees: Int, featureSubsetStrategy: String):DecisionTreeMetadata =
{...}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于每个类别值特征fi，&lt;r&gt;多分类情况下&lt;/r&gt;：首先计算最大可能分箱maxPossibleBins确定情况下，能够允许的最多类别值m，如果fi的类别值超过m，则当作有序处理；否则认为该特征是无序的，并计算分箱数。&lt;/p&gt;

&lt;p&gt;&lt;r&gt;二分类/回归情况下&lt;/r&gt;：当作有序处理，即numBins(fi) = numCategories(fi)。&lt;/p&gt;

&lt;h3&gt;findSplitsBins&lt;/h3&gt;

&lt;p&gt;&lt;sg&gt;这是DecisionTree.scala中一个方法，基于buildMetadata统计的划分点数目和分箱数，加上数据采样，确定每个特征的划分点值。&lt;/sg&gt;&lt;/p&gt;

&lt;p&gt;&lt;sg&gt;如果存在连续值特征，则对数据集进行无放回采样，采样个数为max(maxBins*maxBins, 10000)。接着进行分箱操作（见findSplitsBinsBySorting函数，findSplitsForContinuousFeature函数）。&lt;/sg&gt;&lt;/p&gt;

&lt;p&gt;&lt;sg&gt;&lt;strong&gt;treeRDD,BaggedRDD&lt;/strong&gt;:
treeRDD的类型是：RDD[TreePoint],treePoint类有两个属性：label和binnedFeatures。BaggedPoint类有两个属性datum和subsampleWeights(在各个采样后数据集中的权重，即出现次数)。
&lt;/sg&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def findSplitsBins(input:RDD[LabeledPoint],
metadata:DecisionTreeMetadata): (Array[Array[Split]], Array[Array[Bin]]) =
{ ... }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;h3&gt;findBestSplits&lt;/h3&gt;
&lt;sg&gt;对于当前节点，计算准则函数下的信息增益，选择最优切分特征和最优切分点。&lt;/sg&gt;&lt;/p&gt;

&lt;p&gt;&lt;gr&gt;其他&lt;/gr&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;configuration&lt;/strong&gt;:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;类名&lt;/th&gt;
&lt;th&gt;枚举值&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Algo&lt;/td&gt;
&lt;td&gt;Classification, Regression&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;FeatureType&lt;/td&gt;
&lt;td&gt;Continuous, Categorical&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;QuantileStrategy&lt;/td&gt;
&lt;td&gt;Sort, MinMax, ApproxHist&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;EnsembleCombiningStrategy&lt;/td&gt;
&lt;td&gt;Average, Sum, Vote&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;impurtiy,loss&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Impurity是准则函数的抽象类，其三个子类分别为Gini,Variance,Entropy。Impurities是Impurity的工厂类，负责解析用户参数，选择对应的准则函数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;private[mllib] object Impurities {
  def fromString(name: String): Impurity = name match {
    case &amp;quot;gini&amp;quot; =&amp;gt; Gini
    case &amp;quot;entropy&amp;quot; =&amp;gt; Entropy
    case &amp;quot;variance&amp;quot; =&amp;gt; Variance
    case _ =&amp;gt; throw new IllegalArgumentException(s&amp;quot;Did not recognize
    Impurity name: $name&amp;quot;)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Loss与Impurity情况类似，它是损失函数的抽象类，其三个子类分别是AbsoluteError,SquaredError,LogLoss。Losses是Loss的工厂类，负责判断损失的所属类别。&lt;/p&gt;

&lt;h2 id=&#34;参考链接&#34;&gt;参考链接&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://spark.apache.org/docs/latest/mllib-decision-tree.html&#34;&gt;spark mllib - decision tree&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;统计学习方法，李航著&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[ml] Boosting</title>
      <link>http://cherishzhang.github.io/post/ml/boosting/</link>
      <pubDate>Fri, 20 May 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/ml/boosting/</guid>
      <description>

&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_boosting&#34;&gt;Boosting&lt;/a&gt; can be interpreted as &lt;strong&gt;an optimization algorithm&lt;/strong&gt; on a suitable cost function. The algorithms that optimize a cost &lt;strong&gt;function over function&lt;/strong&gt; space by &lt;r&gt;iteratively&lt;/r&gt; choosing a function(week hypothesis) that points in the &lt;strong&gt;negative gradient direction&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;在分类问题中，提升(Boosting)思想，通过改变训练数据的概率分布（权重分布），针对不同的训练数据分布，学习多个弱分类器，将这些分类器线性组合，提高分类的性能。&lt;/p&gt;

&lt;p&gt;围绕提升方法，有&lt;strong&gt;&lt;r&gt;两个问题&lt;/r&gt;&lt;/strong&gt;需要考虑：1.每一轮如何改变训练数据的概率或权值分布；2.如何将弱分类器组合成一个强分类器。&lt;/p&gt;

&lt;h2 id=&#34;adaboost-adaptive-boosting&#34;&gt;AdaBoost(Adaptive Boosting)&lt;/h2&gt;

&lt;p&gt;AdaBoost提高那些被前几轮弱分类器线性组成的分类器&lt;r&gt;错误分类&lt;/r&gt;的样本权值，在下一轮的弱分类器中更加关注没有得到正确分类的数据；采用&lt;r&gt;加权多数表决&lt;/r&gt;的方法，误差率小的弱分类器权值大，误差率大的弱
分类器权值小。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;算法流程&lt;/strong&gt;：（二分类问题）&lt;br/&gt;
&lt;r&gt;输入&lt;/r&gt;：训练数据集\(T=\{(x_1,y_1),(x_2,y_2),&amp;hellip;,(x_N,y_N)\}\)和弱分类器算法 &lt;br/&gt;
&lt;r&gt;输出&lt;/r&gt;：最终分类器\(G(x)\)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;初始化训练样本的&lt;r&gt;权值分布&lt;/r&gt;(均匀分布)&lt;br/&gt;$$D_1=(w_{11},&amp;hellip;,w_{1i},&amp;hellip;,w_{1N}), w_{1i}=\frac{1}{N}, i=1,2,&amp;hellip;,N$$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;对于\(m=1,2,&amp;hellip;,M\)(迭代)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用具有权值分布\(D_m\)的训练数据集学习，得到基本分类器\(G_m(x)\)&lt;/li&gt;
&lt;li&gt;计算\(G_m(x)\)在权值分布\(D_m\)的训练数据集上的&lt;strong&gt;分类误差率&lt;/strong&gt;
$$e_m = P(G_m(x_i) \neq y_i) = \sum_{i=1}^N w_{mi}I(G_m(x_i) \neq y_i)$$&lt;/li&gt;
&lt;li&gt;计算\(G_m(x)\)的系数，即&lt;r&gt;表决权&lt;/r&gt;
$$\alpha_{m} = \frac{1}{2} \log \frac{1-e_m}{e_m}$$&lt;/li&gt;
&lt;li&gt;更新训练数据集的权值分布
$$D_{m+1} = (w_{m+1,1},&amp;hellip;,w_{m+1,i},&amp;hellip;,w_{m+1,N})$$
$$w_{m+1,i} = \frac{w_{mi}}{Z_m} \exp (-\alpha_m y_i G_m(x_i))$$
其中\(Z_m\)是规范化因子,$$Z_m = \sum_{i=1}^N w_{mi} exp(-\alpha_m y_i G_m(x_i))$$&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;构建基本分类器的线性组合，得到&lt;r&gt;最终的分类器&lt;/r&gt;
$$f(x) = \sum_{m=1}^M \alpha_m G_m(x)$$
$$G(x) = sign(f(x))$$&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每轮迭代过程中，Adaboost会重点关注错误分类的样本，使得&lt;strong&gt;算法受噪声和离群点的影响较大&lt;/strong&gt;。\(\alpha_m\)随着 \(e_m\) 的减小而增大，\(w_{mi}\) 也随着\(\alpha_m\) 而改变，不断增大误分类样本的权值，减小正确分类样本的权值。&lt;/p&gt;

&lt;p&gt;AdaBoost的一个解释是，模型为&lt;strong&gt;加法模型，损失函数为指数函数，学习算法为前向分步算法的二分类学习方法&lt;/strong&gt;。前向分步算法(forward stagewise algorithm)求解优化问题的思路是：从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式，就可以简化优化的复杂度。&lt;/p&gt;

&lt;h2 id=&#34;boosting-tree-提升树&#34;&gt;Boosting Tree(提升树)&lt;/h2&gt;

&lt;p&gt;采用&lt;strong&gt;加法模型(基函数的线性组合)与前向分步算法，以决策树为基函数的提升方法&lt;/strong&gt;称为提升树，分类问题采用二叉分类树，回归问题采用二叉回归树。&lt;/p&gt;

&lt;p&gt;提升树表示为决策树的加法模型:
$$f_{M(x)} = \sum_{m=1}^M T(x; \Theta_m)$$
其中，\(T(x;\Theta_m)\)表示决策树，\(\Theta_m\)为决策树的参数，M为树的个数&lt;/p&gt;

&lt;p&gt;不同问题的提升树学习算法，&lt;r&gt;主要区别在于使用的损失函数不同&lt;/r&gt;，包括用平方误差损失函数的回归问题，用指数损失函数的分类问题，以及用一般损失函数的一般决策问题。将Adaboost算法中的基本分类器限制为分类决策树，即为针对二分类问题的提升树算法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;提升树算法&lt;/strong&gt;(回归问题):&lt;br/&gt;
&lt;r&gt;输入&lt;/r&gt;:训练数据集\(T=\{(x_1,y_1),(x_2,y_2),&amp;hellip;,(x_N,y_N)\}\)
&lt;br/&gt;
&lt;r&gt;输出&lt;/r&gt;:提升树\(f_M(x)\)&lt;br/&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;确定初始提升树\(f_0(x) = 0\)&lt;/li&gt;
&lt;li&gt;对m = 1,2,&amp;hellip;,M

&lt;ul&gt;
&lt;li&gt;计算残差： \(r_{mi} = y_{i} - f_{m-1}(x_i), i=1,2,&amp;hellip;,N\)&lt;/li&gt;
&lt;li&gt;拟合残差，学习一个回归树，得到\(T(x;\Theta_m)\)&lt;/li&gt;
&lt;li&gt;更新\(f_m(x)=f_{m-1}(x)+T(x;\Theta_m)\)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;得到最终提升树：\(f_M(x)=\sum_{m=1}^M T(x;\Theta_m)\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;gradient-boosting-梯度提升&#34;&gt;Gradient Boosting(梯度提升)&lt;/h2&gt;

&lt;p&gt;前向分布算法针对特殊的损失函数(平方和指数)，对于一般的损失函数，Freidman提出了梯度提升算法。它利用最速下降法的近似方法，其&lt;r&gt;&lt;strong&gt;关键&lt;/strong&gt;&lt;/r&gt;是利用损失函数的负梯度在当前模型的值最为回归问题提升树算法中的残差的近似值，拟合一个回归树。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;梯度提升算法&lt;/strong&gt;：&lt;br/&gt;
&lt;r&gt;输入&lt;/r&gt;:训练数据集\(T={(x_1,y_1),(x_2,y_2),&amp;hellip;,(x_N,y_N)}\)，损失函数\(L(y,f(x))\)&lt;br/&gt;
&lt;r&gt;输出&lt;/r&gt;:回归树\(\hat{f} (x)\)&lt;br\&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;初始化，估计是损失函数极小化的常数值，即只有一个根节点的树
\(f_0(x) = arg\mathop{min}\limits_{c} \sum_{i=1}^{N} L(y_i,c)\)&lt;/li&gt;
&lt;li&gt;对m=1,2,&amp;hellip;,M

&lt;ul&gt;
&lt;li&gt;对i=1,2,&amp;hellip;,N,计算
\(r_{mi} = - \{\left [\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)} \right]\}_{f(x)=f_{m-1}(x)}\)&lt;/li&gt;
&lt;li&gt;对于\(r_{mi}\)拟合一个回归树，得到第m棵树的叶节点区域\(R_{mj}$, j=1,2,&amp;hellip;,J\)（对于平方损失函数，就是通常所说的残差，对于一般损失函数，即为残差的近似值）&lt;/li&gt;
&lt;li&gt;对于j=1,2,&amp;hellip;,J,计算 $$c_{mj} = arg\mathop{min}\limits_{c} \sum_{x_i \in R_{mj}} L(y_i,f_{m-1}(x_i)+c)$$&lt;/li&gt;
&lt;li&gt;更新\(f_m(x) = f_{m-1}(x)+\sum_{j=1}^J c_{mj}I(x\in R_mj)\)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;得到回归树 $$\hat{f}(x) = f_M(x) = \sum_{m=1}^M \sum_{j=1}^J c_{mj} I(x \in R_{mj})$$&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Data Quality in Recommendation</title>
      <link>http://cherishzhang.github.io/post/paper/Data%20Quality%20in%20Recommendation/</link>
      <pubDate>Wed, 16 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>http://cherishzhang.github.io/post/paper/Data%20Quality%20in%20Recommendation/</guid>
      <description>

&lt;p&gt;论文名称：&lt;a href=&#34;http://dl.acm.org/citation.cfm?id=2799670&amp;amp;preflayout=tabs&#34;&gt;Data Quality Matters in Recommender Systems, RecSys &amp;lsquo;15&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;摘要&#34;&gt;摘要&lt;/h2&gt;

&lt;p&gt;&lt;r&gt;数据质量好坏&lt;/r&gt;在信息系统中是一个很重要的因素，但在推荐系统中考虑得却不是很多。比较常用的方法是ad-hoc式的清洗，如，去除数据集中的噪声和不可靠的记录。显露出的缺点是,没有结合数据集本身的特性。&lt;/p&gt;

&lt;p&gt;论文中的主要贡献是考虑推荐系统中两个核心的数据质量问题：&lt;strong&gt;&lt;r&gt;sparsity&lt;/r&gt;&lt;/strong&gt; 和 &lt;strong&gt;&lt;r&gt;redundancy&lt;/r&gt;&lt;/strong&gt;，并设计了数据集相关的&lt;r&gt;阈值模型&lt;/r&gt;和&lt;r&gt;采样等级模型&lt;/r&gt;，然后在一系列的公开数据集上做了验证试验。&lt;/p&gt;

&lt;h2 id=&#34;具体方法&#34;&gt;具体方法&lt;/h2&gt;

&lt;h3 id=&#34;关于数据的稀疏性&#34;&gt;关于数据的稀疏性&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;阈值模型(Threshold model)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;很多基于分数的推荐系统数据集中包含一部分&lt;strong&gt;冷启动&lt;/strong&gt;的用户和物品。一般的数据清洗操会去除这些用户和物品的打分信息。现实中的问题是，怎样得到最优的清洗阈值。最简单的暴力方法(brute-force)是评估所有可能的组合，但时间空间复杂度高，在实际中不可行。&lt;/p&gt;

&lt;p&gt;首先，明确目标:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The aim is to develop a heuristic method that predicts the optimal thresholds for a given user-item rating matrix, without building the model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然后，采用了一些合理的假设，如假设要预测的物品阈值是与&lt;strong&gt;物品向量&lt;/strong&gt;的平均长度&lt;code&gt;$\overline{r}_i$&lt;/code&gt;相关，在打分矩阵中，即为平均每个物品被打分的次数。由于推荐数据集中，只有一小部分的流行商品会被打分多次，大部分商品被打分的次数相对很少，这引入了模型的另外一个特征，即物品向量长度的&lt;strong&gt;power-law&lt;/strong&gt;分布,令H为物品向量长度的分布函数，拟合函数\(H={Ax}^{(-m)}\),其中x是单个物品向量的长度，m是个正数。最终可以得到数据集对应的m参数值。m越大，长尾分布中的尾部越向下，反映到数据集上，就是更加少的物品被多次打分过，也就是说，m越大，数据集本身越稀疏。&lt;/p&gt;

&lt;p&gt;得到以上两个特征因素，接下来就是构建阈值模型，得到公式: \({IT}_d = \gamma*\frac{log(\overline{r}_i)}{m^2}\)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;评价方法&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;总共24个数据集，包括10个开放数据集，如Movielens, Million Songs, Flixster, Moviepilot, Filmtipset, Yelp, Yahoo! Music(broken down into albums, artists, and tracks), and BookCrossing。以及14个从公司或站点获取的专业数据集(proprietary)。&lt;/p&gt;

&lt;p&gt;每个数据集9-1划分成训练集和测试集，使用*Precision@K*作为衡量测试集的标准(K为测试集的总记录数)，10折交叉验证，平均准确率作为最后的得分。&lt;/p&gt;

&lt;p&gt;实验过程：
- 首先，寻找每个数据集d的最优IT值。具体对于某个IT值，过滤掉低于IT的item，对剩余数据进行矩阵分解(Matrix Factorization),建立推荐模型，并通过测试集评测模型的准确率。不断迭代，增长IT，寻找使得测试集准确率最高的IT作为最优阈值\({IT}^{opt}_d\)，对应的准确率为\({P}^{opt}_d\)。
- 接下来，对24个数据集采用leave-one-out交叉验证。在23个数据集上训练阈值模型，然后预测剩余数据集d的阈值\({IT}^{pred}_d\),并在过滤后的数据上训练推荐模型，计算准确率\(P^{pred}_d\)。&lt;/p&gt;

&lt;p&gt;两个评测指标：&lt;/p&gt;

&lt;p&gt;$$ NTE_d = |{IT}^{opt}_d - {IT}^{pred}_d|/{IT}^{opt}_d $$&lt;/p&gt;

&lt;p&gt;$$ AR_d = P^{pred}_d/P^{opt}_d $$&lt;/p&gt;

&lt;p&gt;注：&lt;strong&gt;IT&lt;/strong&gt;(item threshold), &lt;strong&gt;AR&lt;/strong&gt;(accuracy ratio), &lt;strong&gt;NTE&lt;/strong&gt;(normalized threshold error)&lt;/p&gt;

&lt;p&gt;通过计算，\({NTE}_d\)和\({AR}_d\)之间的相关度为-0.54。这表明IT的误差越小，推荐模型的准确率越高。&lt;/p&gt;

&lt;h4 id=&#34;关于数据的冗余性&#34;&gt;&lt;strong&gt;关于数据的冗余性&lt;/strong&gt;&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;模型&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;数据集可以通过随机采样，建立模型，使得尽可能得与在全部数据上建立模型的结果相似。这对于实际中的大规模数据是有效的。&lt;/p&gt;

&lt;p&gt;模型目标：
   - The aim is to pick the &lt;strong&gt;loweset sampling rate&lt;/strong&gt; that will still result in the recommendation model as close as possible to the model that would have been built using the complete data.&lt;/p&gt;

&lt;p&gt;与阈值模型不同的是，在采样率的取值上没有最优。因为在全体数据上构建推荐模型总是最优的。&lt;/p&gt;

&lt;p&gt;定义\(SR\)为采样数据的推荐结果与全体数据的推荐结果相似度不低于\(\Delta\)时的最低采样率。&lt;/p&gt;

&lt;p&gt;定义\(U_d, I_d, R_d\)为数据集d中的用户数量，商品数量，和打分项个数，给定采样率\(SR\),从所有用户中随机选取\(SR*U_d\)个用户，在采样的数据集上建立推荐模型，对一个固定的测试集产生预测结果。比较与全部数据预测的结果差异。&lt;/p&gt;

&lt;p&gt;以对用户采样举例，假设数据的冗余性与三个因素相关a.用户数 b.打分矩阵的稀疏性 c.V-structure，得到公式：&lt;/p&gt;

&lt;p&gt;$${SR}_d = tanh(\frac{1}{{V-structure}_d * \sqrt{U_d} * \frac{R_d}{I_d * U_d}})$$&lt;/p&gt;

&lt;p&gt;其中V-structure的定义为平均相似度之比，分子中的每对用户至少有一个打分项目相同，分母中的每对用户为全体用户的所有组合可能。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;评价&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;采用19个专业数据集，9-1比例分训练集和测试集，10折交叉验证。实验主要验证了模型的合理性。&lt;/p&gt;

&lt;h3 id=&#34;我的思考&#34;&gt;&lt;strong&gt;我的思考&lt;/strong&gt;&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;这篇论文提出了两个模型公式，并完成了实验验证。优点是模型公式与数据集本身的特性相关，不干涉推荐系统的建模过程。&lt;/li&gt;
&lt;li&gt;遗留下的问题：数据的稀疏性和冗余性只针对MF推荐模型，还可以扩展到与其他推荐模型相结合。&lt;/li&gt;
&lt;li&gt;考虑其他的评测指标，如覆盖率和多样性；同时，模型中还可以结合物品的内容属性和用户的位置属性。&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Telstra Network Disruptions</title>
      <link>http://cherishzhang.github.io/post/kaggle/Telstra/</link>
      <pubDate>Wed, 02 Mar 2016 21:03:56 +0800</pubDate>
      
      <guid>http://cherishzhang.github.io/post/kaggle/Telstra/</guid>
      <description>

&lt;h2 id=&#34;1-overview&#34;&gt;&lt;strong&gt;1. Overview&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The goal of the &lt;a href=&#34;https://www.kaggle.com/c/telstra-recruiting-network/&#34;&gt;problem&lt;/a&gt; is to predict Telstr network&amp;rsquo;s fault severity at a time at a particular location based on the &lt;em&gt;log data&lt;/em&gt; available.&lt;/p&gt;

&lt;p&gt;The target has 3 categories:0,1,2. It&amp;rsquo;s a multiclass classification problems.Different types of features are extracted from log files and other sources: event_type.csv, log_feature.csv, resource_type.csv,severity_type.csv.&lt;/p&gt;

&lt;p&gt;My final score is &lt;em&gt;0.44917(72 of 974)&lt;/em&gt; in private leaderboard. Here are my &lt;a href=&#34;https://github.com/Cherishzhang/kaggle/tree/master/Telstra&#34;&gt;code&lt;/a&gt;, and I will record my solution.&lt;/p&gt;

&lt;h2 id=&#34;2-feature-engineer&#34;&gt;&lt;strong&gt;2. Feature Engineer&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Selecting and designing good features is an important area in machine learning, which is called &lt;em&gt;feature engineering&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;At First, I just merge all files on Id. Here are five type of features. they are location, severity_type, resource_type, event_type, log_feature.
location and severity_type are just one-to-one variables, and others are many_to_one variables.
There are about 1200 locations, some are only in the test set. The correlation between location variable and target is 0.27, it gave me a hint that neighbouring locations may be similar in the network&amp;rsquo;s fault severity problem. I used one_hot encode to solve the many_to_one features. So there are about 400+ features in the initial stage.&lt;/p&gt;

&lt;p&gt;I read a paper about how to preprocess high-cardinality Categorical attributes in classification and regression problems, but it seemed to bring little help. I will tried it again after the competition.&lt;/p&gt;

&lt;p&gt;On the forum, there are a heated discussion about the magic feature. I spent much of my time to find it.
It is the order of the same location in the severity file which follows the order of fault occurrence.It&amp;rsquo;s called &lt;em&gt;Intra-location order&lt;/em&gt;.&lt;br /&gt;
It really did a big help to the final score which improved almost 0.06. To each record, I compute the target(&lt;em&gt;fault_severity&lt;/em&gt;) probabilities from the previous same location records, and used the &lt;em&gt;previous-target_probabilities&lt;/em&gt; as a feature to build the model.&lt;/p&gt;

&lt;h2 id=&#34;3-build-models-and-ensembling&#34;&gt;&lt;strong&gt;3. Build models and Ensembling&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;I tried many models, decision tree, random forests, svm and xgboost. The xgboost model is performed well.
On ensembling, I just average the random forest and xgboost result as the final result.&lt;/p&gt;

&lt;h2 id=&#34;4-what-i-learned-from-other-kagglers&#34;&gt;&lt;strong&gt;4. What I learned from other kagglers&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;On the platform, kagglers are willing to share their ideas. Here are some valuable ideas on the &lt;a href=&#34;https://www.kaggle.com/c/telstra-recruiting-network/forums/t/19239/it-s-been-fun-post-your-code-github-links-here-after-the-competition&#34;&gt;Competition Forum&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;some-useful-tips-on-feature-engineering&#34;&gt;Some useful tips on feature engineering&lt;/h3&gt;

&lt;p&gt;It&amp;rsquo;s the most important step in machine learning whatever models you used.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;About location&lt;br /&gt;
&lt;em&gt;Similar location numbers have similar fault severity.(Treat the location as numeric.)&lt;/em&gt;
&lt;em&gt;Don&amp;rsquo;t one-hot-encode the location, Tree-based classifiers are not good at handling huge sparse feature matrix.&lt;/em&gt;
&lt;em&gt;Frequency of LogType_203 = 0 &amp;amp; LogType_203 &amp;gt;0 per location&lt;/em&gt;
&lt;em&gt;The records in log data are arranged in the order of time.(the magic feature)&lt;/em&gt; Here are two ways to encode
the information, One is for each location, use the row number, which starts from 1 to the total number of rows for that locaiton. The other
is to normalize it between 0 and 1.
&lt;em&gt;percentile transformation of location counts&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;About log feature&lt;br /&gt;
&lt;em&gt;Pattern of log feature&lt;/em&gt;
&amp;ldquo;one hot&amp;rdquo; encoding for all log features with volume &amp;gt; 0, for all rows.
Each &amp;ldquo;one hot&amp;rdquo; encoded pattern treated as a string.
Assigned integer ID to each to each string, used as feature.
The log transform for the count of &amp;ldquo;pattern of log feature&amp;rdquo;, the log transform for counts as &amp;ldquo;pattern of event&amp;rdquo;
and &amp;ldquo;pattern of resource&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Common categorical variables&lt;br /&gt;
&lt;em&gt;For high-cardinality categorical variables, frequency works well.(Add the frequency of each location in both train and test set.)&lt;/em&gt;
&lt;em&gt;Summary statistics to reduce one-to-many relationship to one-to-one, and two-way or more-way interaction among multiple variables.&lt;/em&gt;
&lt;em&gt;Meta features(using Logistic regression to fit sparse matrix as predictors, then ensemble the model).&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;A useful solution&lt;br /&gt;
&amp;gt;a.The order of id on log_feature was frozen.&lt;br /&gt;
&amp;gt;b.Converting location, log_feature into numbers and generating count, mean, sum, etc. features (feature set A)&lt;br /&gt;
&amp;gt;c.Feature B was generated by shifting A forward by 1 row&lt;br /&gt;
&amp;gt;d.Feature C was generated by shifting B backward by 1 row&lt;br /&gt;
&amp;gt;Combining A, B, and C and training xgb, RF, GBM models. My final model is an ensemble model of these models.&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>